<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Applied Machine Learning Using mlr3 in R - 10&nbsp; Advanced Technical Aspects of mlr3</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/chapter11/large-scale_benchmarking.html" rel="next">
<link href="../../chapters/chapter9/preprocessing.html" rel="prev">
<link href="../../Figures/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html">Advanced Topics</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Advanced Technical Aspects of mlr3</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Applied Machine Learning Using mlr3 in R</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mlr-org/mlr3book/tree/main/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Applied-Machine-Learning-Using-mlr3-in-R.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter1/introduction_and_overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction and Overview</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
 <span class="menu-text">Fundamentals</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/data_and_basic_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data and Basic Modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter3/evaluation_and_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluation and Benchmarking</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">Tuning and Feature Selection</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter4/hyperparameter_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advanced Tuning Methods and Black Box Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter6/feature_selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Feature Selection</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">Pipelines and Preprocessing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter7/sequential_pipelines.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Sequential Pipelines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter8/non-sequential_pipelines_and_tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Non-sequential Pipelines and Tuning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter9/preprocessing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Preprocessing</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Advanced Technical Aspects of mlr3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter11/large-scale_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Large-Scale Benchmarking</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter12/model_interpretation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Model Interpretation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter13/beyond_regression_and_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Beyond Regression and Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter14/algorithmic_fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Algorithmic Fairness</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Solutions to exercises</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/tasks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Tasks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/overview-tables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Overview Tables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/errata.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Errata</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-parallelization" id="toc-sec-parallelization" class="nav-link active" data-scroll-target="#sec-parallelization"><span class="header-section-number">10.1</span> Parallelization</a>
  <ul class="collapse">
<li><a href="#sec-parallel-learner" id="toc-sec-parallel-learner" class="nav-link" data-scroll-target="#sec-parallel-learner"><span class="header-section-number">10.1.1</span> Parallelization of Learners</a></li>
  <li><a href="#sec-parallel-resample" id="toc-sec-parallel-resample" class="nav-link" data-scroll-target="#sec-parallel-resample"><span class="header-section-number">10.1.2</span> Parallelization of Resamplings and Benchmarks</a></li>
  <li><a href="#sec-parallel-tuning" id="toc-sec-parallel-tuning" class="nav-link" data-scroll-target="#sec-parallel-tuning"><span class="header-section-number">10.1.3</span> Parallelization of Tuning</a></li>
  <li><a href="#sec-nested-resampling-parallelization" id="toc-sec-nested-resampling-parallelization" class="nav-link" data-scroll-target="#sec-nested-resampling-parallelization"><span class="header-section-number">10.1.4</span> Nested Resampling Parallelization</a></li>
  <li><a href="#sec-parallel-predict" id="toc-sec-parallel-predict" class="nav-link" data-scroll-target="#sec-parallel-predict"><span class="header-section-number">10.1.5</span> Parallelization of Predictions</a></li>
  </ul>
</li>
  <li>
<a href="#sec-error-handling" id="toc-sec-error-handling" class="nav-link" data-scroll-target="#sec-error-handling"><span class="header-section-number">10.2</span> Error Handling</a>
  <ul class="collapse">
<li><a href="#sec-encapsulation" id="toc-sec-encapsulation" class="nav-link" data-scroll-target="#sec-encapsulation"><span class="header-section-number">10.2.1</span> Encapsulation</a></li>
  <li><a href="#sec-fallback" id="toc-sec-fallback" class="nav-link" data-scroll-target="#sec-fallback"><span class="header-section-number">10.2.2</span> Fallback Learners</a></li>
  </ul>
</li>
  <li><a href="#sec-logging" id="toc-sec-logging" class="nav-link" data-scroll-target="#sec-logging"><span class="header-section-number">10.3</span> Logging</a></li>
  <li>
<a href="#sec-backends" id="toc-sec-backends" class="nav-link" data-scroll-target="#sec-backends"><span class="header-section-number">10.4</span> Data Backends</a>
  <ul class="collapse">
<li><a href="#databases-with-databackenddplyr" id="toc-databases-with-databackenddplyr" class="nav-link" data-scroll-target="#databases-with-databackenddplyr"><span class="header-section-number">10.4.1</span> Databases with DataBackendDplyr</a></li>
  <li><a href="#parquet-files-with-databackendduckdb" id="toc-parquet-files-with-databackendduckdb" class="nav-link" data-scroll-target="#parquet-files-with-databackendduckdb"><span class="header-section-number">10.4.2</span> Parquet Files with DataBackendDuckDB</a></li>
  </ul>
</li>
  <li><a href="#sec-extending" id="toc-sec-extending" class="nav-link" data-scroll-target="#sec-extending"><span class="header-section-number">10.5</span> Extending mlr3 and Defining a New <code>Measure</code></a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">10.6</span> Conclusion</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">10.7</span> Exercises</a></li>
  <li><a href="#citation" id="toc-citation" class="nav-link" data-scroll-target="#citation"><span class="header-section-number">10.8</span> Citation</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/mlr-org/mlr3book/edit/main/book/chapters/chapter10/advanced_technical_aspects_of_mlr3.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mlr-org/mlr3book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/mlr-org/mlr3book/blob/main/book/chapters/chapter10/advanced_technical_aspects_of_mlr3.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html">Advanced Topics</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Advanced Technical Aspects of mlr3</span></a></li></ol></nav><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-technical" class="quarto-section-identifier"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Advanced Technical Aspects of mlr3</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><p><strong>Michel Lang</strong> <br><em>Research Center Trustworthy Data Science and Security, and TU Dortmund University</em></p>
<p><strong>Sebastian Fischer</strong> <br><em>Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)</em></p>
<p><strong>Raphael Sonabend</strong> <br><em>Imperial College London</em> <br><br></p>
<p>In the previous chapters, we demonstrated how to turn machine learning concepts and methods into code. In this chapter we will turn to those technical details that can be important for more advanced uses of <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a>, including:</p>
<ul>
<li>Parallelization with the <a href="https://cran.r-project.org/package=future"><code>future</code></a> framework (<a href="#sec-parallelization" class="quarto-xref"><span>Section 10.1</span></a>);</li>
<li>Error handling and debugging (<a href="#sec-error-handling" class="quarto-xref"><span>Section 10.2</span></a>);</li>
<li>Adjusting the logger to your needs (<a href="#sec-logging" class="quarto-xref"><span>Section 10.3</span></a>);</li>
<li>Working with out-of-memory data, e.g., data stored in databases (<a href="#sec-backends" class="quarto-xref"><span>Section 10.4</span></a>); and</li>
<li>Adding new classes to <code>mlr3</code> (<a href="#sec-extending" class="quarto-xref"><span>Section 10.5</span></a>).</li>
</ul>
<section id="sec-parallelization" class="level2 page-columns page-full" data-number="10.1"><h2 data-number="10.1" class="anchored" data-anchor-id="sec-parallelization">
<span class="header-section-number">10.1</span> Parallelization</h2>
<p>The term parallelization refers to running multiple algorithms in parallel, i.e., executing them simultaneously on multiple CPU cores, CPUs, or computational nodes. Not all algorithms can be parallelized, but when they can, parallelization allows significant savings in computation time.</p>
<p>In general, there are many possibilities to parallelize, depending on the hardware to run the computations. If you only have a single CPU with multiple cores, then <em>threads</em> or <em>processes</em> are ways to utilize all cores on a local machine. If you have multiple machines on the other hand, they can communicate and exchange information via protocols such as <em>network sockets</em> or the <em>Message Passing Interface</em>. Larger computational sites rely on scheduling systems to orchestrate the computation for multiple users and usually offer a shared network file system all machines can access. Interacting with scheduling systems on compute clusters is covered in <a href="../chapter11/large-scale_benchmarking.html#sec-hpc-exec" class="quarto-xref"><span>Section 11.2</span></a> using the R package <a href="https://cran.r-project.org/package=batchtools"><code>batchtools</code></a>.</p>
<p>There are a few pieces of terminology associated with parallelization that we will use in this section:</p>
<ul>
<li>The parallelization backend is the hardware to parallelize with a respective interface provided by an R package. Many parallelization backends have different APIs, so we use the <a href="https://cran.r-project.org/package=future"><code>future</code></a> package as a unified, abstraction layer for many parallelization backends. From a user perspective, <code>mlr3</code> interfaces with <code>future</code> directly so all you will need to do is configure the backend before starting any computations.</li>
<li>The Main process is the R session or process that orchestrates the computational work, called jobs.</li>
<li>Workers are the R sessions, processes, or machines that receive the jobs, perform calculations, and then send the results back to Main.</li>
</ul>
<div class="no-row-height column-margin column-container"><span class="margin-aside">Parallelization Backend</span></div><p>An important step in parallel programming involves the identification of sections of the program flow that are both time-consuming (‘bottlenecks’) and can run independently of a different section, i.e., section A’s operations are not dependent on the results of section B’s operations, and vice versa. Fortunately, these sections are usually relatively easy to spot for machine learning experiments:</p>
<ol type="1">
<li>Training of a learning algorithm (or other computationally intensive parts of a machine learning pipeline) <em>may</em> contain independent sections which can run in parallel, e.g.
<ul>
<li>A single decision tree iterates over all features to find the best split point, for each feature independently.</li>
<li>A random forest usually fits hundreds of trees independently.</li>
</ul>
The key principle that makes parallelization possible for these examples (and in general in many fields of statistics and ML) is called data parallelism, which means the same operation is performed concurrently on different elements of the input data. Parallelization of learning algorithms is covered in <a href="#sec-parallel-learner" class="quarto-xref"><span>Section 10.1.1</span></a>.</li>
<li>Resampling consists of independent repetitions of train-test-splits and benchmarking consists of multiple independent resamplings (<a href="#sec-parallel-resample" class="quarto-xref"><span>Section 10.1.2</span></a>).</li>
<li>Tuning (<a href="../chapter4/hyperparameter_optimization.html" class="quarto-xref"><span>Chapter 4</span></a>) often is iterated benchmarking, embedded in a sequential procedure that determines the hyperparameter configurations to try next. While many tuning algorithms are inherently sequential to some degree, there are some (e.g., random search) that can propose multiple configurations in parallel to be evaluated independently, providing another level for parallelization (<a href="#sec-nested-resampling-parallelization" class="quarto-xref"><span>Section 10.1.4</span></a>).</li>
<li>Predictions of a single learner for multiple observations can be computed independently (<a href="#sec-parallel-predict" class="quarto-xref"><span>Section 10.1.5</span></a>).</li>
</ol>
<div class="no-row-height column-margin column-container"><span class="margin-aside">Data Parallelism</span></div><div class="page-columns page-full"><p>These examples are referred to as “embarrassingly parallel” as they are so easy to parallelize. If we can formulate the problem as a function that can be passed to map-like functions such as <a href="https://www.rdocumentation.org/packages/base/topics/lapply"><code>lapply()</code></a>, then you have an embarrassingly parallel problem. However, just because a problem <em>can</em> be parallelized, it does not follow that every operation in a problem <em>should</em> be parallelized. Starting and terminating workers as well as possible communication between workers comes at a price in the form of additionally required runtime which is called parallelization overhead. This overhead strongly varies between parallelization backends and must be carefully weighed against the runtime of the sequential execution to determine if parallelization is worth the effort. If the sequential execution is comparably fast, enabling parallelization may introduce additional complexity with little runtime savings, or could even slow down the execution. It is possible to control the granularity of the parallelization to reduce the parallelization overhead. For example, we could reduce the overhead of parallelizing a <code>for</code>-loop with 1000 iterations on four CPU cores by chunking the work of the 1000 jobs into four computational jobs performing 250 iterations each, resulting in four big jobs and not 1000 small ones.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Embarrassingly Parallel</span><span class="margin-aside">Parallelization Overhead</span><span class="margin-aside">Granularity</span></div></div>
<p>This effect is illustrated in the following code chunk using a socket cluster with the <a href="https://cran.r-project.org/package=parallel"><code>parallel</code></a> package, which has a <code>chunk.size</code> option so we do not need to manually create chunks:</p>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># set up a socket cluster with 4 workers on the local machine</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">parallel</span><span class="op">)</span></span>
<span><span class="va">cores</span> <span class="op">=</span> <span class="fl">4</span></span>
<span><span class="va">cl</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/parallel/makeCluster.html">makeCluster</a></span><span class="op">(</span><span class="va">cores</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># vector to operate on</span></span>
<span><span class="va">x</span> <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10000</span></span>
<span></span>
<span><span class="co"># fast function to parallelize</span></span>
<span><span class="va">f</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">y</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># unchunked approach: 1000 jobs</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/system.time.html">system.time</a></span><span class="op">(</span><span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/parallel/clusterApply.html">parSapply</a></span><span class="op">(</span><span class="va">cl</span>, <span class="va">x</span>, <span class="va">f</span>, chunk.size <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">}</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   user  system elapsed 
  1.622   0.196 110.111 </code></pre>
</div>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># chunked approach: 4 jobs</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/system.time.html">system.time</a></span><span class="op">(</span><span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/parallel/clusterApply.html">parSapply</a></span><span class="op">(</span><span class="va">cl</span>, <span class="va">x</span>, <span class="va">f</span>, chunk.size <span class="op">=</span> <span class="fl">2500</span><span class="op">)</span><span class="op">}</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   user  system elapsed 
  0.005   0.000   0.072 </code></pre>
</div>
</div>
<div class="page-columns page-full"><p>Whenever you have the option to control the granularity by setting the chunk size, you should aim for at least as many jobs as workers. However, if there are too few job chunks with strongly dissimilar runtimes, the system may end up waiting for the last chunk to finish, while other resources are idle. This is referred to as synchronization overhead. You should therefore aim for chunks with a runtime of at least several seconds, so that the parallelization overhead remains reasonable, while still having enough chunks to ensure that you can fully utilize the system. If you have heterogeneous runtimes, you can consider grouping jobs so that the runtimes of the chunks are more homogeneous. If runtimes can be estimated, then both <code><a href="https://rdrr.io/pkg/batchtools/man/chunk.html">batchtools::binpack()</a></code> and <code><a href="https://rdrr.io/pkg/batchtools/man/chunk.html">batchtools::lpt()</a></code> (documented together with the <a href="https://www.rdocumentation.org/packages/batchtools/topics/chunk"><code>chunk()</code></a> function) are useful for chunking jobs. If runtimes cannot be estimated, then it can be useful to randomize the order of jobs. Otherwise jobs could be accidentally ordered by runtime, for example because they are sorted by a hyperparameter that has a strong influence on training time. Naively chunking jobs could then lead to some chunks containing much more expensive jobs than others, resulting in avoidable underutilization of resources. <a href="https://mlr3misc.mlr-org.com"><code>mlr3misc</code></a> ships with the functions <a href="https://mlr3misc.mlr-org.com/reference/chunk.html"><code>chunk()</code></a> and <a href="https://mlr3misc.mlr-org.com/reference/chunk_vector.html"><code>chunk_vector()</code></a> that conveniently chunk jobs and also shuffle them by default. There are also options to control the chunk size for parallelization in <code>mlr3</code>, which are discussed in <a href="#sec-parallel-resample" class="quarto-xref"><span>Section 10.1.2</span></a>.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Synchronization Overhead</span></div></div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reproducibility
</div>
</div>
<div class="callout-body-container callout-body">
<p>Reproducibility is often a concern during parallelization because special Pseudorandom number generators (PRNGs) may be required <span class="citation" data-cites="future119">(<a href="../references.html#ref-future119" role="doc-biblioref">Bengtsson 2020</a>)</span>. However, <a href="https://cran.r-project.org/package=future"><code>future</code></a> ensures that all workers will receive the same PRNG streams, independent of the number of workers <span class="citation" data-cites="future119">(<a href="../references.html#ref-future119" role="doc-biblioref">Bengtsson 2020</a>)</span>. Therefore, <code>mlr3</code> experiments will be reproducible as long as you use <code>set.seed</code> at the start of your scripts (with the PRNG of your choice).</p>
</div>
</div>
<section id="sec-parallel-learner" class="level3" data-number="10.1.1"><h3 data-number="10.1.1" class="anchored" data-anchor-id="sec-parallel-learner">
<span class="header-section-number">10.1.1</span> Parallelization of Learners</h3>
<p>At the lowest level, external code can be parallelized if available in underlying implementations. For example, while fitting a single decision tree, each split that divides the data into two disjoint partitions requires a search for the best cut point on all <span class="math inline">\(p\)</span> features. Instead of iterating over all features sequentially, the search can be broken down into <span class="math inline">\(p\)</span> threads, each searching for the best cut point on a single feature. These threads can then be scheduled depending on available CPU cores, as there is no need for communication between the threads. After all the threads have finished, the results are collected and merged before terminating the threads. The <span class="math inline">\(p\)</span> best-cut points per feature are collected and aggregated to the single best-cut point across all features by iterating over the <span class="math inline">\(p\)</span> results sequentially.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
GPU Computation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Parallelization on GPUs is not covered in this book. <code>mlr3</code> only distributes the fitting of multiple learners, e.g., during resampling, benchmarking, or tuning. On this rather abstract level, GPU parallelization does not work efficiently. However, some learning procedures can be compiled against CUDA/OpenCL to utilize the GPU while fitting a single model. We refer to the respective documentation of the learner’s implementation, e.g., <a href="https://xgboost.readthedocs.io/en/stable/gpu/">https://xgboost.readthedocs.io/en/stable/gpu/</a> for XGBoost.</p>
</div>
</div>
<p>Threading is implemented in the compiled code of the package (e.g., in C or C++), which means that the R interpreter calls the external code and waits for the results to be returned, without noticing that the computations are executed in parallel. Therefore, threading can conflict with certain parallel backends, leading the system to be overutilized in the best-case scenario, or causing hangs or segfaults in the worst case. For this reason, we introduced the convention that threading parallelization is turned off by default. Hyperparameters that control the number of threads are tagged with the label <code>"threads"</code>:</p>
<div class="cell">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_ranger</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.ranger"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># show all hyperparameters tagged with "threads"</span></span>
<span><span class="va">lrn_ranger</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">ids</span><span class="op">(</span>tags <span class="op">=</span> <span class="st">"threads"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "num.threads"</code></pre>
</div>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># The number of threads is initialized to 1</span></span>
<span><span class="va">lrn_ranger</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span><span class="op">$</span><span class="va">num.threads</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1</code></pre>
</div>
</div>
<p>To enable the parallelization for this learner, <code>mlr3</code> provides the helper function <a href="https://mlr3.mlr-org.com/reference/set_threads.html"><code>set_threads()</code></a>, which automatically adjusts the hyperparameters associated with builtin learner parallelization:</p>
<div class="cell">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># use four CPUs</span></span>
<span><span class="fu">set_threads</span><span class="op">(</span><span class="va">lrn_ranger</span>, n <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;LearnerClassifRanger:classif.ranger&gt;
* Model: -
* Parameters: num.threads=4
* Packages: mlr3, mlr3learners, ranger
* Predict Types:  [response], prob
* Feature Types: logical, integer, numeric, character, factor,
  ordered
* Properties: hotstart_backward, importance, multiclass,
  oob_error, twoclass, weights</code></pre>
</div>
</div>
<p>If we did not specify an argument for the <code>n</code> parameter then the default is a heuristic to detect the correct number using <a href="https://www.rdocumentation.org/packages/parallelly/topics/availableCores"><code>availableCores()</code></a>. This heuristic is not always ideal (interested readers might want to look up “Amdahl’s Law”) and utilizing all available cores is occasionally counterproductive and can slow down overall runtime <span class="citation" data-cites="avoiddetect">(<a href="../references.html#ref-avoiddetect" role="doc-biblioref">Bengtsson 2022</a>)</span>, moreover using all cores is not ideal if:</p>
<ul>
<li>You want to simultaneously use your system for other purposes.</li>
<li>You are on a multi-user system and want to spare some resources for other users.</li>
<li>You have linked R to a threaded BLAS implementation like OpenBLAS and your learners make heavy use of linear algebra.</li>
</ul>
<div class="cell">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># auto-detect cores on the local machine</span></span>
<span><span class="fu">set_threads</span><span class="op">(</span><span class="va">lrn_ranger</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;LearnerClassifRanger:classif.ranger&gt;
* Model: -
* Parameters: num.threads=4
* Packages: mlr3, mlr3learners, ranger
* Predict Types:  [response], prob
* Feature Types: logical, integer, numeric, character, factor,
  ordered
* Properties: hotstart_backward, importance, multiclass,
  oob_error, twoclass, weights</code></pre>
</div>
</div>
<p>To control how many cores are set, we recommend manually setting the number of CPUs in your system’s <code>.Rprofile</code> file:</p>
<div class="cell">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/options.html">options</a></span><span class="op">(</span>mc.cores <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>There are also other approaches for parallelization of learners, e.g.&nbsp;by directly supporting one specific parallelization backend or a parallelization framework like <a href="https://cran.r-project.org/package=foreach"><code>foreach</code></a>. If this is supported, parallelization must be explicitly activated, e.g.&nbsp;by setting a hyperparameter. If you need to parallelize on the learner level because a single model fit takes too much time, and you only fit a few of these models, consult the documentation of the respective learner. In many scenarios, it makes more sense to parallelize on a different level like resampling or benchmarking which is covered in the following subsections.</p>
</section><section id="sec-parallel-resample" class="level3" data-number="10.1.2"><h3 data-number="10.1.2" class="anchored" data-anchor-id="sec-parallel-resample">
<span class="header-section-number">10.1.2</span> Parallelization of Resamplings and Benchmarks</h3>
<p>In addition to parallel learners, most machine learning experiments can be easily parallelized during resampling. By definition, resampling is performed by aggregating over independent repetitions of multiple train-test splits.</p>
<p><code>mlr3</code> makes use of <a href="https://cran.r-project.org/package=future"><code>future</code></a> to enable parallelization over resampling iterations using the parallel backend, which can be configured by the user via the <a href="https://www.rdocumentation.org/packages/future/topics/plan"><code>plan()</code></a> function.</p>
<p>By example, we will look at parallelizing three-fold CV for a decision tree on the sonar task (<a href="#fig-parallel-overview" class="quarto-xref">Figure&nbsp;<span>10.1</span></a>). We use the <a href="https://www.rdocumentation.org/packages/future/topics/multisession"><code>multisession</code></a> plan (which internally uses socket clusters from the <code>parallel</code> package) that should work on all operating systems.</p>
<div class="cell">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://future.futureverse.org">future</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># select the multisession backend to use</span></span>
<span><span class="fu">future</span><span class="fu">::</span><span class="fu"><a href="https://future.futureverse.org/reference/plan.html">plan</a></span><span class="op">(</span><span class="st">"multisession"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># run our experiment</span></span>
<span><span class="va">tsk_sonar</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"sonar"</span><span class="op">)</span></span>
<span><span class="va">lrn_rpart</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span></span>
<span><span class="va">rsmp_cv3</span> <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/system.time.html">system.time</a></span><span class="op">(</span><span class="op">{</span><span class="fu">resample</span><span class="op">(</span><span class="va">tsk_sonar</span>, <span class="va">lrn_rpart</span>, <span class="va">rsmp_cv3</span><span class="op">)</span><span class="op">}</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   user  system elapsed 
  0.149   0.001   1.004 </code></pre>
</div>
</div>
<p>By default, all CPUs of your machine are used unless you specify the argument <code>workers</code> in <code><a href="https://future.futureverse.org/reference/plan.html">future::plan()</a></code> (see the previous section for issues that this might cause). In contrast to threads, the technical overhead for starting workers, communicating objects, sending back results, and shutting down the workers is quite large for the <code>"multisession"</code> backend.</p>
<p>The <a href="https://www.rdocumentation.org/packages/future/topics/multicore"><code>multicore</code></a> backend comes with more overhead than threading, but considerably less overhead than <code>"multisession"</code>, as the <code>"multicore"</code> backend only copies R objects when modified (‘copy-on-write’), whereas objects are always copied to the respective session before any computation for <code>"multisession"</code>. The <code>"multicore"</code> backend has the major disadvantage that it is not supported on Windows systems - for this reason, we will stick with the <code>"multisession"</code> backend for all examples here.</p>
<p>In general, it is advised to only consider parallelization for resamplings where each iteration runs at least a few seconds. There are two <code>mlr3</code> options to control the execution and granularity:</p>
<ul>
<li>If <code>mlr3.exec_random</code> is set to <code>TRUE</code> (default), the order of jobs is randomized in resamplings and benchmarks. This can help if you run a benchmark or tuning with heterogeneous runtimes.</li>
<li>Option <code>mlr3.exec_chunk_size</code> can be used to control how many jobs are mapped to a single <code>future</code> and defaults to <code>1</code>. The value of this option is passed to <a href="https://www.rdocumentation.org/packages/future.apply/topics/future_mapply"><code>future_mapply()</code></a> and <code>future.scheduling</code> is constantly set to <code>TRUE</code>.</li>
</ul>
<p>Tuning the chunk size can help in some rare cases to mitigate the parallelization overhead but is unlikely to be useful in larger problems or longer runtimes.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-parallel-overview" class="quarto-figure quarto-figure-center quarto-float anchored" alt="Flow chart starting with a rectangular box that says 'Main', with an arrow to a diamond that says 'resample()'. This has three arrows to 'Worker 1-3' respectively, each arrow is labeled 'Fold 1-3' respectively. Each of the worker boxes points to the same diamond that says 'ResampleResult' and each arrow is labeled 'Prediction 1-3' respectively.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-parallel-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/mlr3book_figures-30.svg" class="img-fluid figure-img" style="width:100.0%" alt="Flow chart starting with a rectangular box that says 'Main', with an arrow to a diamond that says 'resample()'. This has three arrows to 'Worker 1-3' respectively, each arrow is labeled 'Fold 1-3' respectively. Each of the worker boxes points to the same diamond that says 'ResampleResult' and each arrow is labeled 'Prediction 1-3' respectively.">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-parallel-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.1: Parallelization of a resampling using three-fold CV. The main process calls the <code><a href="https://mlr3.mlr-org.com/reference/resample.html">resample()</a></code> function, which starts the parallelization process and the computational task is split into three parts for three-fold CV. The folds are passed to three workers, each fitting a model on the respective subset of the task and predicting on the left-out observations. The predictions (and trained models) are communicated back to the main process which combines them into a <code>ResampleResult</code>.
</figcaption></figure>
</div>
</div>
</div>
<p>Benchmarks can be seen as a collection of multiple independent resamplings where a combination of a task, a learner, and a resampling strategy defines one resampling to perform. In pseudo-code, the calculation can be written as</p>
<pre><code>foreach combination of (task, learner, resampling strategy) {
    foreach resampling iteration {
        execute(resampling, j)
    }
}</code></pre>
<p>Therefore we could either:</p>
<ol type="1">
<li>Parallelize over all resamplings and execute each resampling sequentially (parallelize outer loop); or</li>
<li>Iterate over all resamplings and execute each resampling in parallel (parallelize inner loop).</li>
</ol>
<p><code>mlr3</code> simplifies this decision for you by flattening all experiments to the same level, i.e., <a href="https://mlr3.mlr-org.com/reference/benchmark.html"><code>benchmark()</code></a> iterates over the elements of the Cartesian product of the iterations of the outer and inner loops. Therefore, there is no need to decide whether you want to parallelize the tuning <em>or</em> the resampling, you always parallelize both. This approach makes the computation fine-grained and allows the <code>future</code> backend to group the jobs into chunks of suitable size (depending on the number of workers), it also makes the procedure identical to parallelizing resampling:</p>
<div class="cell">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># simple benchmark design</span></span>
<span><span class="va">design</span> <span class="op">=</span> <span class="fu">benchmark_grid</span><span class="op">(</span><span class="fu">tsks</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"sonar"</span>, <span class="st">"penguins"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  <span class="fu">lrns</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"classif.featureless"</span>, <span class="st">"classif.rpart"</span><span class="op">)</span><span class="op">)</span>, <span class="va">rsmp_cv3</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># enable parallelization</span></span>
<span><span class="fu">future</span><span class="fu">::</span><span class="fu"><a href="https://future.futureverse.org/reference/plan.html">plan</a></span><span class="op">(</span><span class="st">"multisession"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># run benchmark in parallel</span></span>
<span><span class="va">bmr</span> <span class="op">=</span> <span class="fu">benchmark</span><span class="op">(</span><span class="va">design</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>See <a href="../chapter11/large-scale_benchmarking.html#sec-hpc-exec" class="quarto-xref"><span>Section 11.2</span></a> for larger benchmark experiments that may have a cumulative runtime of weeks, months or even years.</p>
</section><section id="sec-parallel-tuning" class="level3" data-number="10.1.3"><h3 data-number="10.1.3" class="anchored" data-anchor-id="sec-parallel-tuning">
<span class="header-section-number">10.1.3</span> Parallelization of Tuning</h3>
<p>Tuning is usually an iterative procedure, consisting of steps that are themselves embarrassingly parallel. In each iteration, a tuner proposes a batch of hyperparameter configurations (which could be of size <code>1</code>), which can then be evaluated in parallel. After each iteration, most tuners adapt themselves in some way based on the obtained performance values. Random and grid search are exceptions as they do not choose configurations based on past results, instead, for these tuners, all evaluations are independent and can, in principle, be fully parallelized.</p>
<p>Tuning is implemented in <code>mlr3</code> as iterative benchmarks. The <a href="https://mlr3tuning.mlr-org.com/reference/Tuner.html"><code>Tuner</code></a> proposes a batch of learners, each with a different configuration in its <code>$param_set$values</code>, where the size of the batch can usually be controlled with the <code>batch_size</code> configuration parameter. This batch is passed to <a href="https://mlr3.mlr-org.com/reference/benchmark.html"><code>benchmark()</code></a> with the resampling strategy of the tuning instance.</p>
<p>Since each call to <code><a href="https://mlr3.mlr-org.com/reference/benchmark.html">benchmark()</a></code> depends on previous results, it is generally not possible to parallelize tuning at a higher “level” than individual benchmarks. Instead, the individual <code><a href="https://mlr3.mlr-org.com/reference/benchmark.html">benchmark()</a></code> evaluations are parallelized by <code>mlr3</code> as if they were experiments without tuning. This means that the individual resampling iterations of each evaluated configuration are all parallelized at the same time. To ensure full parallelization, make sure that the <code>batch_size</code> multiplied by the number of resampling iterations is at least equal to the number of available workers. If you expect homogeneous runtimes, i.e., you are tuning over a single learner or pipeline without any hyperparameters with a large influence on the runtime, aim for a multiple of the number of workers. In general, larger batches allow for more parallelization, while smaller batches imply a more frequent evaluation of the termination criteria. Independently of whether you use parallelization, the termination criteria are only checked between evaluations of batches.</p>
<p>The following code shows a parallelized execution of random search with the termination criterion set to 20 iterations and a moderate batch size, where 36 resampling splits – 12 configurations of three splits each – are evaluated in parallel on four workers. The batch size, set to a multiple of the number of workers, ensures that available resources are used efficiently. However, note that the tuning only terminates after a multiple of the given batch size, in this case after 24 evaluations.</p>
<div class="cell">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">future</span><span class="fu">::</span><span class="fu"><a href="https://future.futureverse.org/reference/plan.html">plan</a></span><span class="op">(</span><span class="st">"multisession"</span>, workers <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span></span>
<span><span class="va">instance</span> <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span></span>
<span>  <span class="fu">tnr</span><span class="op">(</span><span class="st">"random_search"</span>, batch_size <span class="op">=</span> <span class="fl">12</span><span class="op">)</span>,</span>
<span>  <span class="fu">tsk</span><span class="op">(</span><span class="st">"penguins"</span><span class="op">)</span>,</span>
<span>  <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.rpart"</span>, minsplit <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">2</span>, <span class="fl">128</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>,</span>
<span>  term_evals <span class="op">=</span> <span class="fl">20</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="va">n_evals</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 24</code></pre>
</div>
</div>
<p>In this example, we could have increased the batch size to 20 to make use of available resources in the most efficient way while stopping exactly at the number of evaluations, however this does not generalize to other termination criteria where we do not know the number of evaluations in advance. For example, if we used <code>trm("perf_reached")</code> with a batch size of 12, then if the first configuration of the batch yielded better performance than the given threshold, the remaining 11 configurations would still be unnecessarily evaluated.</p>
</section><section id="sec-nested-resampling-parallelization" class="level3" data-number="10.1.4"><h3 data-number="10.1.4" class="anchored" data-anchor-id="sec-nested-resampling-parallelization">
<span class="header-section-number">10.1.4</span> Nested Resampling Parallelization</h3>
<p>Nested resampling can conceptually be parallelized at three different levels, each corresponding to jobs of different granularity:</p>
<ol type="1">
<li>The parallelization of the outer resampling. A job is then the tuning of a learner on the respective training set of the outer resampling splits.</li>
<li>The parallel evaluation of the batch of hyperparameter configurations proposed in one tuning iteration. A job is then, for example, the cross-validation of such a configuration.</li>
<li>The parallelization of the inner resampling in tuning. A job is then a train-predict-score step of a single configuration.</li>
</ol>
<p>This is demonstrated in the pseudocode below, which is a simplified form of Algorithm 3 from <span class="citation" data-cites="hpo_practical">Bischl et al. (<a href="../references.html#ref-hpo_practical" role="doc-biblioref">2023</a>)</span>:</p>
<div class="cell">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># outer resampling, level 1:</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_len</a></span><span class="op">(</span><span class="va">n_outer_splits</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># tuning instance, in this example mainly represents the archive</span></span>
<span>  <span class="va">tuning_inst</span> <span class="op">=</span> <span class="fu">ti</span><span class="op">(</span><span class="va">...</span><span class="op">)</span></span>
<span>  <span class="va">inner_task</span> <span class="op">=</span> <span class="fu">get_training_task</span><span class="op">(</span><span class="va">task</span>, <span class="va">outer_splits</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="co"># tuning loop, the details of which depend on the tuner being used</span></span>
<span>  <span class="co"># This does not correspond to a level:</span></span>
<span>  <span class="kw">while</span> <span class="op">(</span><span class="op">!</span><span class="va">tuning_inst</span><span class="op">$</span><span class="va">is_terminated</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">proposed_points</span> <span class="op">=</span> <span class="fu">propose_points</span><span class="op">(</span><span class="va">tuning_inst</span><span class="op">$</span><span class="va">archive</span>, <span class="va">batch_size</span><span class="op">)</span></span>
<span>    <span class="co"># Evaluation of configurations, level 2:</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">hp_configuration</span> <span class="kw">in</span> <span class="va">proposed_points</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="va">split_performances</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="op">)</span></span>
<span>      <span class="co"># Inner resampling, level 3:</span></span>
<span>      <span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_len</a></span><span class="op">(</span><span class="va">n_inner_splits</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="va">split_performances</span><span class="op">[</span><span class="va">j</span><span class="op">]</span> <span class="op">=</span> <span class="fu">evaluate_performance</span><span class="op">(</span></span>
<span>          <span class="va">learner</span>, <span class="va">hp_configuration</span>, <span class="va">inner_task</span>, <span class="va">inner_splits</span><span class="op">[[</span><span class="va">j</span><span class="op">]</span><span class="op">]</span></span>
<span>        <span class="op">)</span></span>
<span>      <span class="op">}</span></span>
<span>      <span class="va">performance</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/aggregate.html">aggregate</a></span><span class="op">(</span><span class="va">split_performances</span><span class="op">)</span></span>
<span>      <span class="fu">update_archive</span><span class="op">(</span><span class="va">tuning_inst</span><span class="op">$</span><span class="va">archive</span>, <span class="va">configuration</span>, <span class="va">performance</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="fu">evaluate_performance</span><span class="op">(</span></span>
<span>    <span class="va">learner</span>, <span class="va">tuning_inst</span><span class="op">$</span><span class="va">result</span>, <span class="va">task</span>, <span class="va">outer_splits</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This algorithm is implemented in <code>mlr3</code> in a slightly more efficient manner. At the second level (the evaluation of hyperparameter configurations), it exploits the functionality of <code><a href="https://mlr3.mlr-org.com/reference/benchmark.html">benchmark()</a></code>: a <code>Learner</code> object is created for each proposed hyperparameter configuration and all learners are resampled in a benchmark experiment in the innermost for-loop, effectively executing the second level along with the third level on a finer granularity (number of proposed points times number of inner resampling iterations). Hence, when parallelizing nested resampling in <code>mlr3</code>, the user only has to choose between two options: parallelizing the outer resampling or the inner benchmarking.</p>
<p>By example, let us tune the <code>minsplit</code> argument of a classification tree using an <a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html"><code>AutoTuner</code></a> (<a href="../chapter4/hyperparameter_optimization.html#sec-autotuner" class="quarto-xref"><span>Section 4.2</span></a>) and random search with only two iterations. Note that this is a didactic example to illustrate the interplay of the different parallelization levels and not a realistic setup. We use holdout for inner resampling and set the <code>batch_size</code> to <code>2</code>, which yields two independent iterations in the inner benchmark experiment. A five-fold CV is used for our outer resampling. For the sake of simplicity, we will also ignore the final model fit the <code>AutoTuner</code> performs after tuning. Below, we run the example sequentially without parallelization:</p>
<div class="cell">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mlr3tuning.mlr-org.com">mlr3tuning</a></span><span class="op">)</span></span>
<span><span class="co"># reset to default sequential plan</span></span>
<span><span class="fu">future</span><span class="fu">::</span><span class="fu"><a href="https://future.futureverse.org/reference/plan.html">plan</a></span><span class="op">(</span><span class="st">"sequential"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">lrn_rpart</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.rpart"</span>,</span>
<span>  minsplit  <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/to_tune.html">to_tune</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">128</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">lrn_rpart_tuned</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/auto_tuner.html">auto_tuner</a></span><span class="op">(</span><span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html">tnr</a></span><span class="op">(</span><span class="st">"random_search"</span>, batch_size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>,</span>
<span>  <span class="va">lrn_rpart</span>, <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"holdout"</span><span class="op">)</span>, <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr</a></span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="va">rr</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/resample.html">resample</a></span><span class="op">(</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"penguins"</span><span class="op">)</span>, <span class="va">lrn_rpart_tuned</span>, <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">5</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now either opt to parallelize the outer CV or the inner benchmarking. Let us assume we have a single CPU with four cores (C1 - C4) available and each inner holdout evaluation during tuning takes four seconds. If we parallelize the outer five-fold CV (<a href="#fig-parallel-outer" class="quarto-xref">Figure&nbsp;<span>10.2</span></a>), each of the four cores would run one outer resampling first, the computation of the fifth iteration has to wait as there are no more available cores.</p>
<div class="cell">
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Parallelize outer loop</span></span>
<span><span class="fu">future</span><span class="fu">::</span><span class="fu"><a href="https://future.futureverse.org/reference/plan.html">plan</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="st">"multisession"</span>, <span class="st">"sequential"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Alternative: skip specification of 2nd level, since future</span></span>
<span><span class="co"># sets all levels after the first to "sequential" by default</span></span>
<span><span class="fu">future</span><span class="fu">::</span><span class="fu"><a href="https://future.futureverse.org/reference/plan.html">plan</a></span><span class="op">(</span><span class="st">"multisession"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This approach is illustrated in <a href="#fig-parallel-outer" class="quarto-xref">Figure&nbsp;<span>10.2</span></a>. Each of the four workers starts with the computation of a different inner benchmark, each of which runs sequentially and therefore takes eight seconds on one worker. As there are more jobs than workers, the remaining fifth iteration of the outer resampling is queued on C1 <strong>after</strong> the first four iterations are finished after eight seconds. During the computation of the fifth outer resampling iteration, only C1 is busy, the other three cores are idle.</p>
<p>In contrast, if we parallelize the inner benchmark (<a href="#fig-parallel-inner" class="quarto-xref">Figure&nbsp;<span>10.3</span></a>) then the outer resampling runs sequentially: the five inner benchmarks are scheduled one after the other, each of which runs its two holdout evaluations in parallel on two cores; meanwhile, C3 and C4 are idle.</p>
<div class="cell">
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Parallelize inner loop</span></span>
<span><span class="fu">future</span><span class="fu">::</span><span class="fu"><a href="https://future.futureverse.org/reference/plan.html">plan</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="st">"sequential"</span>, <span class="st">"multisession"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-parallel-outer" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-parallel-outer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/cpu_utilization_1.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-parallel-outer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.2: CPU utilization for four CPUs while parallelizing the outer five-fold CV with a sequential two-fold CV inside. Jobs are labeled as [iteration outer]-[iteration inner].
</figcaption></figure>
</div>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-parallel-inner" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-parallel-inner-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/cpu_utilization_2.svg" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-parallel-inner-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.3: CPU utilization for four cores while parallelizing the inner benchmarking (consisting of two holdout evaluations) with a sequential five-fold CV outside. Jobs are labeled as [iteration outer]-[iteration inner].
</figcaption></figure>
</div>
</div>
</div>
<p>In this example, both possibilities for parallelization are not exploiting the full potential of the four cores. With parallelization of the outer loop, all results are computed after 16 seconds, if we parallelize the inner loop we obtain them after 20 seconds, and in both cases some CPU cores remain idle for at least some of the time.</p>
<p><code>mlr3</code> and <code>future</code> make it possible to enable parallelization for both loops for nested parallelization, even on different parallelization backends, which can be useful in some distributed computing setups. Note that the detection of available cores does not work for such a nested parallelization and the number of workers must be manually set instead:</p>
<div class="cell">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Runs both loops in parallel</span></span>
<span><span class="fu">future</span><span class="fu">::</span><span class="fu"><a href="https://future.futureverse.org/reference/plan.html">plan</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://future.futureverse.org/reference/tweak.html">tweak</a></span><span class="op">(</span><span class="st">"multisession"</span>, workers <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://future.futureverse.org/reference/tweak.html">tweak</a></span><span class="op">(</span><span class="st">"multisession"</span>, workers <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This example would run on up to four cores on the local machine: first, two new sessions would be spawned for the outer loop. Both new sessions then spawn two additional sessions each to evaluate the inner benchmark. Although two cores are still idle when the fifth outer resampling iteration runs, this approach reduces the total runtime to 12 seconds, which is optimal in this example.</p>
</section><section id="sec-parallel-predict" class="level3" data-number="10.1.5"><h3 data-number="10.1.5" class="anchored" data-anchor-id="sec-parallel-predict">
<span class="header-section-number">10.1.5</span> Parallelization of Predictions</h3>
<p>Finally, predictions from a single learner can be parallelized as the predictions of multiple observations are independent. For most learners, training is the bottleneck and parallelizing the prediction is not a worthwhile endeavor, but there can be exceptions, e.g., if your test dataset is very large.</p>
<p>To predict in parallel, the test data is first split into multiple groups and the predict method of the learner is applied to each group in parallel using an active backend configured via <a href="https://www.rdocumentation.org/packages/future/topics/plan"><code>plan()</code></a>. The resulting predictions are then combined internally in a second step. To avoid predicting in parallel accidentally, parallel predictions must be enabled in the learner via the <code>parallel_predict</code> field:</p>
<div class="cell">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># train random forest on sonar task</span></span>
<span><span class="va">tsk_sonar</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"sonar"</span><span class="op">)</span></span>
<span><span class="va">lrn_rpart</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span></span>
<span><span class="va">lrn_rpart</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_sonar</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># set up parallel predict on four workers</span></span>
<span><span class="fu">future</span><span class="fu">::</span><span class="fu"><a href="https://future.futureverse.org/reference/plan.html">plan</a></span><span class="op">(</span><span class="st">"multisession"</span>, workers <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span><span class="va">lrn_rpart</span><span class="op">$</span><span class="va">parallel_predict</span> <span class="op">=</span> <span class="cn">TRUE</span></span>
<span></span>
<span><span class="co"># predict</span></span>
<span><span class="va">prediction</span> <span class="op">=</span> <span class="va">lrn_rpart</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">tsk_sonar</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section></section><section id="sec-error-handling" class="level2 page-columns page-full" data-number="10.2"><h2 data-number="10.2" class="anchored" data-anchor-id="sec-error-handling">
<span class="header-section-number">10.2</span> Error Handling</h2>
<p>In large experiments, it is not uncommon that a model fit or prediction fails with an error. This is because the algorithms have to process arbitrary data, and not all eventualities can always be handled. While we try to identify obvious problems before execution, such as when missing values occur for a learner that cannot handle them, other problems are far more complex to detect. Examples include numerical problems that may cause issues in training (e.g., due to lack of convergence), or new levels of categorical variables appearing in the prediction step. Different learners behave quite differently when encountering such problems: some models signal a warning during the training step that they failed to fit but return a baseline model, while other models stop the execution. During prediction, some learners error and refuse to predict the response for observations they cannot handle, while others may predict <code>NA</code>. In this section, we will discuss how to prevent these errors from causing the program to stop when we do not want it to (e.g., during a benchmark experiment).</p>
<p>For illustration (and internal testing) of error handling, <code>mlr3</code> ships with <code>lrn("classif.debug")</code> and <code>lrn("regr.debug")</code>:</p>
<div class="cell">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_penguins</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"penguins"</span><span class="op">)</span></span>
<span><span class="va">lrn_debug</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.debug"</span><span class="op">)</span></span>
<span><span class="va">lrn_debug</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;LearnerClassifDebug:classif.debug&gt;: Debug Learner for Classification
* Model: -
* Parameters: list()
* Packages: mlr3
* Predict Types:  [response], prob
* Feature Types: logical, integer, numeric, character, factor,
  ordered
* Properties: hotstart_forward, missings, multiclass, twoclass</code></pre>
</div>
</div>
<p>This learner lets us simulate problems that are frequently encountered in ML. It can be configured to stochastically trigger warnings, errors, and even segfaults, during training or prediction.</p>
<p>With the learner’s default settings, the learner will remember a random label and constantly predict this label without signaling any conditions. In the following code we tell the learner to signal an error during the training step:</p>
<div class="cell">
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># set probability to signal an error to `1`</span></span>
<span><span class="va">lrn_debug</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span><span class="op">$</span><span class="va">error_train</span> <span class="op">=</span> <span class="fl">1</span></span>
<span><span class="va">lrn_debug</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_penguins</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in .__LearnerClassifDebug__.train(self = self, private = private, : Error from classif.debug-&gt;train()</code></pre>
</div>
</div>
<p>Now we can look at how to deal with errors during <code>mlr3</code> experiments.</p>
<section id="sec-encapsulation" class="level3 page-columns page-full" data-number="10.2.1"><h3 data-number="10.2.1" class="anchored" data-anchor-id="sec-encapsulation">
<span class="header-section-number">10.2.1</span> Encapsulation</h3>
<p>Encapsulation ensures that signaled conditions (e.g., messages, warnings and errors) are intercepted and that all conditions raised during the training or prediction step are logged into the learner without interrupting the program flow. This means that models can be used for fitting and predicting and any conditions can be analyzed post hoc. However, the result of the experiment will be a missing model and/or predictions, depending on where the error occurs. In <a href="#sec-fallback" class="quarto-xref"><span>Section 10.2.2</span></a>, we will discuss fallback learners to replace missing models and/or predictions.</p>
<div class="page-columns page-full"><p>Each <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> contains the field <code>$encapsulate</code> to control how the train or predict steps are wrapped. The first way to encapsulate the execution is provided by the package <a href="https://cran.r-project.org/package=evaluate"><code>evaluate</code></a>, which evaluates R expressions and captures and tracks conditions (outputs, messages, warnings or errors) without letting them stop the process (see documentation of <a href="https://mlr3misc.mlr-org.com/reference/encapsulate.html"><code>encapsulate()</code></a> for full details):</p><div class="no-row-height column-margin column-container"><span class="margin-aside">$encapsulate</span></div></div>
<div class="cell">
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># trigger warning and error in training</span></span>
<span><span class="va">lrn_debug</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.debug"</span>, warning_train <span class="op">=</span> <span class="fl">1</span>, error_train <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># enable encapsulation for train() and predict()</span></span>
<span><span class="va">lrn_debug</span><span class="op">$</span><span class="va">encapsulate</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>train <span class="op">=</span> <span class="st">"evaluate"</span>, predict <span class="op">=</span> <span class="st">"evaluate"</span><span class="op">)</span></span>
<span><span class="va">lrn_debug</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_penguins</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note how we passed <code>"evaluate"</code> to <code>train</code> and <code>predict</code> to enable encapsulation in both training and predicting. However, we could have only set encapsulation for one of these stages by instead passing <code>c(train = "evaluate", predict = "none")</code> or <code>c(train = "none", predict = "evaluate")</code>.</p>
<p>Note that encapsulation captures all output written to the standard output (stdout) and standard error (stderr) streams and stores them in the learner’s log. However, in some computational setups, the calling process needs to operate on the log output, such as the <a href="https://cran.r-project.org/package=batchtools"><code>batchtools</code></a> package in <a href="../chapter11/large-scale_benchmarking.html" class="quarto-xref"><span>Chapter 11</span></a>. In this case, use the encapsulation method <code>"try"</code> instead, which catches signaled conditions but does not suppress the output.</p>
<p>After training the learner, one can access the log via the fields <code>log</code>, <code>warnings</code> and <code>errors</code>:</p>
<div class="cell">
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_debug</span><span class="op">$</span><span class="va">log</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   stage   class                                 msg
1: train warning Warning from classif.debug-&gt;train()
2: train   error   Error from classif.debug-&gt;train()</code></pre>
</div>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_debug</span><span class="op">$</span><span class="va">warnings</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Warning from classif.debug-&gt;train()"</code></pre>
</div>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_debug</span><span class="op">$</span><span class="va">errors</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Error from classif.debug-&gt;train()"</code></pre>
</div>
</div>
<p>Another encapsulation method is implemented in the <a href="https://cran.r-project.org/package=callr"><code>callr</code></a> package. In contrast to <code>evaluate</code>, the computation is handled in a separate R process. This guards the calling session against segmentation faults which otherwise would tear down the complete main R session (if we demonstrate that here we would break our book). On the downside, starting new processes comes with comparably more computational overhead.</p>
<div class="cell">
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_debug</span><span class="op">$</span><span class="va">encapsulate</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>train <span class="op">=</span> <span class="st">"callr"</span>, predict <span class="op">=</span> <span class="st">"callr"</span><span class="op">)</span></span>
<span><span class="co"># set segfault_train and remove warning_train and error_train</span></span>
<span><span class="va">lrn_debug</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>segfault_train <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">lrn_debug</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span>task <span class="op">=</span> <span class="va">tsk_penguins</span><span class="op">)</span><span class="op">$</span><span class="va">errors</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "callr process exited with status -11"</code></pre>
</div>
</div>
<p>As well as catching errors, we can also set a timeout, in seconds, so that learners do not run for an indefinite time (e.g., due to failing to converge) but are terminated after a specified time. This works most reliably when using <code>callr</code> encapsulation, since the <code>evaluate</code> method is sometimes not able to interrupt a learner if it gets stuck in external compiled code. If learners are interrupted, then this is logged as an error by the encapsulation process. Again, the timeout can be set separately for training and prediction:</p>
<div class="cell">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># near instant timeout for training, no timeout for predict</span></span>
<span><span class="va">lrn_debug</span><span class="op">$</span><span class="va">timeout</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>train <span class="op">=</span> <span class="fl">1e-5</span>, predict <span class="op">=</span> <span class="cn">Inf</span><span class="op">)</span></span>
<span><span class="va">lrn_debug</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span>task <span class="op">=</span> <span class="va">tsk_penguins</span><span class="op">)</span><span class="op">$</span><span class="va">errors</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "reached elapsed time limit"</code></pre>
</div>
</div>
<p>With these methods, we can now catch all conditions and post hoc analyze messages, warnings and errors.</p>
<p>Unfortunately, catching errors and ensuring an upper time limit is only half the battle. If there are errors during training then we will not have a trained model to query, or if there are errors during predicting, then we will not have predictions to analyze:</p>
<div class="cell">
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># no saved model as there was an error during training</span></span>
<span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.debug"</span>, error_train <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_penguins</span><span class="op">)</span><span class="op">$</span><span class="va">model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in .__LearnerClassifDebug__.train(self = self, private = private, : Error from classif.debug-&gt;train()</code></pre>
</div>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># saved model</span></span>
<span><span class="va">lrn_debug</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.debug"</span>, error_predict <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_penguins</span><span class="op">)</span></span>
<span><span class="va">lrn_debug</span><span class="op">$</span><span class="va">model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$response
[1] "Adelie"

$pid
[1] 10872

$iter
NULL

$id
[1] "7849c4c8-f923-4864-9882-86146517fb9d"

attr(,"class")
[1] "classif.debug_model"</code></pre>
</div>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co">#  but no predictions due to an error during predicting</span></span>
<span><span class="va">lrn_debug</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">tsk_penguins</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in .__LearnerClassifDebug__.predict(self = self, private = private, : Error from classif.debug-&gt;predict()</code></pre>
</div>
</div>
<p>Missing learners and/or predictions are particularly problematic during automated processes such as resampling, benchmarking, or tuning (<a href="../chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-encapsulation-fallback" class="quarto-xref"><span>Section 5.1.1</span></a>), as results cannot be aggregated properly across iterations. In the next section, we will look at fallback learners that impute missing models and predictions.</p>
</section><section id="sec-fallback" class="level3 page-columns page-full" data-number="10.2.2"><h3 data-number="10.2.2" class="anchored" data-anchor-id="sec-fallback">
<span class="header-section-number">10.2.2</span> Fallback Learners</h3>
<p>Say an error has occurred when training a model in one or more iterations during resampling, then there are three methods to proceed with our experiment:</p>
<ol type="1">
<li>Ignore iterations with failures – This might be the most frequent approach in practice, however, it is <strong>not</strong> statistically sound. Say we are trying to evaluate the performance of a model. This model might error if in some resampling splits, there are factor levels during predicting that were not seen during training, thus leading to the model being unable to handle these and erroring. If we discarded failed iterations, our model would appear to perform well despite it failing to make predictions for an entire class of features.</li>
<li>Penalize failing learners – Instead of ignoring failed iterations, we could impute the worst possible score (as defined by a given <a href="https://mlr3.mlr-org.com/reference/Measure.html"><code>Measure</code></a>) and thereby heavily penalize the learner for failing. However, this will often be too harsh for many problems, and for some measures, there is no reasonable value to impute.</li>
<li>Train and predict with a fallback learner – Instead of imputing with the worst possible score, we could train a baseline learner and make predictions from this model.</li>
</ol>
<div class="no-row-height column-margin column-container"><span class="margin-aside">Fallback Learner</span></div><p>We strongly recommend the final option, which is statistically sound and can be easily used in any practical experiment. <code>mlr3</code> includes two baseline learners: <code>lrn("classif.featureless")</code>, which, in its default configuration, always predicts the majority class, and <code>lrn("regr.featureless")</code>, which predicts the average response by default.</p>
<div class="page-columns page-full"><p>To make this procedure convenient during resampling and benchmarking, we support fitting a baseline (though in theory you could use any <code>Learner</code>) as a fallback learner by passing a <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> to <code>$fallback</code>. In the next example, we add a classification baseline to our debug learner, so that when the debug learner errors, <code>mlr3</code> falls back to the predictions of the featureless learner internally. Note that while encapsulation is not enabled explicitly, it is automatically enabled and set to <code>"evaluate"</code> if a fallback learner is added.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">$fallback</span></div></div>
<div class="cell">
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_debug</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.debug"</span>, error_train <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">lrn_debug</span><span class="op">$</span><span class="va">fallback</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.featureless"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">lrn_debug</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_penguins</span><span class="op">)</span></span>
<span><span class="va">lrn_debug</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;LearnerClassifDebug:classif.debug&gt;: Debug Learner for Classification
* Model: -
* Parameters: error_train=1
* Packages: mlr3
* Predict Types:  [response], prob
* Feature Types: logical, integer, numeric, character, factor,
  ordered
* Properties: hotstart_forward, missings, multiclass, twoclass
* Errors: Error from classif.debug-&gt;train()</code></pre>
</div>
</div>
<p>The learner’s log contains the captured error, and although no model is stored as the error was in training, we can still obtain predictions from our fallback:</p>
<div class="cell">
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_debug</span><span class="op">$</span><span class="va">log</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   stage class                               msg
1: train error Error from classif.debug-&gt;train()</code></pre>
</div>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_debug</span><span class="op">$</span><span class="va">model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>NULL</code></pre>
</div>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">prediction</span> <span class="op">=</span> <span class="va">lrn_debug</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">tsk_penguins</span><span class="op">)</span></span>
<span><span class="va">prediction</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>classif.ce 
    0.5581 </code></pre>
</div>
</div>
<p>In the following snippet, we compare the debug learner with a simple classification tree. We re-parametrize the debug learner to fail in roughly 50% of the resampling iterations during the training step:</p>
<div class="cell">
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_debug</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.debug"</span>, error_train <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="va">lrn_debug</span><span class="op">$</span><span class="va">fallback</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.featureless"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">aggr</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/benchmark.html">benchmark</a></span><span class="op">(</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/benchmark_grid.html">benchmark_grid</a></span><span class="op">(</span></span>
<span>  <span class="va">tsk_penguins</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">lrn_debug</span>, <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">20</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span>conditions <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">aggr</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">learner_id</span>, <span class="va">warnings</span>, <span class="va">errors</span>, <span class="va">classif.ce</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      learner_id warnings errors classif.ce
1: classif.debug        0     12    0.61944
2: classif.rpart        0      0    0.05523</code></pre>
</div>
</div>
<p>Even though the debug learner occasionally failed to provide predictions, we still obtained a statistically sound aggregated performance value which we can compare to the aggregated performance of the classification tree. It is also possible to split the benchmark up into separate <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a> objects which sometimes helps to get more context. E.g., if we only want to have a closer look into the debug learner, we can extract the errors from the corresponding resample results:</p>
<div class="cell">
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">rr</span> <span class="op">=</span> <span class="va">aggr</span><span class="op">[</span><span class="va">learner_id</span> <span class="op">==</span> <span class="st">"classif.debug"</span><span class="op">]</span><span class="op">$</span><span class="va">resample_result</span><span class="op">[[</span><span class="fl">1L</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="va">rr</span><span class="op">$</span><span class="va">errors</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   iteration                               msg
1:         2 Error from classif.debug-&gt;train()
2:         4 Error from classif.debug-&gt;train()</code></pre>
</div>
</div>
<p>In summary, combining encapsulation and fallback learners makes it possible to benchmark and tune unreliable or unstable learning algorithms in a convenient and statistically sound fashion.</p>
</section></section><section id="sec-logging" class="level2" data-number="10.3"><h2 data-number="10.3" class="anchored" data-anchor-id="sec-logging">
<span class="header-section-number">10.3</span> Logging</h2>
<p><code>mlr3</code> uses the <a href="https://cran.r-project.org/package=lgr"><code>lgr</code></a> package to control the verbosity of the output, i.e., to decide how much output is shown when <code>mlr3</code> operations are run, from suppression of all non-critical messages to detailed messaging for debugging. In this section, we will cover how to change logging levels, redirect output, and finally change the timing of logging feedback.</p>
<p><code>mlr3</code> uses the following verbosity levels from <code>lgr</code>:</p>
<ul>
<li>
<code>"warn"</code> – Only non-breaking warnings are logged</li>
<li>
<code>"info"</code> – Information such as model runtimes are logged, as well as warnings</li>
<li>
<code>"debug"</code> – Detailed messaging for debugging, as well as information and warnings</li>
</ul>
<p>The default log level in <code>mlr3</code> is <code>"info"</code>, this means that messages are only displayed for messages that are informative or worse, i.e., <code>"info"</code> and <code>"warn"</code>.</p>
<p>To change the logging threshold you need to retrieve the <code>R6</code> logger object from <code>lgr</code>, and then call <code>$set_threshold()</code>, for example, to lower the logging threshold to enable debugging messaging we would change the threshold to <code>"debug"</code>:</p>
<div class="cell">
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">lgr</span><span class="fu">::</span><span class="fu"><a href="https://s-fleck.github.io/lgr/reference/get_logger.html">get_logger</a></span><span class="op">(</span><span class="st">"mlr3"</span><span class="op">)</span><span class="op">$</span><span class="fu">set_threshold</span><span class="op">(</span><span class="st">"debug"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Or to suppress all messaging except warnings:</p>
<div class="cell">
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">lgr</span><span class="fu">::</span><span class="fu"><a href="https://s-fleck.github.io/lgr/reference/get_logger.html">get_logger</a></span><span class="op">(</span><span class="st">"mlr3"</span><span class="op">)</span><span class="op">$</span><span class="fu">set_threshold</span><span class="op">(</span><span class="st">"warn"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>lgr</code> comes with a global option called <code>"lgr.default_threshold"</code> which can be set via <code><a href="https://rdrr.io/r/base/options.html">options()</a></code> to make your choice permanent across sessions (note this will affect all packages using <code>lgr</code>), e.g., <code>options(lgr.default_threshold = "info")</code>.</p>
<p>The packages in <code>mlr3</code> that make use of optimization, i.e., <a href="https://mlr3tuning.mlr-org.com"><code>mlr3tuning</code></a> or <a href="https://mlr3fselect.mlr-org.com"><code>mlr3fselect</code></a>, use the logger of their base package <a href="https://bbotk.mlr-org.com"><code>bbotk</code></a>. This means you could disable “info”-logging from the <code>mlr3</code> logger, but keep the output from <code>mlr3tuning</code>:</p>
<div class="cell">
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">lgr</span><span class="fu">::</span><span class="fu"><a href="https://s-fleck.github.io/lgr/reference/get_logger.html">get_logger</a></span><span class="op">(</span><span class="st">"mlr3"</span><span class="op">)</span><span class="op">$</span><span class="fu">set_threshold</span><span class="op">(</span><span class="st">"warn"</span><span class="op">)</span></span>
<span><span class="fu">lgr</span><span class="fu">::</span><span class="fu"><a href="https://s-fleck.github.io/lgr/reference/get_logger.html">get_logger</a></span><span class="op">(</span><span class="st">"bbotk"</span><span class="op">)</span><span class="op">$</span><span class="fu">set_threshold</span><span class="op">(</span><span class="st">"info"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>By default, output from <code>lgr</code> is printed in the console, however, you could choose to redirect this to a file in various formats, for example to a JSON file:</p>
<div class="cell">
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tf</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/tempfile.html">tempfile</a></span><span class="op">(</span><span class="st">"mlr3log_"</span>, fileext <span class="op">=</span> <span class="st">".json"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># get the logger as R6 object</span></span>
<span><span class="va">logger</span> <span class="op">=</span> <span class="fu">lgr</span><span class="fu">::</span><span class="fu"><a href="https://s-fleck.github.io/lgr/reference/get_logger.html">get_logger</a></span><span class="op">(</span><span class="st">"mlr3"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># add Json appender</span></span>
<span><span class="va">logger</span><span class="op">$</span><span class="fu">add_appender</span><span class="op">(</span><span class="fu">lgr</span><span class="fu">::</span><span class="va"><a href="https://s-fleck.github.io/lgr/reference/AppenderFile.html">AppenderJson</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">tf</span><span class="op">)</span>, name <span class="op">=</span> <span class="st">"json"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># signal a warning</span></span>
<span><span class="va">logger</span><span class="op">$</span><span class="fu">warn</span><span class="op">(</span><span class="st">"this is a warning from mlr3"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>WARN  [11:45:38.135] [mlr3] this is a warning from mlr3</code></pre>
</div>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># print the contents of the file (splitting over two lines)</span></span>
<span><span class="va">x</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/readLines.html">readLines</a></span><span class="op">(</span><span class="va">tf</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/substr.html">substr</a></span><span class="op">(</span><span class="va">x</span>, <span class="fl">1</span>, <span class="fl">71</span><span class="op">)</span>, <span class="st">"\n"</span>, <span class="fu"><a href="https://rdrr.io/r/base/substr.html">substr</a></span><span class="op">(</span><span class="va">x</span>, <span class="fl">72</span>, <span class="fu"><a href="https://rdrr.io/r/base/nchar.html">nchar</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{"level":300,"timestamp":"2024-02-26 11:45:38","logger":"mlr3","caller"
:"eval","msg":"[mlr3] this is a warning from mlr3"}</code></pre>
</div>
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># remove the appender again</span></span>
<span><span class="va">logger</span><span class="op">$</span><span class="fu">remove_appender</span><span class="op">(</span><span class="st">"json"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>See the vignettes in the <code>lgr</code> for more comprehensive examples.</p>
<p>When using parallelization and/or encapsulation, logs may be delayed, out of order, or, in case of some errors, not present at all. When it is necessary to have immediate access to log messages, e.g., when debugging, one may choose to disable <code>future</code> and encapsulation. To enable ‘debug mode’, set <code>options(mlr3.debug = TRUE)</code> and ensure the <code>$encapsulate</code> slot of learners is set to <code>"none"</code> (default) or <code>"evaluate"</code>. Debug mode should only be enabled during debugging and not in production use as it disables parallelization and leads to unexpected RNG behavior that prevents reproducibility.</p>
</section><section id="sec-backends" class="level2" data-number="10.4"><h2 data-number="10.4" class="anchored" data-anchor-id="sec-backends">
<span class="header-section-number">10.4</span> Data Backends</h2>
<p><code>Task</code> objects store their data in an abstract data object, the <a href="https://mlr3.mlr-org.com/reference/DataBackend.html"><code>DataBackend</code></a>. A data backend provides a unified API to retrieve subsets of the data or query information about it, regardless of how the data is stored on the system. The default backend uses <a href="https://cran.r-project.org/package=data.table"><code>data.table</code></a> via the <a href="https://mlr3.mlr-org.com/reference/DataBackendDataTable.html"><code>DataBackendDataTable</code></a> class as a very fast and efficient in-memory database.</p>
<p>While storing the task’s data in memory is most efficient for accessing it for model fitting, there are two major disadvantages:</p>
<ol type="1">
<li>Even if only a small proportion of the data is required, for example when doing subsampling, the complete dataset sits in, and consumes, memory. This is especially a problem if you work with large tasks or many tasks simultaneously, e.g., for benchmarking.</li>
<li>During parallelization (<a href="#sec-parallelization" class="quarto-xref"><span>Section 10.1</span></a>), the complete data needs to be transferred to the workers which can increase the overhead.</li>
</ol>
<p>To avoid these drawbacks, especially for larger data, it can be necessary to interface out-of-memory data to reduce the memory requirements. This way, only the part of the data which is currently required by the learners will be placed in the main memory to operate on. There are multiple options to handle this:</p>
<ol type="1">
<li>
<a href="https://mlr3db.mlr-org.com/reference/DataBackendDplyr.html"><code>DataBackendDplyr</code></a>, which interfaces the R package <a href="https://cran.r-project.org/package=dbplyr"><code>dbplyr</code></a>, extending <a href="https://cran.r-project.org/package=dplyr"><code>dplyr</code></a> to work on many popular SQL databases like <em>MariaDB</em>, <em>PostgresSQL</em>, or <em>SQLite</em>.</li>
<li>
<a href="https://mlr3db.mlr-org.com/reference/DataBackendDuckDB.html"><code>DataBackendDuckDB</code></a> for the <em>DuckDB</em> database connected via <a href="https://cran.r-project.org/package=duckdb"><code>duckdb</code></a>, which is a fast, zero-configuration alternative to SQLite.</li>
<li>
<a href="https://mlr3db.mlr-org.com/reference/DataBackendDuckDB.html"><code>DataBackendDuckDB</code></a> for Parquet files. This means the data does not need to be converted to DuckDB’s native storage format and instead you can work directly on directories containing one or multiple files stored in the popular Parquet format.</li>
</ol>
<p>In the following, we will show how to work with each of these choices using <a href="https://mlr3db.mlr-org.com"><code>mlr3db</code></a>.</p>
<section id="databases-with-databackenddplyr" class="level3" data-number="10.4.1"><h3 data-number="10.4.1" class="anchored" data-anchor-id="databases-with-databackenddplyr">
<span class="header-section-number">10.4.1</span> Databases with DataBackendDplyr</h3>
<p>To demonstrate <a href="https://mlr3db.mlr-org.com/reference/DataBackendDplyr.html"><code>DataBackendDplyr</code></a> we use the (pretty big) NYC flights dataset from the <a href="https://cran.r-project.org/package=nycflights13"><code>nycflights13</code></a> package and move it into a SQLite database. Although <a href="https://mlr3db.mlr-org.com/reference/as_sqlite_backend.html"><code>as_sqlite_backend()</code></a> provides a convenient function to perform this step, we construct the database manually here.</p>
<div class="cell">
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># load data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/ns-load.html">requireNamespace</a></span><span class="op">(</span><span class="st">"DBI"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/ns-load.html">requireNamespace</a></span><span class="op">(</span><span class="st">"RSQLite"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/ns-load.html">requireNamespace</a></span><span class="op">(</span><span class="st">"nycflights13"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"flights"</span>, package <span class="op">=</span> <span class="st">"nycflights13"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">flights</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 336776     19</code></pre>
</div>
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># add column of unique row ids</span></span>
<span><span class="va">flights</span><span class="op">$</span><span class="va">row_id</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">flights</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># create sqlite database in temporary file</span></span>
<span><span class="va">path</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/tempfile.html">tempfile</a></span><span class="op">(</span><span class="st">"flights"</span>, fileext <span class="op">=</span> <span class="st">".sqlite"</span><span class="op">)</span></span>
<span><span class="va">con</span> <span class="op">=</span> <span class="fu">DBI</span><span class="fu">::</span><span class="fu"><a href="https://dbi.r-dbi.org/reference/dbConnect.html">dbConnect</a></span><span class="op">(</span><span class="fu">RSQLite</span><span class="fu">::</span><span class="fu"><a href="https://rsqlite.r-dbi.org/reference/SQLite.html">SQLite</a></span><span class="op">(</span><span class="op">)</span>, <span class="va">path</span><span class="op">)</span></span>
<span><span class="va">tbl</span> <span class="op">=</span> <span class="fu">DBI</span><span class="fu">::</span><span class="fu"><a href="https://dbi.r-dbi.org/reference/dbWriteTable.html">dbWriteTable</a></span><span class="op">(</span><span class="va">con</span>, <span class="st">"flights"</span>, <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="va">flights</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">DBI</span><span class="fu">::</span><span class="fu"><a href="https://dbi.r-dbi.org/reference/dbDisconnect.html">dbDisconnect</a></span><span class="op">(</span><span class="va">con</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># remove in-memory data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/rm.html">rm</a></span><span class="op">(</span><span class="va">flights</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With the SQLite database stored in file <code>path</code>, we now re-establish a connection and switch to <a href="https://cran.r-project.org/package=dplyr"><code>dplyr</code></a>/<a href="https://cran.r-project.org/package=dbplyr"><code>dbplyr</code></a> for some essential preprocessing.</p>
<div class="cell">
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># establish connection</span></span>
<span><span class="va">con</span> <span class="op">=</span> <span class="fu">DBI</span><span class="fu">::</span><span class="fu"><a href="https://dbi.r-dbi.org/reference/dbConnect.html">dbConnect</a></span><span class="op">(</span><span class="fu">RSQLite</span><span class="fu">::</span><span class="fu"><a href="https://rsqlite.r-dbi.org/reference/SQLite.html">SQLite</a></span><span class="op">(</span><span class="op">)</span>, <span class="va">path</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># select the "flights" table</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dbplyr.tidyverse.org/">dbplyr</a></span><span class="op">)</span></span>
<span><span class="va">tbl</span> <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/tbl.html">tbl</a></span><span class="op">(</span><span class="va">con</span>, <span class="st">"flights"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As databases are intended to store large volumes of data, a natural first step is to subset and filter the data to suitable dimensions. Therefore, we build up an SQL query in a step-wise fashion using <code>dplyr</code> verbs and:</p>
<ol type="1">
<li>Select a subset of columns to work on;</li>
<li>Remove observations where the arrival delay (<code>arr_delay</code>) has a missing value;</li>
<li>Filter the data to only use every second row (to reduce example runtime); and</li>
<li>Merge factor levels of the feature <code>carrier</code> so infrequent carriers are replaced by level “other”.</li>
</ol>
<div class="cell">
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># 1. subset columns</span></span>
<span><span class="va">keep</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"row_id"</span>, <span class="st">"year"</span>, <span class="st">"month"</span>, <span class="st">"day"</span>, <span class="st">"hour"</span>, <span class="st">"minute"</span>, <span class="st">"dep_time"</span>,</span>
<span>  <span class="st">"arr_time"</span>, <span class="st">"carrier"</span>, <span class="st">"flight"</span>, <span class="st">"air_time"</span>, <span class="st">"distance"</span>, <span class="st">"arr_delay"</span><span class="op">)</span></span>
<span><span class="va">tbl</span> <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">tbl</span>, <span class="fu"><a href="https://tidyselect.r-lib.org/reference/all_of.html">all_of</a></span><span class="op">(</span><span class="va">keep</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 2. filter by missing</span></span>
<span><span class="va">tbl</span> <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">tbl</span>, <span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">arr_delay</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 3. select every other row</span></span>
<span><span class="va">tbl</span> <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">tbl</span>, <span class="va">row_id</span> <span class="op"><a href="https://rdrr.io/r/base/Arithmetic.html">%%</a></span> <span class="fl">2</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 4. merge infrequent carriers</span></span>
<span><span class="va">infrequent</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"OO"</span>, <span class="st">"HA"</span>, <span class="st">"YV"</span>, <span class="st">"F9"</span>, <span class="st">"AS"</span>, <span class="st">"FL"</span>, <span class="st">"VX"</span>, <span class="st">"WN"</span><span class="op">)</span></span>
<span><span class="va">tbl</span> <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span><span class="va">tbl</span>, carrier <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/case_when.html">case_when</a></span><span class="op">(</span></span>
<span>  <span class="va">carrier</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="va">infrequent</span> <span class="op">~</span> <span class="st">"other"</span>,</span>
<span>  <span class="cn">TRUE</span> <span class="op">~</span> <span class="va">carrier</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Having prepared our data, we can now create a <a href="https://mlr3db.mlr-org.com/reference/DataBackendDplyr.html"><code>DataBackendDplyr</code></a> and can then query basic information from our new <a href="https://mlr3.mlr-org.com/reference/DataBackend.html"><code>DataBackend</code></a>:</p>
<div class="cell">
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https:///mlr3db.mlr-org.com">mlr3db</a></span><span class="op">)</span></span>
<span><span class="va">backend_flights</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/as_data_backend.html">as_data_backend</a></span><span class="op">(</span><span class="va">tbl</span>, primary_key <span class="op">=</span> <span class="st">"row_id"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>nrow <span class="op">=</span> <span class="va">backend_flights</span><span class="op">$</span><span class="va">nrow</span>, ncol <span class="op">=</span> <span class="va">backend_flights</span><span class="op">$</span><span class="va">ncol</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  nrow   ncol 
163707     13 </code></pre>
</div>
<div class="sourceCode" id="cb74"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">backend_flights</span><span class="op">$</span><span class="fu">head</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   row_id year month day hour minute dep_time arr_time carrier flight
1:      2 2013     1   1    5     29      533      850      UA   1714
2:      4 2013     1   1    5     45      544     1004      B6    725
3:      6 2013     1   1    5     58      554      740      UA   1696
4:      8 2013     1   1    6      0      557      709      EV   5708
5:     10 2013     1   1    6      0      558      753      AA    301
6:     12 2013     1   1    6      0      558      853      B6     71
3 variables not shown: [air_time, distance, arr_delay]</code></pre>
</div>
</div>
<p>Note that the <code>DataBackendDplyr</code> can only operate on the data we provided, so does not ‘know’ about the rows and columns we already filtered out (this is in contrast to using <code>$filter</code> and <code>$subset</code> as in <a href="../chapter2/data_and_basic_modeling.html#sec-tasks-mutators" class="quarto-xref"><span>Section 2.1.3</span></a>, which only remove row or column roles and not the rows/columns themselves).</p>
<p>With a backend constructed, we can now use the standard <code>mlr3</code> API:</p>
<div class="cell">
<div class="sourceCode" id="cb76"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_flights</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/as_task_regr.html">as_task_regr</a></span><span class="op">(</span><span class="va">backend_flights</span>, id <span class="op">=</span> <span class="st">"flights_sqlite"</span>,</span>
<span>  target <span class="op">=</span> <span class="st">"arr_delay"</span><span class="op">)</span></span>
<span><span class="va">rsmp_sub002</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"subsampling"</span>, ratio <span class="op">=</span> <span class="fl">0.02</span>, repeats <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Above we created a regression task by passing a backend as the first argument and then created a resampling strategy where we will subsample 2% of the observations three times. In each resampling iteration, only the required subset of the data is queried from the SQLite database and passed to our learner:</p>
<div class="cell">
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">rr</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/resample.html">resample</a></span><span class="op">(</span><span class="va">tsk_flights</span>, <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"regr.rpart"</span><span class="op">)</span>, <span class="va">rsmp_sub002</span><span class="op">)</span></span>
<span><span class="va">measures</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msrs</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"regr.rmse"</span>, <span class="st">"time_train"</span>, <span class="st">"time_predict"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">rr</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span><span class="va">measures</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   regr.rmse   time_train time_predict 
      35.954        1.332       15.662 </code></pre>
</div>
</div>
<p>As we have finished our experiment we can now close our connection, which we can do by removing the <code>tbl</code> object referencing the connection and then closing it.</p>
<div class="cell">
<div class="sourceCode" id="cb79"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/rm.html">rm</a></span><span class="op">(</span><span class="va">tbl</span><span class="op">)</span></span>
<span><span class="fu">DBI</span><span class="fu">::</span><span class="fu"><a href="https://dbi.r-dbi.org/reference/dbDisconnect.html">dbDisconnect</a></span><span class="op">(</span><span class="va">con</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="parquet-files-with-databackendduckdb" class="level3" data-number="10.4.2"><h3 data-number="10.4.2" class="anchored" data-anchor-id="parquet-files-with-databackendduckdb">
<span class="header-section-number">10.4.2</span> Parquet Files with DataBackendDuckDB</h3>
<p>DuckDB databases provide a modern alternative to SQLite, tailored to the needs of ML. Parquet is a popular column-oriented data storage format supporting efficient compression, making it far superior to other popular data exchange formats such as CSV.</p>
<p>Converting a <code>data.frame</code> to DuckDB is possible by passing the <code>data.frame</code> to convert and the <code>path</code> to store the data to <a href="https://mlr3db.mlr-org.com/reference/as_duckdb_backend.html"><code>as_duckdb_backend()</code></a>. By example, below we first query the location of an example dataset in a Parquet file shipped with <code>mlr3db</code> and then convert the resulting <a href="https://mlr3db.mlr-org.com/reference/DataBackendDuckDB.html"><code>DataBackendDuckDB</code></a> object into a classification task, all without loading the dataset into memory:</p>
<div class="cell">
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">path</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/system.file.html">system.file</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/file.path.html">file.path</a></span><span class="op">(</span><span class="st">"extdata"</span>, <span class="st">"spam.parquet"</span><span class="op">)</span>,</span>
<span>  package <span class="op">=</span> <span class="st">"mlr3db"</span><span class="op">)</span></span>
<span><span class="va">backend</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3db.mlr-org.com/reference/as_duckdb_backend.html">as_duckdb_backend</a></span><span class="op">(</span><span class="va">path</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/as_task_classif.html">as_task_classif</a></span><span class="op">(</span><span class="va">backend</span>, target <span class="op">=</span> <span class="st">"type"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;TaskClassif:backend&gt; (4601 x 58)
* Target: type
* Properties: twoclass
* Features (57):
  - dbl (57): address, addresses, all, business, capitalAve,
    capitalLong, capitalTotal, charDollar, charExclamation,
    charHash, charRoundbracket, charSemicolon,
    charSquarebracket, conference, credit, cs, data, direct,
    edu, email, font, free, george, hp, hpl, internet, lab,
    labs, mail, make, meeting, money, num000, num1999, num3d,
    num415, num650, num85, num857, order, original, our, over,
    parts, people, pm, project, re, receive, remove, report,
    table, technology, telnet, will, you, your</code></pre>
</div>
</div>
<p>Accessing the data internally triggers a query and the required subsets of data are fetched to be stored in an in-memory <code>data.frame</code>. After the retrieved data is processed, the garbage collector can release the occupied memory. The backend can also operate on a folder with multiple parquet files.</p>
</section></section><section id="sec-extending" class="level2" data-number="10.5"><h2 data-number="10.5" class="anchored" data-anchor-id="sec-extending">
<span class="header-section-number">10.5</span> Extending mlr3 and Defining a New <code>Measure</code>
</h2>
<p>After getting this far in the book you are well on your way to being an <code>mlr3</code> expert and may even want to add more classes to our universe. While many classes could be extended, all have a similar design interface and so, we will only demonstrate how to create a custom <a href="https://mlr3.mlr-org.com/reference/Measure.html"><code>Measure</code></a>. If you are interested in implementing new learners, <code>PipeOp</code>s, or tuners, then check out the vignettes in the respective packages: <a href="https://mlr3extralearners.mlr-org.com"><code>mlr3extralearners</code></a>, <a href="https://mlr3pipelines.mlr-org.com"><code>mlr3pipelines</code></a>, or <a href="https://mlr3tuning.mlr-org.com"><code>mlr3tuning</code></a>. If you are considering creating a package that adds an entirely new task type then feel free to contact us for some support via GitHub, email, or Mattermost. This section assumes good knowledge of <code>R6</code>, see <a href="../chapter1/introduction_and_overview.html#sec-r6" class="quarto-xref"><span>Section 1.5.1</span></a> for a brief introduction and references to further resources.</p>
<p>As an example, let us consider a regression measure that scores a prediction as <code>1</code> if the difference between the true and predicted values is less than one standard deviation of the truth, or scores the prediction as <code>0</code> otherwise. In maths this would be defined as <span class="math inline">\(f(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^n \mathbb{I}(|y_i - \hat{y}_i| &lt; \sigma_y)\)</span>, where <span class="math inline">\(\sigma_y\)</span> is the standard deviation of the truth and <span class="math inline">\(\mathbb{I}\)</span> is the indicator function. In code, this measure may be written as:</p>
<div class="cell">
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">threshold_acc</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">truth</span>, <span class="va">response</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">truth</span> <span class="op">-</span> <span class="va">response</span><span class="op">)</span> <span class="op">&lt;</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">truth</span><span class="op">)</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu">threshold_acc</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">100</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">11</span>, <span class="fl">6</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6667</code></pre>
</div>
</div>
<p>By definition of this measure, its values are bounded in <span class="math inline">\([0, 1]\)</span> where a perfect score of <span class="math inline">\(1\)</span> would mean all predictions are within a standard deviation of the truth, hence for this measure larger scores are better.</p>
<p>To use this measure in <code>mlr3</code>, we need to create a new <a href="https://www.rdocumentation.org/packages/R6/topics/R6Class"><code>R6Class</code></a>, which will inherit from <code>Measure</code> and in this case specifically from <a href="https://mlr3.mlr-org.com/reference/MeasureRegr.html"><code>MeasureRegr</code></a>. The code for this new measure is in the snippet below, with an explanation following it. This code chunk can be used as a template for the majority of performance measures.</p>
<div class="cell">
<div class="sourceCode" id="cb84"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">MeasureRegrThresholdAcc</span> <span class="op">=</span> <span class="fu">R6</span><span class="fu">::</span><span class="kw"><a href="https://r6.r-lib.org/reference/R6Class.html">R6Class</a></span><span class="op">(</span><span class="st">"MeasureRegrThresholdAcc"</span>,</span>
<span>  inherit <span class="op">=</span> <span class="fu">mlr3</span><span class="fu">::</span><span class="va"><a href="https://mlr3.mlr-org.com/reference/MeasureRegr.html">MeasureRegr</a></span>, <span class="co"># regression measure</span></span>
<span>  public <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>    initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span> <span class="co"># initialize class</span></span>
<span>      <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span></span>
<span>        id <span class="op">=</span> <span class="st">"thresh_acc"</span>, <span class="co"># unique ID</span></span>
<span>        packages <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/character.html">character</a></span><span class="op">(</span><span class="op">)</span>, <span class="co"># no package dependencies</span></span>
<span>        properties <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/character.html">character</a></span><span class="op">(</span><span class="op">)</span>, <span class="co">#&nbsp;no special properties</span></span>
<span>        predict_type <span class="op">=</span> <span class="st">"response"</span>, <span class="co"># measures response prediction</span></span>
<span>        range <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>, <span class="co"># results in values between (0, 1)</span></span>
<span>        minimize <span class="op">=</span> <span class="cn">FALSE</span> <span class="co"># larger values are better</span></span>
<span>      <span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">)</span>,</span>
<span></span>
<span>  private <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>    <span class="co"># define score as private method</span></span>
<span>    .score <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">prediction</span>, <span class="va">...</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="co"># define loss</span></span>
<span>      <span class="va">threshold_acc</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">truth</span>, <span class="va">response</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">truth</span> <span class="op">-</span> <span class="va">response</span><span class="op">)</span> <span class="op">&lt;</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">truth</span><span class="op">)</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span>      <span class="op">}</span></span>
<span>      <span class="co"># call loss function</span></span>
<span>      <span class="fu">threshold_acc</span><span class="op">(</span><span class="va">prediction</span><span class="op">$</span><span class="va">truth</span>, <span class="va">prediction</span><span class="op">$</span><span class="va">response</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol type="1">
<li>In the first two lines we name the class, here <code>MeasureRegrThresholdAcc</code>, and then state this is a regression measure that inherits from <code>MeasureRegr</code>.</li>
<li>We initialize the class by stating its unique ID is <code>"thresh_acc"</code>, that it does not require any external packages (<code>packages = character()</code>) and that it has no special properties (<code>properties = character()</code>).</li>
<li>We then pass specific details of the loss function which are: it measures the quality of a <code>"response"</code> type prediction, its values range between <code>(0, 1)</code>, and that the loss is optimized as its maximum (<code>minimize = FALSE</code>).</li>
<li>Finally, we define the score itself as a private method called <code>.score</code> where we pass the predictions to the function we defined just above.</li>
</ol>
<p>Sometimes measures require data from the training set, the task, or the learner. These are usually complex edge-cases examples, so we will not go into detail here, for working examples we suggest looking at the code for <a href="https://mlr3proba.mlr-org.com/reference/MeasureSurvSongAUC.html"><code>MeasureSurvSongAUC</code></a> and <a href="https://mlr3proba.mlr-org.com/reference/MeasureSurvAUC.html"><code>MeasureSurvAUC</code></a>. You can also consult the manual page of the <code>Measure</code> for an overview of other properties and meta-data that can be specified.</p>
<p>Once you have defined your measure you can load it with the <code>R6</code> constructor (<code>$new()</code>), or make it available to be constructed with the <code><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr()</a></code> sugar function by adding it to the <a href="https://mlr3.mlr-org.com/reference/mlr_measures.html"><code>mlr_measures</code></a> dictionary:</p>
<div class="cell">
<div class="sourceCode" id="cb85"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_mtcars</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"mtcars"</span><span class="op">)</span></span>
<span><span class="va">split</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/partition.html">partition</a></span><span class="op">(</span><span class="va">tsk_mtcars</span><span class="op">)</span></span>
<span><span class="va">lrn_featureless</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"regr.featureless"</span><span class="op">)</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_mtcars</span>, <span class="va">split</span><span class="op">$</span><span class="va">train</span><span class="op">)</span></span>
<span><span class="va">prediction</span> <span class="op">=</span> <span class="va">lrn_featureless</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">tsk_mtcars</span>, <span class="va">split</span><span class="op">$</span><span class="va">test</span><span class="op">)</span></span>
<span><span class="va">prediction</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span><span class="va">MeasureRegrThresholdAcc</span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>thresh_acc 
    0.7273 </code></pre>
</div>
<div class="sourceCode" id="cb87"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># or add to dictionary by passing a unique key to the first argument</span></span>
<span><span class="co">#  and the class to the second</span></span>
<span><span class="fu">mlr3</span><span class="fu">::</span><span class="va"><a href="https://mlr3.mlr-org.com/reference/mlr_measures.html">mlr_measures</a></span><span class="op">$</span><span class="fu">add</span><span class="op">(</span><span class="st">"regr.thresh_acc"</span>, <span class="va">MeasureRegrThresholdAcc</span><span class="op">)</span></span>
<span><span class="va">prediction</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr</a></span><span class="op">(</span><span class="st">"regr.thresh_acc"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>thresh_acc 
    0.7273 </code></pre>
</div>
</div>
<p>While we only covered how to create a simple regression measure, the process of adding other classes to our universe is in essence the same:</p>
<ol type="1">
<li>Find the right class to inherit from</li>
<li>Add methods that:
<ol type="a">
<li>Initialize the object with the correct properties (<code>$initialize()</code>).</li>
<li>Implement the public and private methods that do the actual computation. In the above example, this was the private <code>$.score()</code> method.</li>
</ol>
</li>
</ol>
<p>We are always happy to chat and welcome new contributors, please get in touch if you need assistance in extending <code>mlr3</code>.</p>
</section><section id="conclusion" class="level2" data-number="10.6"><h2 data-number="10.6" class="anchored" data-anchor-id="conclusion">
<span class="header-section-number">10.6</span> Conclusion</h2>
<p>This chapter covered several advanced topics including parallelization, error handling, logging, working with databases, and extending the <code>mlr3</code> universe. For simple use cases, you will probably not need to know each of these topics in detail, however, we do recommend being familiar at least with error handling and fallback learners, as these are essential to preventing even simple experiments being interrupted. If you are working with large experiments or datasets, then understanding parallelization, logging, and databases will also be essential.</p>
<p>We have not covered any of these topics extensively and therefore recommended the following resources should you want to read more about these areas. If you are interested to learn more about parallelization in R, we recommend <span class="citation" data-cites="Schmidberger2009">Schmidberger et al. (<a href="../references.html#ref-Schmidberger2009" role="doc-biblioref">2009</a>)</span> and <span class="citation" data-cites="Eddelbuettel2020">Eddelbuettel (<a href="../references.html#ref-Eddelbuettel2020" role="doc-biblioref">2020</a>)</span>. To find out more about logging, have a read of the vignettes in <code>lgr</code>, which cover everything from logging to JSON files to retrieving logged objects for debugging. For an overview of available DBMS in R, see the CRAN task view on databases at <a href="https://cran.r-project.org/view=Databases">https://cran.r-project.org/view=Databases</a>, and in particular the vignettes of the <code>dbplyr</code> package for DBMS readily available in <code>mlr3</code>.</p>
<div id="tbl-technical-api" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-technical-api-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;10.1: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable).
</figcaption><div aria-describedby="tbl-technical-api-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<thead><tr class="header">
<th>Class</th>
<th>Constructor/Function</th>
<th>Fields/Methods</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>-</td>
<td><a href="https://www.rdocumentation.org/packages/future/topics/plan"><code>plan()</code></a></td>
<td>-</td>
</tr>
<tr class="even">
<td>-</td>
<td><a href="https://mlr3.mlr-org.com/reference/set_threads.html"><code>set_threads()</code></a></td>
<td>-</td>
</tr>
<tr class="odd">
<td>-</td>
<td><a href="https://www.rdocumentation.org/packages/future/topics/tweak"><code>tweak()</code></a></td>
<td>-</td>
</tr>
<tr class="even">
<td><code>Learner</code></td>
<td><code><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn()</a></code></td>
<td>
<code>$encapsulate</code>; <code>$fallback</code>; <code>$timeout</code>; <code>$parallel_predict</code>; <code>$log</code>
</td>
</tr>
<tr class="odd">
<td><a href="https://www.rdocumentation.org/packages/lgr/topics/Logger"><code>Logger</code></a></td>
<td><a href="https://www.rdocumentation.org/packages/lgr/topics/get_logger"><code>get_logger</code></a></td>
<td><code>$set_threshold()</code></td>
</tr>
<tr class="even">
<td><a href="https://mlr3db.mlr-org.com/reference/DataBackendDplyr.html"><code>DataBackendDplyr</code></a></td>
<td><a href="https://mlr3.mlr-org.com/reference/as_data_backend.html"><code>as_data_backend</code></a></td>
<td>-</td>
</tr>
<tr class="odd">
<td><a href="https://mlr3db.mlr-org.com/reference/DataBackendDuckDB.html"><code>DataBackendDuckDB</code></a></td>
<td><a href="https://mlr3db.mlr-org.com/reference/as_duckdb_backend.html"><code>as_duckdb_backend</code></a></td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section><section id="exercises" class="level2" data-number="10.7"><h2 data-number="10.7" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">10.7</span> Exercises</h2>
<ol type="1">
<li>Consider the following example where you resample a learner (debug learner, sleeps for three seconds during train) on four workers using the multisession backend:</li>
</ol>
<div class="cell">
<div class="sourceCode" id="cb89"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_penguins</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"penguins"</span><span class="op">)</span></span>
<span><span class="va">lrn_debug</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.debug"</span>, sleep_train <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">rsmp_cv6</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">6</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">future</span><span class="fu">::</span><span class="fu"><a href="https://future.futureverse.org/reference/plan.html">plan</a></span><span class="op">(</span><span class="st">"multisession"</span>, workers <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/resample.html">resample</a></span><span class="op">(</span><span class="va">tsk_penguins</span>, <span class="va">lrn_debug</span>, <span class="va">rsmp_cv6</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol type="a">
<li>Assuming you were running this experiment on a computer with four CPUs, and that the learner would actually calculate something and not just sleep: Would all CPUs be busy for the entire time of this calculation?</li>
<li>Prove your point by measuring the elapsed time, e.g., using <a href="https://www.rdocumentation.org/packages/base/topics/system.time"><code>system.time()</code></a>.</li>
<li>What would you change in the setup and why?</li>
</ol>
<ol start="2" type="1">
<li><p>Create a new custom binary classification measure which scores (“prob”-type) predictions. This measure should compute the absolute difference between the predicted probability for the positive class and a 0-1 encoding of the ground truth and then average these values across the test set. Test this with <code>classif.log_reg</code> on <code>tsk(“sonar”)</code>.</p></li>
<li><p>“Tune” the <code>error_train</code> hyperparameter of the <code>classif.debug</code> learner on a continuous interval from 0 to 1, using a simple classification tree as the fallback learner and the penguins task. Tune for 50 iterations using random search and 10-fold cross-validation. Inspect the resulting archive and find out which evaluations resulted in an error, and which did not. Now do the same in the interval 0.3 to 0.7. Are your results surprising?</p></li>
</ol></section><section id="citation" class="level2" data-number="10.8"><h2 data-number="10.8" class="anchored" data-anchor-id="citation">
<span class="header-section-number">10.8</span> Citation</h2>
<p>Please cite this chapter as:</p>
<p>Lang M, Fischer S, Sonabend R. (2024). Advanced Technical Aspects of mlr3. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), <em>Applied Machine Learning Using mlr3 in R</em>. CRC Press. https://mlr3book.mlr-org.com/advanced_technical_aspects_of_mlr3.html.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode bibtex code-with-copy"><code class="sourceCode bibtex"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="va">@incollection</span>{<span class="ot">citekey</span>, </span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">author</span> = "<span class="st">Michel Lang and Sebastian Fischer and Raphael Sonabend</span>", </span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">title</span> = "<span class="st">Advanced Technical Aspects of mlr3</span>",</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">booktitle</span> = "<span class="st">Applied Machine Learning Using {m}lr3 in {R}</span>",</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">publisher</span> = "<span class="st">CRC Press</span>", <span class="st">year</span> = "<span class="st">2024</span>",</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">editor</span> = "<span class="st">Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang</span>", </span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">url</span> = "<span class="st">https://mlr3book.mlr-org.com/advanced_technical_aspects_of_mlr3.html</span>"</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-future119" class="csl-entry" role="listitem">
Bengtsson, Henrik. 2020. <span>“Future 1.19.1 - Making Sure Proper Random Numbers Are Produced in Parallel Processing.”</span> <a href="https://www.jottr.org/2020/09/22/push-for-statistical-sound-rng/" class="uri">https://www.jottr.org/2020/09/22/push-for-statistical-sound-rng/</a>.
</div>
<div id="ref-avoiddetect" class="csl-entry" role="listitem">
———. 2022. <span>“Please Avoid detectCores() in Your <span>R</span> Packages.”</span> <a href="https://www.jottr.org/2022/12/05/avoid-detectcores/" class="uri">https://www.jottr.org/2022/12/05/avoid-detectcores/</a>.
</div>
<div id="ref-hpo_practical" class="csl-entry" role="listitem">
Bischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, et al. 2023. <span>“Hyperparameter Optimization: Foundations, Algorithms, Best Practices, and Open Challenges.”</span> <em>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</em>, e1484. <a href="https://doi.org/10.1002/widm.1484">https://doi.org/10.1002/widm.1484</a>.
</div>
<div id="ref-Eddelbuettel2020" class="csl-entry" role="listitem">
Eddelbuettel, Dirk. 2020. <span>“Parallel Computing with <span>R</span>: A Brief Review.”</span> <em><span>WIREs</span> Computational Statistics</em> 13 (2). <a href="https://doi.org/10.1002/wics.1515">https://doi.org/10.1002/wics.1515</a>.
</div>
<div id="ref-Schmidberger2009" class="csl-entry" role="listitem">
Schmidberger, Markus, Martin Morgan, Dirk Eddelbuettel, Hao Yu, Luke Tierney, and Ulrich Mansmann. 2009. <span>“State of the Art in Parallel Computing with <span>R</span>.”</span> <em>Journal of Statistical Software</em> 31 (1). <a href="https://doi.org/10.18637/jss.v031.i01">https://doi.org/10.18637/jss.v031.i01</a>.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/chapter9/preprocessing.html" class="pagination-link" aria-label="Preprocessing">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Preprocessing</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/chapter11/large-scale_benchmarking.html" class="pagination-link" aria-label="Large-Scale Benchmarking">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Large-Scale Benchmarking</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb91" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="an">aliases:</span></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a><span class="co">  - "/advanced_technical_aspects_of_mlr3.html"</span></span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a><span class="fu"># Advanced Technical Aspects of mlr3 {#sec-technical}</span></span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../common/_setup.qmd &gt;}}</span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a><span class="in">`r chapter = "Advanced Technical Aspects of mlr3"`</span></span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a><span class="in">`r authors(chapter)`</span></span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a>In the previous chapters, we demonstrated how to turn machine learning concepts and methods into code.</span>
<span id="cb91-14"><a href="#cb91-14" aria-hidden="true" tabindex="-1"></a>In this chapter we will turn to those technical details that can be important for more advanced uses of <span class="in">`r mlr3`</span>, including:</span>
<span id="cb91-15"><a href="#cb91-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-16"><a href="#cb91-16" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r index('Parallelization')`</span> with the <span class="in">`r ref_pkg("future")`</span> framework (@sec-parallelization);</span>
<span id="cb91-17"><a href="#cb91-17" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Error handling and <span class="in">`r index('debugging')`</span> (@sec-error-handling);</span>
<span id="cb91-18"><a href="#cb91-18" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Adjusting the logger to your needs (@sec-logging);</span>
<span id="cb91-19"><a href="#cb91-19" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Working with out-of-memory data, e.g., data stored in databases (@sec-backends); and</span>
<span id="cb91-20"><a href="#cb91-20" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Adding new classes to <span class="in">`mlr3`</span> (@sec-extending).</span>
<span id="cb91-21"><a href="#cb91-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-22"><a href="#cb91-22" aria-hidden="true" tabindex="-1"></a><span class="fu">## Parallelization {#sec-parallelization}</span></span>
<span id="cb91-23"><a href="#cb91-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-24"><a href="#cb91-24" aria-hidden="true" tabindex="-1"></a>The term <span class="in">`r index("parallelization")`</span> refers to running multiple algorithms in parallel, i.e., executing them simultaneously on multiple CPU cores, CPUs, or computational nodes.</span>
<span id="cb91-25"><a href="#cb91-25" aria-hidden="true" tabindex="-1"></a>Not all algorithms can be parallelized, but when they can, parallelization allows significant savings in computation time.</span>
<span id="cb91-26"><a href="#cb91-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-27"><a href="#cb91-27" aria-hidden="true" tabindex="-1"></a>In general, there are many possibilities to parallelize, depending on the hardware to run the computations.</span>
<span id="cb91-28"><a href="#cb91-28" aria-hidden="true" tabindex="-1"></a>If you only have a single CPU with multiple cores, then *threads* or *processes* are ways to utilize all cores on a local machine.</span>
<span id="cb91-29"><a href="#cb91-29" aria-hidden="true" tabindex="-1"></a>If you have multiple machines on the other hand, they can communicate and exchange information via protocols such as *network sockets* or the *Message Passing Interface*.</span>
<span id="cb91-30"><a href="#cb91-30" aria-hidden="true" tabindex="-1"></a>Larger computational sites rely on scheduling systems to orchestrate the computation for multiple users and usually offer a shared network file system all machines can access.</span>
<span id="cb91-31"><a href="#cb91-31" aria-hidden="true" tabindex="-1"></a>Interacting with scheduling systems on compute clusters is covered in @sec-hpc-exec using the R package <span class="in">`r ref_pkg("batchtools")`</span>.</span>
<span id="cb91-32"><a href="#cb91-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-33"><a href="#cb91-33" aria-hidden="true" tabindex="-1"></a>There are a few pieces of terminology associated with parallelization that we will use in this section:</span>
<span id="cb91-34"><a href="#cb91-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-35"><a href="#cb91-35" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The <span class="in">`r index("parallelization backend", aside = TRUE)`</span> is the hardware to parallelize with a respective interface provided by an R package.</span>
<span id="cb91-36"><a href="#cb91-36" aria-hidden="true" tabindex="-1"></a>  Many parallelization backends have different APIs, so we use the <span class="in">`r ref_pkg("future")`</span> package as a unified, abstraction layer for many parallelization backends.</span>
<span id="cb91-37"><a href="#cb91-37" aria-hidden="true" tabindex="-1"></a>  From a user perspective, <span class="in">`mlr3`</span> interfaces with <span class="in">`future`</span> directly so all you will need to do is configure the backend before starting any computations.</span>
<span id="cb91-38"><a href="#cb91-38" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The Main process is the R session or process that orchestrates the computational work, called jobs.</span>
<span id="cb91-39"><a href="#cb91-39" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Workers are the R sessions, processes, or machines that receive the jobs, perform calculations, and then send the results back to Main.</span>
<span id="cb91-40"><a href="#cb91-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-41"><a href="#cb91-41" aria-hidden="true" tabindex="-1"></a>An important step in parallel programming involves the identification of sections of the program flow that are both time-consuming ('bottlenecks') and can run independently of a different section, i.e., section A's operations are not dependent on the results of section B's operations, and vice versa.</span>
<span id="cb91-42"><a href="#cb91-42" aria-hidden="true" tabindex="-1"></a>Fortunately, these sections are usually relatively easy to spot for machine learning experiments:</span>
<span id="cb91-43"><a href="#cb91-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-44"><a href="#cb91-44" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Training of a learning algorithm (or other computationally intensive parts of a machine learning pipeline) *may* contain independent sections which can run in parallel, e.g.</span>
<span id="cb91-45"><a href="#cb91-45" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>A single <span class="in">`r index('decision tree')`</span> iterates over all features to find the best split point, for each feature independently.</span>
<span id="cb91-46"><a href="#cb91-46" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>A <span class="in">`r index('random forest')`</span> usually fits hundreds of trees independently.</span>
<span id="cb91-47"><a href="#cb91-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-48"><a href="#cb91-48" aria-hidden="true" tabindex="-1"></a>   The key principle that makes parallelization possible for these examples (and in general in many fields of statistics and ML) is called <span class="in">`r index("data parallelism", aside = TRUE)`</span>, which means the same operation is performed concurrently on different elements of the input data.</span>
<span id="cb91-49"><a href="#cb91-49" aria-hidden="true" tabindex="-1"></a>   Parallelization of learning algorithms is covered in @sec-parallel-learner.</span>
<span id="cb91-50"><a href="#cb91-50" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Resampling consists of independent repetitions of train-test-splits and benchmarking consists of multiple independent resamplings (@sec-parallel-resample).</span>
<span id="cb91-51"><a href="#cb91-51" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Tuning (@sec-optimization) often is iterated benchmarking, embedded in a sequential procedure that determines the hyperparameter configurations to try next. While many tuning algorithms are inherently sequential to some degree, there are some (e.g., random search) that can propose multiple configurations in parallel to be evaluated independently, providing another level for parallelization (@sec-nested-resampling-parallelization).</span>
<span id="cb91-52"><a href="#cb91-52" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Predictions of a single learner for multiple observations can be computed independently (@sec-parallel-predict).</span>
<span id="cb91-53"><a href="#cb91-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-54"><a href="#cb91-54" aria-hidden="true" tabindex="-1"></a>These examples are referred to as "<span class="in">`r index("embarrassingly parallel", aside = TRUE)`</span>" as they are so easy to parallelize.</span>
<span id="cb91-55"><a href="#cb91-55" aria-hidden="true" tabindex="-1"></a>If we can formulate the problem as a function that can be passed to map-like functions such as <span class="in">`r ref("lapply()")`</span>, then you have an embarrassingly parallel problem.</span>
<span id="cb91-56"><a href="#cb91-56" aria-hidden="true" tabindex="-1"></a>However, just because a problem *can* be parallelized, it does not follow that every operation in a problem *should* be parallelized.</span>
<span id="cb91-57"><a href="#cb91-57" aria-hidden="true" tabindex="-1"></a>Starting and terminating workers as well as possible communication between workers comes at a price in the form of additionally required runtime which is called <span class="in">`r index("parallelization overhead", aside = TRUE)`</span>.</span>
<span id="cb91-58"><a href="#cb91-58" aria-hidden="true" tabindex="-1"></a>This overhead strongly varies between parallelization backends and must be carefully weighed against the runtime of the sequential execution to determine if parallelization is worth the effort.</span>
<span id="cb91-59"><a href="#cb91-59" aria-hidden="true" tabindex="-1"></a>If the sequential execution is comparably fast, enabling parallelization may introduce additional complexity with little runtime savings, or could even slow down the execution.</span>
<span id="cb91-60"><a href="#cb91-60" aria-hidden="true" tabindex="-1"></a>It is possible to control the <span class="in">`r index("granularity", aside = TRUE)`</span> of the parallelization to reduce the parallelization overhead.</span>
<span id="cb91-61"><a href="#cb91-61" aria-hidden="true" tabindex="-1"></a>For example, we could reduce the overhead of parallelizing a <span class="in">`for`</span>-loop with 1000 iterations on four CPU cores by <span class="in">`r index('chunking')`</span> the work of the 1000 jobs into four computational jobs performing 250 iterations each, resulting in four big jobs and not 1000 small ones.</span>
<span id="cb91-62"><a href="#cb91-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-63"><a href="#cb91-63" aria-hidden="true" tabindex="-1"></a>This effect is illustrated in the following code chunk using a <span class="in">`r index('socket cluster')`</span> with the <span class="in">`r ref_pkg("parallel")`</span> package, which has a <span class="in">`chunk.size`</span> option so we do not need to manually create chunks:</span>
<span id="cb91-64"><a href="#cb91-64" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-001, eval = TRUE}</span></span>
<span id="cb91-65"><a href="#cb91-65" aria-hidden="true" tabindex="-1"></a><span class="in"># set up a socket cluster with 4 workers on the local machine</span></span>
<span id="cb91-66"><a href="#cb91-66" aria-hidden="true" tabindex="-1"></a><span class="in">library(parallel)</span></span>
<span id="cb91-67"><a href="#cb91-67" aria-hidden="true" tabindex="-1"></a><span class="in">cores = 4</span></span>
<span id="cb91-68"><a href="#cb91-68" aria-hidden="true" tabindex="-1"></a><span class="in">cl = makeCluster(cores)</span></span>
<span id="cb91-69"><a href="#cb91-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-70"><a href="#cb91-70" aria-hidden="true" tabindex="-1"></a><span class="in"># vector to operate on</span></span>
<span id="cb91-71"><a href="#cb91-71" aria-hidden="true" tabindex="-1"></a><span class="in">x = 1:10000</span></span>
<span id="cb91-72"><a href="#cb91-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-73"><a href="#cb91-73" aria-hidden="true" tabindex="-1"></a><span class="in"># fast function to parallelize</span></span>
<span id="cb91-74"><a href="#cb91-74" aria-hidden="true" tabindex="-1"></a><span class="in">f = function(y) sqrt(y + 1)</span></span>
<span id="cb91-75"><a href="#cb91-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-76"><a href="#cb91-76" aria-hidden="true" tabindex="-1"></a><span class="in"># unchunked approach: 1000 jobs</span></span>
<span id="cb91-77"><a href="#cb91-77" aria-hidden="true" tabindex="-1"></a><span class="in">system.time({parSapply(cl, x, f, chunk.size = 1)})</span></span>
<span id="cb91-78"><a href="#cb91-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-79"><a href="#cb91-79" aria-hidden="true" tabindex="-1"></a><span class="in"># chunked approach: 4 jobs</span></span>
<span id="cb91-80"><a href="#cb91-80" aria-hidden="true" tabindex="-1"></a><span class="in">system.time({parSapply(cl, x, f, chunk.size = 2500)})</span></span>
<span id="cb91-81"><a href="#cb91-81" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-82"><a href="#cb91-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-83"><a href="#cb91-83" aria-hidden="true" tabindex="-1"></a>Whenever you have the option to control the granularity by setting the chunk size, you should aim for at least as many jobs as workers.</span>
<span id="cb91-84"><a href="#cb91-84" aria-hidden="true" tabindex="-1"></a>However, if there are too few job chunks with strongly dissimilar runtimes, the system may end up waiting for the last chunk to finish, while other resources are idle.</span>
<span id="cb91-85"><a href="#cb91-85" aria-hidden="true" tabindex="-1"></a>This is referred to as <span class="in">`r index("synchronization overhead", aside = TRUE)`</span>.</span>
<span id="cb91-86"><a href="#cb91-86" aria-hidden="true" tabindex="-1"></a>You should therefore aim for chunks with a runtime of at least several seconds, so that the parallelization overhead remains reasonable, while still having enough chunks to ensure that you can fully utilize the system.</span>
<span id="cb91-87"><a href="#cb91-87" aria-hidden="true" tabindex="-1"></a>If you have heterogeneous runtimes, you can consider grouping jobs so that the runtimes of the chunks are more homogeneous.</span>
<span id="cb91-88"><a href="#cb91-88" aria-hidden="true" tabindex="-1"></a>If runtimes can be estimated, then both <span class="in">`batchtools::binpack()`</span> and <span class="in">`batchtools::lpt()`</span> (documented together with the <span class="in">`r ref("batchtools::chunk()")`</span> function) are useful for chunking jobs.</span>
<span id="cb91-89"><a href="#cb91-89" aria-hidden="true" tabindex="-1"></a>If runtimes cannot be estimated, then it can be useful to randomize the order of jobs.</span>
<span id="cb91-90"><a href="#cb91-90" aria-hidden="true" tabindex="-1"></a>Otherwise jobs could be accidentally ordered by runtime, for example because they are sorted by a hyperparameter that has a strong influence on training time.</span>
<span id="cb91-91"><a href="#cb91-91" aria-hidden="true" tabindex="-1"></a>Naively chunking jobs could then lead to some chunks containing much more expensive jobs than others, resulting in avoidable underutilization of resources.</span>
<span id="cb91-92"><a href="#cb91-92" aria-hidden="true" tabindex="-1"></a><span class="in">`r mlr3misc`</span> ships with the functions <span class="in">`r ref("mlr3misc::chunk()")`</span> and <span class="in">`r ref("chunk_vector()")`</span> that conveniently chunk jobs and also shuffle them by default.</span>
<span id="cb91-93"><a href="#cb91-93" aria-hidden="true" tabindex="-1"></a>There are also options to control the chunk size for parallelization in <span class="in">`mlr3`</span>, which are discussed in @sec-parallel-resample.</span>
<span id="cb91-94"><a href="#cb91-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-95"><a href="#cb91-95" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb91-96"><a href="#cb91-96" aria-hidden="true" tabindex="-1"></a><span class="fu"># Reproducibility</span></span>
<span id="cb91-97"><a href="#cb91-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-98"><a href="#cb91-98" aria-hidden="true" tabindex="-1"></a>Reproducibility is often a concern during parallelization because special Pseudorandom number generators (PRNGs) may be required <span class="co">[</span><span class="ot">@future119</span><span class="co">]</span>.</span>
<span id="cb91-99"><a href="#cb91-99" aria-hidden="true" tabindex="-1"></a>However, <span class="in">`r ref_pkg("future")`</span> ensures that all workers will receive the same PRNG streams, independent of the number of workers <span class="co">[</span><span class="ot">@future119</span><span class="co">]</span>.</span>
<span id="cb91-100"><a href="#cb91-100" aria-hidden="true" tabindex="-1"></a>Therefore, <span class="in">`mlr3`</span> experiments will be reproducible as long as you use <span class="in">`set.seed`</span> at the start of your scripts (with the PRNG of your choice).</span>
<span id="cb91-101"><a href="#cb91-101" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb91-102"><a href="#cb91-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-103"><a href="#cb91-103" aria-hidden="true" tabindex="-1"></a><span class="fu">### Parallelization of Learners {#sec-parallel-learner}</span></span>
<span id="cb91-104"><a href="#cb91-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-105"><a href="#cb91-105" aria-hidden="true" tabindex="-1"></a>At the lowest level, external code can be parallelized if available in underlying implementations.</span>
<span id="cb91-106"><a href="#cb91-106" aria-hidden="true" tabindex="-1"></a>For example, while fitting a single decision tree, each split that divides the data into two disjoint partitions requires a search for the best cut point on all $p$ features.</span>
<span id="cb91-107"><a href="#cb91-107" aria-hidden="true" tabindex="-1"></a>Instead of iterating over all features sequentially, the search can be broken down into $p$ threads, each searching for the best cut point on a single feature.</span>
<span id="cb91-108"><a href="#cb91-108" aria-hidden="true" tabindex="-1"></a>These threads can then be scheduled depending on available CPU cores, as there is no need for communication between the threads.</span>
<span id="cb91-109"><a href="#cb91-109" aria-hidden="true" tabindex="-1"></a>After all the threads have finished, the results are collected and merged before terminating the threads.</span>
<span id="cb91-110"><a href="#cb91-110" aria-hidden="true" tabindex="-1"></a>The $p$ best-cut points per feature are collected and aggregated to the single best-cut point across all features by iterating over the $p$ results sequentially.</span>
<span id="cb91-111"><a href="#cb91-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-112"><a href="#cb91-112" aria-hidden="true" tabindex="-1"></a>:::{.callout-tip}</span>
<span id="cb91-113"><a href="#cb91-113" aria-hidden="true" tabindex="-1"></a><span class="fu">## GPU Computation</span></span>
<span id="cb91-114"><a href="#cb91-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-115"><a href="#cb91-115" aria-hidden="true" tabindex="-1"></a>Parallelization on GPUs is not covered in this book.</span>
<span id="cb91-116"><a href="#cb91-116" aria-hidden="true" tabindex="-1"></a><span class="in">`mlr3`</span> only distributes the fitting of multiple learners, e.g., during resampling, benchmarking, or tuning.</span>
<span id="cb91-117"><a href="#cb91-117" aria-hidden="true" tabindex="-1"></a>On this rather abstract level, GPU parallelization does not work efficiently.</span>
<span id="cb91-118"><a href="#cb91-118" aria-hidden="true" tabindex="-1"></a>However, some learning procedures can be compiled against CUDA/OpenCL to utilize the GPU while fitting a single model.</span>
<span id="cb91-119"><a href="#cb91-119" aria-hidden="true" tabindex="-1"></a>We refer to the respective documentation of the learner's implementation, e.g., <span class="in">`r link("https://xgboost.readthedocs.io/en/stable/gpu/")`</span> for XGBoost.</span>
<span id="cb91-120"><a href="#cb91-120" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb91-121"><a href="#cb91-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-122"><a href="#cb91-122" aria-hidden="true" tabindex="-1"></a><span class="in">`r index('Threading')`</span> is implemented in the compiled code of the package (e.g., in C or C++), which means that the R interpreter calls the external code and waits for the results to be returned, without noticing that the computations are executed in parallel.</span>
<span id="cb91-123"><a href="#cb91-123" aria-hidden="true" tabindex="-1"></a>Therefore, threading can conflict with certain parallel backends, leading the system to be overutilized in the best-case scenario, or causing hangs or segfaults in the worst case.</span>
<span id="cb91-124"><a href="#cb91-124" aria-hidden="true" tabindex="-1"></a>For this reason, we introduced the convention that threading parallelization is turned off by default.</span>
<span id="cb91-125"><a href="#cb91-125" aria-hidden="true" tabindex="-1"></a>Hyperparameters that control the number of threads are tagged with the label <span class="in">`"threads"`</span>:</span>
<span id="cb91-126"><a href="#cb91-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-127"><a href="#cb91-127" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-002}</span></span>
<span id="cb91-128"><a href="#cb91-128" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_ranger = lrn("classif.ranger")</span></span>
<span id="cb91-129"><a href="#cb91-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-130"><a href="#cb91-130" aria-hidden="true" tabindex="-1"></a><span class="in"># show all hyperparameters tagged with "threads"</span></span>
<span id="cb91-131"><a href="#cb91-131" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_ranger$param_set$ids(tags = "threads")</span></span>
<span id="cb91-132"><a href="#cb91-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-133"><a href="#cb91-133" aria-hidden="true" tabindex="-1"></a><span class="in"># The number of threads is initialized to 1</span></span>
<span id="cb91-134"><a href="#cb91-134" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_ranger$param_set$values$num.threads</span></span>
<span id="cb91-135"><a href="#cb91-135" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-136"><a href="#cb91-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-137"><a href="#cb91-137" aria-hidden="true" tabindex="-1"></a>To enable the parallelization for this learner, <span class="in">`mlr3`</span> provides the helper function <span class="in">`r ref("set_threads()")`</span>, which automatically adjusts the hyperparameters associated with builtin learner parallelization:</span>
<span id="cb91-138"><a href="#cb91-138" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-003}</span></span>
<span id="cb91-139"><a href="#cb91-139" aria-hidden="true" tabindex="-1"></a><span class="in"># use four CPUs</span></span>
<span id="cb91-140"><a href="#cb91-140" aria-hidden="true" tabindex="-1"></a><span class="in">set_threads(lrn_ranger, n = 4)</span></span>
<span id="cb91-141"><a href="#cb91-141" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-142"><a href="#cb91-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-143"><a href="#cb91-143" aria-hidden="true" tabindex="-1"></a>If we did not specify an argument for the <span class="in">`n`</span> parameter then the default is a heuristic to detect the correct number using <span class="in">`r ref("parallelly::availableCores()")`</span>.</span>
<span id="cb91-144"><a href="#cb91-144" aria-hidden="true" tabindex="-1"></a>This heuristic is not always ideal (interested readers might want to look up "Amdahl's Law") and utilizing all available cores is occasionally counterproductive and can slow down overall runtime <span class="co">[</span><span class="ot">@avoiddetect</span><span class="co">]</span>, moreover using all cores is not ideal if:</span>
<span id="cb91-145"><a href="#cb91-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-146"><a href="#cb91-146" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>You want to simultaneously use your system for other purposes.</span>
<span id="cb91-147"><a href="#cb91-147" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>You are on a multi-user system and want to spare some resources for other users.</span>
<span id="cb91-148"><a href="#cb91-148" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>You have linked R to a threaded <span class="in">`r index('BLAS', lower = FALSE)`</span> implementation like OpenBLAS and your learners make heavy use of linear algebra.</span>
<span id="cb91-149"><a href="#cb91-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-152"><a href="#cb91-152" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb91-153"><a href="#cb91-153" aria-hidden="true" tabindex="-1"></a><span class="co"># auto-detect cores on the local machine</span></span>
<span id="cb91-154"><a href="#cb91-154" aria-hidden="true" tabindex="-1"></a><span class="fu">set_threads</span>(lrn_ranger)</span>
<span id="cb91-155"><a href="#cb91-155" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-156"><a href="#cb91-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-157"><a href="#cb91-157" aria-hidden="true" tabindex="-1"></a>To control how many cores are set, we recommend manually setting the number of CPUs in your system's <span class="in">`.Rprofile`</span> file:</span>
<span id="cb91-158"><a href="#cb91-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-159"><a href="#cb91-159" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-004, eval = FALSE}</span></span>
<span id="cb91-160"><a href="#cb91-160" aria-hidden="true" tabindex="-1"></a><span class="in">options(mc.cores = 4)</span></span>
<span id="cb91-161"><a href="#cb91-161" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-162"><a href="#cb91-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-163"><a href="#cb91-163" aria-hidden="true" tabindex="-1"></a>There are also other approaches for parallelization of learners, e.g. by directly supporting one specific parallelization backend or a parallelization framework like <span class="in">`r ref_pkg("foreach")`</span>.</span>
<span id="cb91-164"><a href="#cb91-164" aria-hidden="true" tabindex="-1"></a>If this is supported, parallelization must be explicitly activated, e.g. by setting a hyperparameter.</span>
<span id="cb91-165"><a href="#cb91-165" aria-hidden="true" tabindex="-1"></a>If you need to parallelize on the learner level because a single model fit takes too much time, and you only fit a few of these models, consult the documentation of the respective learner.</span>
<span id="cb91-166"><a href="#cb91-166" aria-hidden="true" tabindex="-1"></a>In many scenarios, it makes more sense to parallelize on a different level like resampling or benchmarking which is covered in the following subsections.</span>
<span id="cb91-167"><a href="#cb91-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-168"><a href="#cb91-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-169"><a href="#cb91-169" aria-hidden="true" tabindex="-1"></a><span class="fu">### Parallelization of Resamplings and Benchmarks {#sec-parallel-resample}</span></span>
<span id="cb91-170"><a href="#cb91-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-171"><a href="#cb91-171" aria-hidden="true" tabindex="-1"></a>In addition to parallel learners, most machine learning experiments can be easily parallelized during resampling.</span>
<span id="cb91-172"><a href="#cb91-172" aria-hidden="true" tabindex="-1"></a>By definition, resampling is performed by aggregating over independent repetitions of multiple train-test splits.</span>
<span id="cb91-173"><a href="#cb91-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-174"><a href="#cb91-174" aria-hidden="true" tabindex="-1"></a><span class="in">`mlr3`</span> makes use of <span class="in">`r ref_pkg("future")`</span> to enable parallelization over resampling iterations using the parallel backend, which can be configured by the user via the <span class="in">`r ref("future::plan()")`</span> function.</span>
<span id="cb91-175"><a href="#cb91-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-176"><a href="#cb91-176" aria-hidden="true" tabindex="-1"></a>By example, we will look at parallelizing three-fold CV for a decision tree on the sonar task (@fig-parallel-overview).</span>
<span id="cb91-177"><a href="#cb91-177" aria-hidden="true" tabindex="-1"></a>We use the <span class="in">`r ref("future::multisession")`</span> plan (which internally uses socket clusters from the <span class="in">`parallel`</span> package) that should work on all operating systems.</span>
<span id="cb91-178"><a href="#cb91-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-179"><a href="#cb91-179" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-005}</span></span>
<span id="cb91-180"><a href="#cb91-180" aria-hidden="true" tabindex="-1"></a><span class="in">library(future)</span></span>
<span id="cb91-181"><a href="#cb91-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-182"><a href="#cb91-182" aria-hidden="true" tabindex="-1"></a><span class="in"># select the multisession backend to use</span></span>
<span id="cb91-183"><a href="#cb91-183" aria-hidden="true" tabindex="-1"></a><span class="in">future::plan("multisession")</span></span>
<span id="cb91-184"><a href="#cb91-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-185"><a href="#cb91-185" aria-hidden="true" tabindex="-1"></a><span class="in"># run our experiment</span></span>
<span id="cb91-186"><a href="#cb91-186" aria-hidden="true" tabindex="-1"></a><span class="in">tsk_sonar = tsk("sonar")</span></span>
<span id="cb91-187"><a href="#cb91-187" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_rpart = lrn("classif.rpart")</span></span>
<span id="cb91-188"><a href="#cb91-188" aria-hidden="true" tabindex="-1"></a><span class="in">rsmp_cv3 = rsmp("cv", folds = 3)</span></span>
<span id="cb91-189"><a href="#cb91-189" aria-hidden="true" tabindex="-1"></a><span class="in">system.time({resample(tsk_sonar, lrn_rpart, rsmp_cv3)})</span></span>
<span id="cb91-190"><a href="#cb91-190" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-191"><a href="#cb91-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-192"><a href="#cb91-192" aria-hidden="true" tabindex="-1"></a>By default, all CPUs of your machine are used unless you specify the argument <span class="in">`workers`</span> in <span class="in">`future::plan()`</span> (see the previous section for issues that this might cause).</span>
<span id="cb91-193"><a href="#cb91-193" aria-hidden="true" tabindex="-1"></a>In contrast to threads, the technical overhead for starting workers, communicating objects, sending back results, and shutting down the workers is quite large for the <span class="in">`"multisession"`</span> backend.</span>
<span id="cb91-194"><a href="#cb91-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-195"><a href="#cb91-195" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r ref('future::multicore')`</span> backend comes with more overhead than threading, but considerably less overhead than <span class="in">`"multisession"`</span>, as the <span class="in">`"multicore"`</span> backend only copies R objects when modified ('copy-on-write'), whereas objects are always copied to the respective session before any computation for <span class="in">`"multisession"`</span>.</span>
<span id="cb91-196"><a href="#cb91-196" aria-hidden="true" tabindex="-1"></a>The <span class="in">`"multicore"`</span> backend has the major disadvantage that it is not supported on Windows systems - for this reason, we will stick with the <span class="in">`"multisession"`</span> backend for all examples here.</span>
<span id="cb91-197"><a href="#cb91-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-198"><a href="#cb91-198" aria-hidden="true" tabindex="-1"></a>In general, it is advised to only consider parallelization for resamplings where each iteration runs at least a few seconds.</span>
<span id="cb91-199"><a href="#cb91-199" aria-hidden="true" tabindex="-1"></a>There are two <span class="in">`mlr3`</span> options to control the execution and granularity:</span>
<span id="cb91-200"><a href="#cb91-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-201"><a href="#cb91-201" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If <span class="in">`mlr3.exec_random`</span> is set to <span class="in">`TRUE`</span> (default), the order of jobs is randomized in resamplings and benchmarks.</span>
<span id="cb91-202"><a href="#cb91-202" aria-hidden="true" tabindex="-1"></a>  This can help if you run a benchmark or tuning with heterogeneous runtimes.</span>
<span id="cb91-203"><a href="#cb91-203" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Option <span class="in">`mlr3.exec_chunk_size`</span> can be used to control how many jobs are mapped to a single <span class="in">`future`</span> and defaults to <span class="in">`1`</span>.</span>
<span id="cb91-204"><a href="#cb91-204" aria-hidden="true" tabindex="-1"></a>  The value of this option is passed to <span class="in">`r ref("future.apply::future_mapply()")`</span> and <span class="in">`future.scheduling`</span> is constantly set to <span class="in">`TRUE`</span>.</span>
<span id="cb91-205"><a href="#cb91-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-206"><a href="#cb91-206" aria-hidden="true" tabindex="-1"></a>Tuning the chunk size can help in some rare cases to mitigate the parallelization overhead but is unlikely to be useful in larger problems or longer runtimes.</span>
<span id="cb91-207"><a href="#cb91-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-208"><a href="#cb91-208" aria-hidden="true" tabindex="-1"></a><span class="in">```{r large_benchmarking-051, echo = FALSE}</span></span>
<span id="cb91-209"><a href="#cb91-209" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-parallel-overview</span></span>
<span id="cb91-210"><a href="#cb91-210" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Parallelization of a resampling using three-fold CV. The main process calls the `resample()` function, which starts the parallelization process and the computational task is split into three parts for three-fold CV. The folds are passed to three workers, each fitting a model on the respective subset of the task and predicting on the left-out observations. The predictions (and trained models) are communicated back to the main process which combines them into a `ResampleResult`."</span></span>
<span id="cb91-211"><a href="#cb91-211" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-alt: "Flow chart starting with a rectangular box that says 'Main', with an arrow to a diamond that says 'resample()'. This has three arrows to 'Worker 1-3' respectively, each arrow is labeled 'Fold 1-3' respectively. Each of the worker boxes points to the same diamond that says 'ResampleResult' and each arrow is labeled 'Prediction 1-3' respectively."</span></span>
<span id="cb91-212"><a href="#cb91-212" aria-hidden="true" tabindex="-1"></a><span class="in">include_multi_graphics("mlr3book_figures-30")</span></span>
<span id="cb91-213"><a href="#cb91-213" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-214"><a href="#cb91-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-215"><a href="#cb91-215" aria-hidden="true" tabindex="-1"></a>Benchmarks\index{benchmark experiments} can be seen as a collection of multiple independent resamplings where a combination of a task, a learner, and a resampling strategy defines one resampling to perform.</span>
<span id="cb91-216"><a href="#cb91-216" aria-hidden="true" tabindex="-1"></a>In pseudo-code, the calculation can be written as</span>
<span id="cb91-217"><a href="#cb91-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-218"><a href="#cb91-218" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-219"><a href="#cb91-219" aria-hidden="true" tabindex="-1"></a><span class="in">foreach combination of (task, learner, resampling strategy) {</span></span>
<span id="cb91-220"><a href="#cb91-220" aria-hidden="true" tabindex="-1"></a><span class="in">    foreach resampling iteration {</span></span>
<span id="cb91-221"><a href="#cb91-221" aria-hidden="true" tabindex="-1"></a><span class="in">        execute(resampling, j)</span></span>
<span id="cb91-222"><a href="#cb91-222" aria-hidden="true" tabindex="-1"></a><span class="in">    }</span></span>
<span id="cb91-223"><a href="#cb91-223" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb91-224"><a href="#cb91-224" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-225"><a href="#cb91-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-226"><a href="#cb91-226" aria-hidden="true" tabindex="-1"></a>Therefore we could either:</span>
<span id="cb91-227"><a href="#cb91-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-228"><a href="#cb91-228" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Parallelize over all resamplings and execute each resampling sequentially (parallelize outer loop); or</span>
<span id="cb91-229"><a href="#cb91-229" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Iterate over all resamplings and execute each resampling in parallel (parallelize inner loop).</span>
<span id="cb91-230"><a href="#cb91-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-231"><a href="#cb91-231" aria-hidden="true" tabindex="-1"></a><span class="in">`mlr3`</span> simplifies this decision for you by flattening all experiments to the same level, i.e., <span class="in">`r ref("benchmark()")`</span> iterates over the elements of the Cartesian product of the iterations of the outer and inner loops.</span>
<span id="cb91-232"><a href="#cb91-232" aria-hidden="true" tabindex="-1"></a>Therefore, there is no need to decide whether you want to parallelize the tuning *or* the resampling, you always parallelize both.</span>
<span id="cb91-233"><a href="#cb91-233" aria-hidden="true" tabindex="-1"></a>This approach makes the computation fine-grained and allows the <span class="in">`future`</span> backend to group the jobs into chunks of suitable size (depending on the number of workers), it also makes the procedure identical to parallelizing resampling:</span>
<span id="cb91-234"><a href="#cb91-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-235"><a href="#cb91-235" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-006}</span></span>
<span id="cb91-236"><a href="#cb91-236" aria-hidden="true" tabindex="-1"></a><span class="in"># simple benchmark design</span></span>
<span id="cb91-237"><a href="#cb91-237" aria-hidden="true" tabindex="-1"></a><span class="in">design = benchmark_grid(tsks(c("sonar", "penguins")),</span></span>
<span id="cb91-238"><a href="#cb91-238" aria-hidden="true" tabindex="-1"></a><span class="in">  lrns(c("classif.featureless", "classif.rpart")), rsmp_cv3)</span></span>
<span id="cb91-239"><a href="#cb91-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-240"><a href="#cb91-240" aria-hidden="true" tabindex="-1"></a><span class="in"># enable parallelization</span></span>
<span id="cb91-241"><a href="#cb91-241" aria-hidden="true" tabindex="-1"></a><span class="in">future::plan("multisession")</span></span>
<span id="cb91-242"><a href="#cb91-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-243"><a href="#cb91-243" aria-hidden="true" tabindex="-1"></a><span class="in"># run benchmark in parallel</span></span>
<span id="cb91-244"><a href="#cb91-244" aria-hidden="true" tabindex="-1"></a><span class="in">bmr = benchmark(design)</span></span>
<span id="cb91-245"><a href="#cb91-245" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-246"><a href="#cb91-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-247"><a href="#cb91-247" aria-hidden="true" tabindex="-1"></a>See @sec-hpc-exec for larger benchmark experiments that may have a cumulative runtime of weeks, months or even years.</span>
<span id="cb91-248"><a href="#cb91-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-249"><a href="#cb91-249" aria-hidden="true" tabindex="-1"></a><span class="fu">### Parallelization of Tuning {#sec-parallel-tuning}</span></span>
<span id="cb91-250"><a href="#cb91-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-251"><a href="#cb91-251" aria-hidden="true" tabindex="-1"></a>Tuning is usually an iterative procedure, consisting of steps that are themselves embarrassingly parallel.</span>
<span id="cb91-252"><a href="#cb91-252" aria-hidden="true" tabindex="-1"></a>In each iteration, a tuner proposes a batch of hyperparameter configurations (which could be of size <span class="in">`1`</span>), which can then be evaluated in parallel.</span>
<span id="cb91-253"><a href="#cb91-253" aria-hidden="true" tabindex="-1"></a>After each iteration, most tuners adapt themselves in some way based on the obtained performance values.</span>
<span id="cb91-254"><a href="#cb91-254" aria-hidden="true" tabindex="-1"></a>Random and grid search are exceptions as they do not choose configurations based on past results, instead, for these tuners, all evaluations are independent and can, in principle, be fully parallelized.</span>
<span id="cb91-255"><a href="#cb91-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-256"><a href="#cb91-256" aria-hidden="true" tabindex="-1"></a>Tuning is implemented in <span class="in">`mlr3`</span> as iterative benchmarks.</span>
<span id="cb91-257"><a href="#cb91-257" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r ref("Tuner")`</span> proposes a batch of learners, each with a different configuration in its <span class="in">`$param_set$values`</span>, where the size of the batch can usually be controlled with the <span class="in">`batch_size`</span> configuration parameter.</span>
<span id="cb91-258"><a href="#cb91-258" aria-hidden="true" tabindex="-1"></a>This batch is passed to <span class="in">`r ref("benchmark()")`</span> with the resampling strategy of the tuning instance.</span>
<span id="cb91-259"><a href="#cb91-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-260"><a href="#cb91-260" aria-hidden="true" tabindex="-1"></a>Since each call to <span class="in">`benchmark()`</span> depends on previous results, it is generally not possible to parallelize tuning at a higher "level" than individual benchmarks.</span>
<span id="cb91-261"><a href="#cb91-261" aria-hidden="true" tabindex="-1"></a>Instead, the individual <span class="in">`benchmark()`</span> evaluations are parallelized by <span class="in">`mlr3`</span> as if they were experiments without tuning.</span>
<span id="cb91-262"><a href="#cb91-262" aria-hidden="true" tabindex="-1"></a>This means that the individual resampling iterations of each evaluated configuration are all parallelized at the same time.</span>
<span id="cb91-263"><a href="#cb91-263" aria-hidden="true" tabindex="-1"></a>To ensure full parallelization, make sure that the <span class="in">`batch_size`</span> multiplied by the number of resampling iterations is at least equal to the number of available workers.</span>
<span id="cb91-264"><a href="#cb91-264" aria-hidden="true" tabindex="-1"></a>If you expect homogeneous runtimes, i.e., you are tuning over a single learner or pipeline without any hyperparameters with a large influence on the runtime, aim for a multiple of the number of workers.</span>
<span id="cb91-265"><a href="#cb91-265" aria-hidden="true" tabindex="-1"></a>In general, larger batches allow for more parallelization, while smaller batches imply a more frequent evaluation of the termination criteria.</span>
<span id="cb91-266"><a href="#cb91-266" aria-hidden="true" tabindex="-1"></a>Independently of whether you use parallelization, the termination criteria are only checked between evaluations of batches.</span>
<span id="cb91-267"><a href="#cb91-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-268"><a href="#cb91-268" aria-hidden="true" tabindex="-1"></a>The following code shows a parallelized execution of random search with the termination criterion set to 20 iterations and a moderate batch size, where 36 resampling splits -- 12 configurations of three splits each -- are evaluated in parallel on four workers.</span>
<span id="cb91-269"><a href="#cb91-269" aria-hidden="true" tabindex="-1"></a>The batch size, set to a multiple of the number of workers, ensures that available resources are used efficiently.</span>
<span id="cb91-270"><a href="#cb91-270" aria-hidden="true" tabindex="-1"></a>However, note that the tuning only terminates after a multiple of the given batch size, in this case after 24 evaluations.</span>
<span id="cb91-271"><a href="#cb91-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-274"><a href="#cb91-274" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb91-275"><a href="#cb91-275" aria-hidden="true" tabindex="-1"></a>future<span class="sc">::</span><span class="fu">plan</span>(<span class="st">"multisession"</span>, <span class="at">workers =</span> <span class="dv">4</span>)</span>
<span id="cb91-276"><a href="#cb91-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-277"><a href="#cb91-277" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">tune</span>(</span>
<span id="cb91-278"><a href="#cb91-278" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tnr</span>(<span class="st">"random_search"</span>, <span class="at">batch_size =</span> <span class="dv">12</span>),</span>
<span id="cb91-279"><a href="#cb91-279" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tsk</span>(<span class="st">"penguins"</span>),</span>
<span id="cb91-280"><a href="#cb91-280" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>, <span class="at">minsplit =</span> <span class="fu">to_tune</span>(<span class="dv">2</span>, <span class="dv">128</span>)),</span>
<span id="cb91-281"><a href="#cb91-281" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb91-282"><a href="#cb91-282" aria-hidden="true" tabindex="-1"></a>  <span class="at">term_evals =</span> <span class="dv">20</span></span>
<span id="cb91-283"><a href="#cb91-283" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb91-284"><a href="#cb91-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-285"><a href="#cb91-285" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>archive<span class="sc">$</span>n_evals</span>
<span id="cb91-286"><a href="#cb91-286" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-287"><a href="#cb91-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-288"><a href="#cb91-288" aria-hidden="true" tabindex="-1"></a>In this example, we could have increased the batch size to 20 to make use of available resources in the most efficient way while stopping exactly at the number of evaluations, however this does not generalize to other termination criteria where we do not know the number of evaluations in advance.</span>
<span id="cb91-289"><a href="#cb91-289" aria-hidden="true" tabindex="-1"></a>For example, if we used <span class="in">`trm("perf_reached")`</span> with a batch size of 12, then if the first configuration of the batch yielded better performance than the given threshold, the remaining 11 configurations would still be unnecessarily evaluated.</span>
<span id="cb91-290"><a href="#cb91-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-291"><a href="#cb91-291" aria-hidden="true" tabindex="-1"></a><span class="fu">### Nested Resampling Parallelization {#sec-nested-resampling-parallelization}</span></span>
<span id="cb91-292"><a href="#cb91-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-293"><a href="#cb91-293" aria-hidden="true" tabindex="-1"></a>Nested resampling can conceptually be parallelized at three different levels, each corresponding to jobs of different granularity:</span>
<span id="cb91-294"><a href="#cb91-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-295"><a href="#cb91-295" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The parallelization of the outer resampling.</span>
<span id="cb91-296"><a href="#cb91-296" aria-hidden="true" tabindex="-1"></a>  A job is then the tuning of a learner on the respective training set of the outer resampling splits.</span>
<span id="cb91-297"><a href="#cb91-297" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>The parallel evaluation of the batch of hyperparameter configurations proposed in one tuning iteration.</span>
<span id="cb91-298"><a href="#cb91-298" aria-hidden="true" tabindex="-1"></a>  A job is then, for example, the cross-validation of such a configuration.</span>
<span id="cb91-299"><a href="#cb91-299" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>The parallelization of the inner resampling in tuning.</span>
<span id="cb91-300"><a href="#cb91-300" aria-hidden="true" tabindex="-1"></a>  A job is then a train-predict-score step of a single configuration.</span>
<span id="cb91-301"><a href="#cb91-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-302"><a href="#cb91-302" aria-hidden="true" tabindex="-1"></a>This is demonstrated in the pseudocode below, which is a simplified form of Algorithm 3 from @hpo_practical:</span>
<span id="cb91-303"><a href="#cb91-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-304"><a href="#cb91-304" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, eval = FALSE}</span></span>
<span id="cb91-305"><a href="#cb91-305" aria-hidden="true" tabindex="-1"></a><span class="in"># outer resampling, level 1:</span></span>
<span id="cb91-306"><a href="#cb91-306" aria-hidden="true" tabindex="-1"></a><span class="in">for (i in seq_len(n_outer_splits)) {</span></span>
<span id="cb91-307"><a href="#cb91-307" aria-hidden="true" tabindex="-1"></a><span class="in">  # tuning instance, in this example mainly represents the archive</span></span>
<span id="cb91-308"><a href="#cb91-308" aria-hidden="true" tabindex="-1"></a><span class="in">  tuning_inst = ti(...)</span></span>
<span id="cb91-309"><a href="#cb91-309" aria-hidden="true" tabindex="-1"></a><span class="in">  inner_task = get_training_task(task, outer_splits[[i]])</span></span>
<span id="cb91-310"><a href="#cb91-310" aria-hidden="true" tabindex="-1"></a><span class="in">  # tuning loop, the details of which depend on the tuner being used</span></span>
<span id="cb91-311"><a href="#cb91-311" aria-hidden="true" tabindex="-1"></a><span class="in">  # This does not correspond to a level:</span></span>
<span id="cb91-312"><a href="#cb91-312" aria-hidden="true" tabindex="-1"></a><span class="in">  while (!tuning_inst$is_terminated) {</span></span>
<span id="cb91-313"><a href="#cb91-313" aria-hidden="true" tabindex="-1"></a><span class="in">    proposed_points = propose_points(tuning_inst$archive, batch_size)</span></span>
<span id="cb91-314"><a href="#cb91-314" aria-hidden="true" tabindex="-1"></a><span class="in">    # Evaluation of configurations, level 2:</span></span>
<span id="cb91-315"><a href="#cb91-315" aria-hidden="true" tabindex="-1"></a><span class="in">    for (hp_configuration in proposed_points) {</span></span>
<span id="cb91-316"><a href="#cb91-316" aria-hidden="true" tabindex="-1"></a><span class="in">      split_performances = numeric()</span></span>
<span id="cb91-317"><a href="#cb91-317" aria-hidden="true" tabindex="-1"></a><span class="in">      # Inner resampling, level 3:</span></span>
<span id="cb91-318"><a href="#cb91-318" aria-hidden="true" tabindex="-1"></a><span class="in">      for (j in seq_len(n_inner_splits)) {</span></span>
<span id="cb91-319"><a href="#cb91-319" aria-hidden="true" tabindex="-1"></a><span class="in">        split_performances[j] = evaluate_performance(</span></span>
<span id="cb91-320"><a href="#cb91-320" aria-hidden="true" tabindex="-1"></a><span class="in">          learner, hp_configuration, inner_task, inner_splits[[j]]</span></span>
<span id="cb91-321"><a href="#cb91-321" aria-hidden="true" tabindex="-1"></a><span class="in">        )</span></span>
<span id="cb91-322"><a href="#cb91-322" aria-hidden="true" tabindex="-1"></a><span class="in">      }</span></span>
<span id="cb91-323"><a href="#cb91-323" aria-hidden="true" tabindex="-1"></a><span class="in">      performance = aggregate(split_performances)</span></span>
<span id="cb91-324"><a href="#cb91-324" aria-hidden="true" tabindex="-1"></a><span class="in">      update_archive(tuning_inst$archive, configuration, performance)</span></span>
<span id="cb91-325"><a href="#cb91-325" aria-hidden="true" tabindex="-1"></a><span class="in">    }</span></span>
<span id="cb91-326"><a href="#cb91-326" aria-hidden="true" tabindex="-1"></a><span class="in">  }</span></span>
<span id="cb91-327"><a href="#cb91-327" aria-hidden="true" tabindex="-1"></a><span class="in">  evaluate_performance(</span></span>
<span id="cb91-328"><a href="#cb91-328" aria-hidden="true" tabindex="-1"></a><span class="in">    learner, tuning_inst$result, task, outer_splits[[i]]</span></span>
<span id="cb91-329"><a href="#cb91-329" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb91-330"><a href="#cb91-330" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb91-331"><a href="#cb91-331" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-332"><a href="#cb91-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-333"><a href="#cb91-333" aria-hidden="true" tabindex="-1"></a>This algorithm is implemented in <span class="in">`mlr3`</span> in a slightly more efficient manner.</span>
<span id="cb91-334"><a href="#cb91-334" aria-hidden="true" tabindex="-1"></a>At the second level (the evaluation of hyperparameter configurations), it exploits the functionality of <span class="in">`benchmark()`</span>: a <span class="in">`Learner`</span> object is created for each proposed hyperparameter configuration and all learners are resampled in a benchmark experiment in the innermost for-loop, effectively executing the second level along with the third level on a finer granularity (number of proposed points times number of inner resampling iterations).</span>
<span id="cb91-335"><a href="#cb91-335" aria-hidden="true" tabindex="-1"></a>Hence, when parallelizing nested resampling in <span class="in">`mlr3`</span>, the user only has to choose between two options: parallelizing the outer resampling or the inner benchmarking.</span>
<span id="cb91-336"><a href="#cb91-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-337"><a href="#cb91-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-338"><a href="#cb91-338" aria-hidden="true" tabindex="-1"></a>By example, let us tune the <span class="in">`minsplit`</span> argument of a classification tree using an <span class="in">`r ref("AutoTuner")`</span> (@sec-autotuner) and random search with only two iterations.</span>
<span id="cb91-339"><a href="#cb91-339" aria-hidden="true" tabindex="-1"></a>Note that this is a didactic example to illustrate the interplay of the different parallelization levels and not a realistic setup.</span>
<span id="cb91-340"><a href="#cb91-340" aria-hidden="true" tabindex="-1"></a>We use holdout for inner resampling and set the <span class="in">`batch_size`</span> to <span class="in">`2`</span>, which yields two independent iterations in the inner benchmark experiment.</span>
<span id="cb91-341"><a href="#cb91-341" aria-hidden="true" tabindex="-1"></a>A five-fold CV is used for our outer resampling.</span>
<span id="cb91-342"><a href="#cb91-342" aria-hidden="true" tabindex="-1"></a>For the sake of simplicity, we will also ignore the final model fit the <span class="in">`AutoTuner`</span> performs after tuning.</span>
<span id="cb91-343"><a href="#cb91-343" aria-hidden="true" tabindex="-1"></a>Below, we run the example sequentially without parallelization:</span>
<span id="cb91-344"><a href="#cb91-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-345"><a href="#cb91-345" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-007}</span></span>
<span id="cb91-346"><a href="#cb91-346" aria-hidden="true" tabindex="-1"></a><span class="in">library(mlr3tuning)</span></span>
<span id="cb91-347"><a href="#cb91-347" aria-hidden="true" tabindex="-1"></a><span class="in"># reset to default sequential plan</span></span>
<span id="cb91-348"><a href="#cb91-348" aria-hidden="true" tabindex="-1"></a><span class="in">future::plan("sequential")</span></span>
<span id="cb91-349"><a href="#cb91-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-350"><a href="#cb91-350" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_rpart = lrn("classif.rpart",</span></span>
<span id="cb91-351"><a href="#cb91-351" aria-hidden="true" tabindex="-1"></a><span class="in">  minsplit  = to_tune(2, 128))</span></span>
<span id="cb91-352"><a href="#cb91-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-353"><a href="#cb91-353" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_rpart_tuned = auto_tuner(tnr("random_search", batch_size = 2),</span></span>
<span id="cb91-354"><a href="#cb91-354" aria-hidden="true" tabindex="-1"></a><span class="in">  lrn_rpart, rsmp("holdout"), msr("classif.ce"), 2)</span></span>
<span id="cb91-355"><a href="#cb91-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-356"><a href="#cb91-356" aria-hidden="true" tabindex="-1"></a><span class="in">rr = resample(tsk("penguins"), lrn_rpart_tuned, rsmp("cv", folds = 5))</span></span>
<span id="cb91-357"><a href="#cb91-357" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-358"><a href="#cb91-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-359"><a href="#cb91-359" aria-hidden="true" tabindex="-1"></a>We can now either opt to parallelize the outer CV or the inner benchmarking.</span>
<span id="cb91-360"><a href="#cb91-360" aria-hidden="true" tabindex="-1"></a>Let us assume we have a single CPU with four cores (C1 - C4) available and each inner holdout evaluation during tuning takes four seconds.</span>
<span id="cb91-361"><a href="#cb91-361" aria-hidden="true" tabindex="-1"></a>If we parallelize the outer five-fold CV (@fig-parallel-outer), each of the four cores  would run one outer resampling first, the computation of the fifth iteration has to wait as there are no more available cores.</span>
<span id="cb91-362"><a href="#cb91-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-363"><a href="#cb91-363" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-009, eval = FALSE}</span></span>
<span id="cb91-364"><a href="#cb91-364" aria-hidden="true" tabindex="-1"></a><span class="in"># Parallelize outer loop</span></span>
<span id="cb91-365"><a href="#cb91-365" aria-hidden="true" tabindex="-1"></a><span class="in">future::plan(list("multisession", "sequential"))</span></span>
<span id="cb91-366"><a href="#cb91-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-367"><a href="#cb91-367" aria-hidden="true" tabindex="-1"></a><span class="in"># Alternative: skip specification of 2nd level, since future</span></span>
<span id="cb91-368"><a href="#cb91-368" aria-hidden="true" tabindex="-1"></a><span class="in"># sets all levels after the first to "sequential" by default</span></span>
<span id="cb91-369"><a href="#cb91-369" aria-hidden="true" tabindex="-1"></a><span class="in">future::plan("multisession")</span></span>
<span id="cb91-370"><a href="#cb91-370" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-371"><a href="#cb91-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-372"><a href="#cb91-372" aria-hidden="true" tabindex="-1"></a>This approach is illustrated in @fig-parallel-outer.</span>
<span id="cb91-373"><a href="#cb91-373" aria-hidden="true" tabindex="-1"></a>Each of the four workers starts with the computation of a different inner benchmark, each of which runs sequentially and therefore takes eight seconds on one worker.</span>
<span id="cb91-374"><a href="#cb91-374" aria-hidden="true" tabindex="-1"></a>As there are more jobs than workers, the remaining fifth iteration of the outer resampling is queued on C1 **after** the first four iterations are finished after eight seconds.</span>
<span id="cb91-375"><a href="#cb91-375" aria-hidden="true" tabindex="-1"></a>During the computation of the fifth outer resampling iteration, only C1 is busy, the other three cores are idle.</span>
<span id="cb91-376"><a href="#cb91-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-377"><a href="#cb91-377" aria-hidden="true" tabindex="-1"></a>In contrast, if we parallelize the inner benchmark (@fig-parallel-inner) then the outer resampling runs sequentially: the five inner benchmarks are scheduled one after the other, each of which runs its two holdout evaluations in parallel on two cores; meanwhile, C3 and C4 are idle.</span>
<span id="cb91-378"><a href="#cb91-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-379"><a href="#cb91-379" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-010, eval = FALSE}</span></span>
<span id="cb91-380"><a href="#cb91-380" aria-hidden="true" tabindex="-1"></a><span class="in"># Parallelize inner loop</span></span>
<span id="cb91-381"><a href="#cb91-381" aria-hidden="true" tabindex="-1"></a><span class="in">future::plan(list("sequential", "multisession"))</span></span>
<span id="cb91-382"><a href="#cb91-382" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-383"><a href="#cb91-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-384"><a href="#cb91-384" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, cpu-utilization-1}</span></span>
<span id="cb91-385"><a href="#cb91-385" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb91-386"><a href="#cb91-386" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-width: 5.5</span></span>
<span id="cb91-387"><a href="#cb91-387" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-parallel-outer</span></span>
<span id="cb91-388"><a href="#cb91-388" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: CPU utilization for four CPUs while parallelizing the outer five-fold CV with a sequential two-fold CV inside. Jobs are labeled as [iteration outer]-[iteration inner].</span></span>
<span id="cb91-389"><a href="#cb91-389" aria-hidden="true" tabindex="-1"></a><span class="in">include_multi_graphics("cpu_utilization_1")</span></span>
<span id="cb91-390"><a href="#cb91-390" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-391"><a href="#cb91-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-392"><a href="#cb91-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-393"><a href="#cb91-393" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, include = FALSE}</span></span>
<span id="cb91-394"><a href="#cb91-394" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-responsive: false</span></span>
<span id="cb91-395"><a href="#cb91-395" aria-hidden="true" tabindex="-1"></a><span class="in">#gantt</span></span>
<span id="cb91-396"><a href="#cb91-396" aria-hidden="true" tabindex="-1"></a><span class="in">#    title CPU Utilization</span></span>
<span id="cb91-397"><a href="#cb91-397" aria-hidden="true" tabindex="-1"></a><span class="in">#    dateFormat  s</span></span>
<span id="cb91-398"><a href="#cb91-398" aria-hidden="true" tabindex="-1"></a><span class="in">#    axisFormat %S</span></span>
<span id="cb91-399"><a href="#cb91-399" aria-hidden="true" tabindex="-1"></a><span class="in">#    section C1</span></span>
<span id="cb91-400"><a href="#cb91-400" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 1-1           :done, 0, 4s</span></span>
<span id="cb91-401"><a href="#cb91-401" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 1-2           :done, 4, 4s</span></span>
<span id="cb91-402"><a href="#cb91-402" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 5-1           :done, 8, 4s</span></span>
<span id="cb91-403"><a href="#cb91-403" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 5-2           :done, 12, 4s</span></span>
<span id="cb91-404"><a href="#cb91-404" aria-hidden="true" tabindex="-1"></a><span class="in">#</span></span>
<span id="cb91-405"><a href="#cb91-405" aria-hidden="true" tabindex="-1"></a><span class="in">#    section C2</span></span>
<span id="cb91-406"><a href="#cb91-406" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 2-1           :done, 0, 4s</span></span>
<span id="cb91-407"><a href="#cb91-407" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 2-2           :done, 4, 4s</span></span>
<span id="cb91-408"><a href="#cb91-408" aria-hidden="true" tabindex="-1"></a><span class="in">#    Idle                    :8, 8s</span></span>
<span id="cb91-409"><a href="#cb91-409" aria-hidden="true" tabindex="-1"></a><span class="in">#</span></span>
<span id="cb91-410"><a href="#cb91-410" aria-hidden="true" tabindex="-1"></a><span class="in">#    section C3</span></span>
<span id="cb91-411"><a href="#cb91-411" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 3-1           :done, 0, 4s</span></span>
<span id="cb91-412"><a href="#cb91-412" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 3-2           :done, 4, 4s</span></span>
<span id="cb91-413"><a href="#cb91-413" aria-hidden="true" tabindex="-1"></a><span class="in">#    Idle                    :8, 8s</span></span>
<span id="cb91-414"><a href="#cb91-414" aria-hidden="true" tabindex="-1"></a><span class="in">#</span></span>
<span id="cb91-415"><a href="#cb91-415" aria-hidden="true" tabindex="-1"></a><span class="in">#    section C4</span></span>
<span id="cb91-416"><a href="#cb91-416" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 4-1           :done, 0, 4s</span></span>
<span id="cb91-417"><a href="#cb91-417" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 4-2           :done, 4, 4s</span></span>
<span id="cb91-418"><a href="#cb91-418" aria-hidden="true" tabindex="-1"></a><span class="in">#    Idle                    :8, 8s</span></span>
<span id="cb91-419"><a href="#cb91-419" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-420"><a href="#cb91-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-421"><a href="#cb91-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-422"><a href="#cb91-422" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, cpu-utilization-2}</span></span>
<span id="cb91-423"><a href="#cb91-423" aria-hidden="true" tabindex="-1"></a><span class="in">#| echo: false</span></span>
<span id="cb91-424"><a href="#cb91-424" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-width: 5.5</span></span>
<span id="cb91-425"><a href="#cb91-425" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-parallel-inner</span></span>
<span id="cb91-426"><a href="#cb91-426" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: CPU utilization for four cores while parallelizing the inner benchmarking (consisting of two holdout evaluations) with a sequential five-fold CV outside. Jobs are labeled as [iteration outer]-[iteration inner].</span></span>
<span id="cb91-427"><a href="#cb91-427" aria-hidden="true" tabindex="-1"></a><span class="in">include_multi_graphics("cpu_utilization_2")</span></span>
<span id="cb91-428"><a href="#cb91-428" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-429"><a href="#cb91-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-430"><a href="#cb91-430" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, include = FALSE}</span></span>
<span id="cb91-431"><a href="#cb91-431" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-responsive: false</span></span>
<span id="cb91-432"><a href="#cb91-432" aria-hidden="true" tabindex="-1"></a><span class="in">#gantt</span></span>
<span id="cb91-433"><a href="#cb91-433" aria-hidden="true" tabindex="-1"></a><span class="in">#    title CPU Utilization</span></span>
<span id="cb91-434"><a href="#cb91-434" aria-hidden="true" tabindex="-1"></a><span class="in">#    dateFormat  s</span></span>
<span id="cb91-435"><a href="#cb91-435" aria-hidden="true" tabindex="-1"></a><span class="in">#    axisFormat %S</span></span>
<span id="cb91-436"><a href="#cb91-436" aria-hidden="true" tabindex="-1"></a><span class="in">#    section C1</span></span>
<span id="cb91-437"><a href="#cb91-437" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 1-1           :done, 0, 4s</span></span>
<span id="cb91-438"><a href="#cb91-438" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 2-1           :done, 4, 4s</span></span>
<span id="cb91-439"><a href="#cb91-439" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 3-1           :done, 8, 4s</span></span>
<span id="cb91-440"><a href="#cb91-440" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 4-1           :done, 12, 4s</span></span>
<span id="cb91-441"><a href="#cb91-441" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 5-1           :done, 16, 4s</span></span>
<span id="cb91-442"><a href="#cb91-442" aria-hidden="true" tabindex="-1"></a><span class="in">#</span></span>
<span id="cb91-443"><a href="#cb91-443" aria-hidden="true" tabindex="-1"></a><span class="in">#    section C2</span></span>
<span id="cb91-444"><a href="#cb91-444" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 1-2           :done, 0, 4s</span></span>
<span id="cb91-445"><a href="#cb91-445" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 2-2           :done, 4, 4s</span></span>
<span id="cb91-446"><a href="#cb91-446" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 3-2           :done, 8, 4s</span></span>
<span id="cb91-447"><a href="#cb91-447" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 4-2           :done, 12, 4s</span></span>
<span id="cb91-448"><a href="#cb91-448" aria-hidden="true" tabindex="-1"></a><span class="in">#    Iteration 5-2           :done, 16, 4s</span></span>
<span id="cb91-449"><a href="#cb91-449" aria-hidden="true" tabindex="-1"></a><span class="in">#</span></span>
<span id="cb91-450"><a href="#cb91-450" aria-hidden="true" tabindex="-1"></a><span class="in">#    section C3</span></span>
<span id="cb91-451"><a href="#cb91-451" aria-hidden="true" tabindex="-1"></a><span class="in">#    Idle                    :0, 20s</span></span>
<span id="cb91-452"><a href="#cb91-452" aria-hidden="true" tabindex="-1"></a><span class="in">#</span></span>
<span id="cb91-453"><a href="#cb91-453" aria-hidden="true" tabindex="-1"></a><span class="in">#    section C4</span></span>
<span id="cb91-454"><a href="#cb91-454" aria-hidden="true" tabindex="-1"></a><span class="in">#    Idle                    :0, 20s</span></span>
<span id="cb91-455"><a href="#cb91-455" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-456"><a href="#cb91-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-457"><a href="#cb91-457" aria-hidden="true" tabindex="-1"></a>In this example, both possibilities for parallelization are not exploiting the full potential of the four cores.</span>
<span id="cb91-458"><a href="#cb91-458" aria-hidden="true" tabindex="-1"></a>With parallelization of the outer loop, all results are computed after 16 seconds, if we parallelize the inner loop we obtain them after 20 seconds, and in both cases some CPU cores remain idle for at least some of the time.</span>
<span id="cb91-459"><a href="#cb91-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-460"><a href="#cb91-460" aria-hidden="true" tabindex="-1"></a><span class="in">`mlr3`</span> and <span class="in">`future`</span> make it possible to enable parallelization for both loops for nested parallelization, even on different parallelization backends, which can be useful in some distributed computing setups.</span>
<span id="cb91-461"><a href="#cb91-461" aria-hidden="true" tabindex="-1"></a>Note that the detection of available cores does not work for such a nested parallelization and the number of workers must be manually set instead:</span>
<span id="cb91-462"><a href="#cb91-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-463"><a href="#cb91-463" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-011, eval = FALSE}</span></span>
<span id="cb91-464"><a href="#cb91-464" aria-hidden="true" tabindex="-1"></a><span class="in"># Runs both loops in parallel</span></span>
<span id="cb91-465"><a href="#cb91-465" aria-hidden="true" tabindex="-1"></a><span class="in">future::plan(list(</span></span>
<span id="cb91-466"><a href="#cb91-466" aria-hidden="true" tabindex="-1"></a><span class="in">  tweak("multisession", workers = 2),</span></span>
<span id="cb91-467"><a href="#cb91-467" aria-hidden="true" tabindex="-1"></a><span class="in">  tweak("multisession", workers = 2)</span></span>
<span id="cb91-468"><a href="#cb91-468" aria-hidden="true" tabindex="-1"></a><span class="in">))</span></span>
<span id="cb91-469"><a href="#cb91-469" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-470"><a href="#cb91-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-471"><a href="#cb91-471" aria-hidden="true" tabindex="-1"></a>This example would run on up to four cores on the local machine: first, two new sessions would be spawned for the outer loop.</span>
<span id="cb91-472"><a href="#cb91-472" aria-hidden="true" tabindex="-1"></a>Both new sessions then spawn two additional sessions each to evaluate the inner benchmark.</span>
<span id="cb91-473"><a href="#cb91-473" aria-hidden="true" tabindex="-1"></a>Although two cores are still idle when the fifth outer resampling iteration runs, this approach reduces the total runtime to 12 seconds, which is optimal in this example.</span>
<span id="cb91-474"><a href="#cb91-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-475"><a href="#cb91-475" aria-hidden="true" tabindex="-1"></a><span class="fu">### Parallelization of Predictions {#sec-parallel-predict}</span></span>
<span id="cb91-476"><a href="#cb91-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-477"><a href="#cb91-477" aria-hidden="true" tabindex="-1"></a>Finally, predictions from a single learner can be parallelized as the predictions of multiple observations are independent.</span>
<span id="cb91-478"><a href="#cb91-478" aria-hidden="true" tabindex="-1"></a>For most learners, training is the bottleneck and parallelizing the prediction is not a worthwhile endeavor, but there can be exceptions, e.g., if your test dataset is very large.</span>
<span id="cb91-479"><a href="#cb91-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-480"><a href="#cb91-480" aria-hidden="true" tabindex="-1"></a>To predict in parallel, the test data is first split into multiple groups and the predict method of the learner is applied to each group in parallel using an active backend configured via <span class="in">`r ref("future::plan()")`</span>.</span>
<span id="cb91-481"><a href="#cb91-481" aria-hidden="true" tabindex="-1"></a>The resulting predictions are then combined internally in a second step.</span>
<span id="cb91-482"><a href="#cb91-482" aria-hidden="true" tabindex="-1"></a>To avoid predicting in parallel accidentally, parallel predictions must be enabled in the learner via the <span class="in">`parallel_predict`</span> field:</span>
<span id="cb91-483"><a href="#cb91-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-484"><a href="#cb91-484" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-012}</span></span>
<span id="cb91-485"><a href="#cb91-485" aria-hidden="true" tabindex="-1"></a><span class="in"># train random forest on sonar task</span></span>
<span id="cb91-486"><a href="#cb91-486" aria-hidden="true" tabindex="-1"></a><span class="in">tsk_sonar = tsk("sonar")</span></span>
<span id="cb91-487"><a href="#cb91-487" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_rpart = lrn("classif.rpart")</span></span>
<span id="cb91-488"><a href="#cb91-488" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_rpart$train(tsk_sonar)</span></span>
<span id="cb91-489"><a href="#cb91-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-490"><a href="#cb91-490" aria-hidden="true" tabindex="-1"></a><span class="in"># set up parallel predict on four workers</span></span>
<span id="cb91-491"><a href="#cb91-491" aria-hidden="true" tabindex="-1"></a><span class="in">future::plan("multisession", workers = 4)</span></span>
<span id="cb91-492"><a href="#cb91-492" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_rpart$parallel_predict = TRUE</span></span>
<span id="cb91-493"><a href="#cb91-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-494"><a href="#cb91-494" aria-hidden="true" tabindex="-1"></a><span class="in"># predict</span></span>
<span id="cb91-495"><a href="#cb91-495" aria-hidden="true" tabindex="-1"></a><span class="in">prediction = lrn_rpart$predict(tsk_sonar)</span></span>
<span id="cb91-496"><a href="#cb91-496" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-497"><a href="#cb91-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-498"><a href="#cb91-498" aria-hidden="true" tabindex="-1"></a><span class="fu">## Error Handling {#sec-error-handling}</span></span>
<span id="cb91-499"><a href="#cb91-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-500"><a href="#cb91-500" aria-hidden="true" tabindex="-1"></a>In large experiments, it is not uncommon that a model fit or prediction fails with an error.\index{debugging}</span>
<span id="cb91-501"><a href="#cb91-501" aria-hidden="true" tabindex="-1"></a>This is because the algorithms have to process arbitrary data, and not all eventualities can always be handled.</span>
<span id="cb91-502"><a href="#cb91-502" aria-hidden="true" tabindex="-1"></a>While we try to identify obvious problems before execution, such as when missing values occur for a learner that cannot handle them, other problems are far more complex to detect.</span>
<span id="cb91-503"><a href="#cb91-503" aria-hidden="true" tabindex="-1"></a>Examples include numerical problems that may cause issues in training (e.g., due to lack of convergence), or new levels of categorical variables appearing in the prediction step.</span>
<span id="cb91-504"><a href="#cb91-504" aria-hidden="true" tabindex="-1"></a>Different learners behave quite differently when encountering such problems: some models signal a warning during the training step that they failed to fit but return a baseline model, while other models stop the execution.</span>
<span id="cb91-505"><a href="#cb91-505" aria-hidden="true" tabindex="-1"></a>During prediction, some learners error and refuse to predict the response for observations they cannot handle, while others may predict <span class="in">`NA`</span>.</span>
<span id="cb91-506"><a href="#cb91-506" aria-hidden="true" tabindex="-1"></a>In this section, we will discuss how to prevent these errors from causing the program to stop when we do not want it to (e.g., during a benchmark experiment).</span>
<span id="cb91-507"><a href="#cb91-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-508"><a href="#cb91-508" aria-hidden="true" tabindex="-1"></a>For illustration (and internal testing) of error handling, <span class="in">`mlr3`</span> ships with <span class="in">`lrn("classif.debug")`</span> and <span class="in">`lrn("regr.debug")`</span>:</span>
<span id="cb91-509"><a href="#cb91-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-510"><a href="#cb91-510" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-013}</span></span>
<span id="cb91-511"><a href="#cb91-511" aria-hidden="true" tabindex="-1"></a><span class="in">tsk_penguins = tsk("penguins")</span></span>
<span id="cb91-512"><a href="#cb91-512" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug = lrn("classif.debug")</span></span>
<span id="cb91-513"><a href="#cb91-513" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug</span></span>
<span id="cb91-514"><a href="#cb91-514" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-515"><a href="#cb91-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-516"><a href="#cb91-516" aria-hidden="true" tabindex="-1"></a>This learner lets us simulate problems that are frequently encountered in ML.</span>
<span id="cb91-517"><a href="#cb91-517" aria-hidden="true" tabindex="-1"></a>It can be configured to stochastically trigger warnings, errors, and even segfaults, during training or prediction.</span>
<span id="cb91-518"><a href="#cb91-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-519"><a href="#cb91-519" aria-hidden="true" tabindex="-1"></a>With the learner's default settings, the learner will remember a random label and constantly predict this label without signaling any conditions.</span>
<span id="cb91-520"><a href="#cb91-520" aria-hidden="true" tabindex="-1"></a>In the following code we tell the learner to signal an error during the training step:</span>
<span id="cb91-521"><a href="#cb91-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-522"><a href="#cb91-522" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-016, error = TRUE}</span></span>
<span id="cb91-523"><a href="#cb91-523" aria-hidden="true" tabindex="-1"></a><span class="in"># set probability to signal an error to `1`</span></span>
<span id="cb91-524"><a href="#cb91-524" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$param_set$values$error_train = 1</span></span>
<span id="cb91-525"><a href="#cb91-525" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$train(tsk_penguins)</span></span>
<span id="cb91-526"><a href="#cb91-526" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-527"><a href="#cb91-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-528"><a href="#cb91-528" aria-hidden="true" tabindex="-1"></a>Now we can look at how to deal with errors during <span class="in">`mlr3`</span> experiments.</span>
<span id="cb91-529"><a href="#cb91-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-530"><a href="#cb91-530" aria-hidden="true" tabindex="-1"></a><span class="fu">### `r index('Encapsulation')` {#sec-encapsulation}</span></span>
<span id="cb91-531"><a href="#cb91-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-532"><a href="#cb91-532" aria-hidden="true" tabindex="-1"></a>Encapsulation ensures that signaled conditions (e.g., messages, warnings and errors) are intercepted and that all conditions raised during the training or prediction step are logged into the learner without interrupting the program flow.</span>
<span id="cb91-533"><a href="#cb91-533" aria-hidden="true" tabindex="-1"></a>This means that models can be used for fitting and predicting and any conditions can be analyzed post hoc.</span>
<span id="cb91-534"><a href="#cb91-534" aria-hidden="true" tabindex="-1"></a>However, the result of the experiment will be a missing model and/or predictions, depending on where the error occurs.</span>
<span id="cb91-535"><a href="#cb91-535" aria-hidden="true" tabindex="-1"></a>In @sec-fallback, we will discuss fallback learners to replace missing models and/or predictions.</span>
<span id="cb91-536"><a href="#cb91-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-537"><a href="#cb91-537" aria-hidden="true" tabindex="-1"></a>Each <span class="in">`r ref("Learner")`</span> contains the field <span class="in">`r index("$encapsulate", parent = "Learner", aside = TRUE, code = TRUE)`</span> to control how the train or predict steps are wrapped.</span>
<span id="cb91-538"><a href="#cb91-538" aria-hidden="true" tabindex="-1"></a>The first way to encapsulate the execution is provided by the package <span class="in">`r ref_pkg("evaluate")`</span>, which evaluates R expressions and captures and tracks conditions (outputs, messages, warnings or errors) without letting them stop the process (see documentation of <span class="in">`r ref("mlr3misc::encapsulate()")`</span> for full details):</span>
<span id="cb91-539"><a href="#cb91-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-540"><a href="#cb91-540" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-017}</span></span>
<span id="cb91-541"><a href="#cb91-541" aria-hidden="true" tabindex="-1"></a><span class="in"># trigger warning and error in training</span></span>
<span id="cb91-542"><a href="#cb91-542" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug = lrn("classif.debug", warning_train = 1, error_train = 1)</span></span>
<span id="cb91-543"><a href="#cb91-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-544"><a href="#cb91-544" aria-hidden="true" tabindex="-1"></a><span class="in"># enable encapsulation for train() and predict()</span></span>
<span id="cb91-545"><a href="#cb91-545" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$encapsulate = c(train = "evaluate", predict = "evaluate")</span></span>
<span id="cb91-546"><a href="#cb91-546" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$train(tsk_penguins)</span></span>
<span id="cb91-547"><a href="#cb91-547" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-548"><a href="#cb91-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-549"><a href="#cb91-549" aria-hidden="true" tabindex="-1"></a>Note how we passed <span class="in">`"evaluate"`</span> to <span class="in">`train`</span> and <span class="in">`predict`</span> to enable encapsulation in both training and predicting.</span>
<span id="cb91-550"><a href="#cb91-550" aria-hidden="true" tabindex="-1"></a>However, we could have only set encapsulation for one of these stages by instead passing <span class="in">`c(train = "evaluate", predict = "none")`</span> or <span class="in">`c(train = "none", predict = "evaluate")`</span>.</span>
<span id="cb91-551"><a href="#cb91-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-552"><a href="#cb91-552" aria-hidden="true" tabindex="-1"></a>Note that encapsulation captures all output written to the standard output (stdout) and standard error (stderr) streams and stores them in the learner's log.</span>
<span id="cb91-553"><a href="#cb91-553" aria-hidden="true" tabindex="-1"></a>However, in some computational setups, the calling process needs to operate on the log output, such as the <span class="in">`r ref_pkg("batchtools")`</span> package in @sec-large-benchmarking.</span>
<span id="cb91-554"><a href="#cb91-554" aria-hidden="true" tabindex="-1"></a>In this case, use the encapsulation method <span class="in">`"try"`</span> instead, which catches signaled conditions but does not suppress the output.</span>
<span id="cb91-555"><a href="#cb91-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-556"><a href="#cb91-556" aria-hidden="true" tabindex="-1"></a>After training the learner, one can access the log via the fields <span class="in">`log`</span>, <span class="in">`warnings`</span> and <span class="in">`errors`</span>:</span>
<span id="cb91-557"><a href="#cb91-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-558"><a href="#cb91-558" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-018}</span></span>
<span id="cb91-559"><a href="#cb91-559" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$log</span></span>
<span id="cb91-560"><a href="#cb91-560" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$warnings</span></span>
<span id="cb91-561"><a href="#cb91-561" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$errors</span></span>
<span id="cb91-562"><a href="#cb91-562" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-563"><a href="#cb91-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-564"><a href="#cb91-564" aria-hidden="true" tabindex="-1"></a>Another encapsulation method is implemented in the <span class="in">`r ref_pkg("callr")`</span> package.</span>
<span id="cb91-565"><a href="#cb91-565" aria-hidden="true" tabindex="-1"></a>In contrast to <span class="in">`evaluate`</span>, the computation is handled in a separate R process.</span>
<span id="cb91-566"><a href="#cb91-566" aria-hidden="true" tabindex="-1"></a>This guards the calling session against segmentation faults which otherwise would tear down the complete main R session (if we demonstrate that here we would break our book).</span>
<span id="cb91-567"><a href="#cb91-567" aria-hidden="true" tabindex="-1"></a>On the downside, starting new processes comes with comparably more computational overhead.</span>
<span id="cb91-568"><a href="#cb91-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-569"><a href="#cb91-569" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-019}</span></span>
<span id="cb91-570"><a href="#cb91-570" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$encapsulate = c(train = "callr", predict = "callr")</span></span>
<span id="cb91-571"><a href="#cb91-571" aria-hidden="true" tabindex="-1"></a><span class="in"># set segfault_train and remove warning_train and error_train</span></span>
<span id="cb91-572"><a href="#cb91-572" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$param_set$values = list(segfault_train = 1)</span></span>
<span id="cb91-573"><a href="#cb91-573" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$train(task = tsk_penguins)$errors</span></span>
<span id="cb91-574"><a href="#cb91-574" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-575"><a href="#cb91-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-576"><a href="#cb91-576" aria-hidden="true" tabindex="-1"></a>As well as catching errors, we can also set a timeout, in seconds, so that learners do not run for an indefinite time (e.g., due to failing to converge) but are terminated after a specified time.</span>
<span id="cb91-577"><a href="#cb91-577" aria-hidden="true" tabindex="-1"></a>This works most reliably when using <span class="in">`callr`</span> encapsulation, since the <span class="in">`evaluate`</span> method is sometimes not able to interrupt a learner if it gets stuck in external compiled code.</span>
<span id="cb91-578"><a href="#cb91-578" aria-hidden="true" tabindex="-1"></a>If learners are interrupted, then this is logged as an error by the encapsulation process.</span>
<span id="cb91-579"><a href="#cb91-579" aria-hidden="true" tabindex="-1"></a>Again, the timeout can be set separately for training and prediction:</span>
<span id="cb91-580"><a href="#cb91-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-581"><a href="#cb91-581" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-020}</span></span>
<span id="cb91-582"><a href="#cb91-582" aria-hidden="true" tabindex="-1"></a><span class="in"># near instant timeout for training, no timeout for predict</span></span>
<span id="cb91-583"><a href="#cb91-583" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$timeout = c(train = 1e-5, predict = Inf)</span></span>
<span id="cb91-584"><a href="#cb91-584" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$train(task = tsk_penguins)$errors</span></span>
<span id="cb91-585"><a href="#cb91-585" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-586"><a href="#cb91-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-587"><a href="#cb91-587" aria-hidden="true" tabindex="-1"></a>With these methods, we can now catch all conditions and post hoc analyze messages, warnings and errors.</span>
<span id="cb91-588"><a href="#cb91-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-589"><a href="#cb91-589" aria-hidden="true" tabindex="-1"></a>Unfortunately, catching errors and ensuring an upper time limit is only half the battle.</span>
<span id="cb91-590"><a href="#cb91-590" aria-hidden="true" tabindex="-1"></a>If there are errors during training then we will not have a trained model to query, or if there are errors during predicting, then we will not have predictions to analyze:</span>
<span id="cb91-591"><a href="#cb91-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-592"><a href="#cb91-592" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-021, error = TRUE}</span></span>
<span id="cb91-593"><a href="#cb91-593" aria-hidden="true" tabindex="-1"></a><span class="in"># no saved model as there was an error during training</span></span>
<span id="cb91-594"><a href="#cb91-594" aria-hidden="true" tabindex="-1"></a><span class="in">lrn("classif.debug", error_train = 1)$train(tsk_penguins)$model</span></span>
<span id="cb91-595"><a href="#cb91-595" aria-hidden="true" tabindex="-1"></a><span class="in"># saved model</span></span>
<span id="cb91-596"><a href="#cb91-596" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug = lrn("classif.debug", error_predict = 1)$train(tsk_penguins)</span></span>
<span id="cb91-597"><a href="#cb91-597" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$model</span></span>
<span id="cb91-598"><a href="#cb91-598" aria-hidden="true" tabindex="-1"></a><span class="in">#  but no predictions due to an error during predicting</span></span>
<span id="cb91-599"><a href="#cb91-599" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$predict(tsk_penguins)</span></span>
<span id="cb91-600"><a href="#cb91-600" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-601"><a href="#cb91-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-602"><a href="#cb91-602" aria-hidden="true" tabindex="-1"></a>Missing learners and/or predictions are particularly problematic during automated processes such as resampling, benchmarking, or tuning (@sec-encapsulation-fallback), as results cannot be aggregated properly across iterations.</span>
<span id="cb91-603"><a href="#cb91-603" aria-hidden="true" tabindex="-1"></a>In the next section, we will look at fallback learners that impute missing models and predictions.</span>
<span id="cb91-604"><a href="#cb91-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-605"><a href="#cb91-605" aria-hidden="true" tabindex="-1"></a><span class="fu">### `r index('Fallback Learners', "fallback learner")` {#sec-fallback}</span></span>
<span id="cb91-606"><a href="#cb91-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-607"><a href="#cb91-607" aria-hidden="true" tabindex="-1"></a>Say an error has occurred when training a model in one or more iterations during resampling, then there are three methods to proceed with our experiment:</span>
<span id="cb91-608"><a href="#cb91-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-609"><a href="#cb91-609" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Ignore iterations with failures -- This might be the most frequent approach in practice, however, it is **not** statistically sound.</span>
<span id="cb91-610"><a href="#cb91-610" aria-hidden="true" tabindex="-1"></a>   Say we are trying to evaluate the performance of a model.</span>
<span id="cb91-611"><a href="#cb91-611" aria-hidden="true" tabindex="-1"></a>   This model might error if in some resampling splits, there are factor levels during predicting that were not seen during training, thus leading to the model being unable to handle these and erroring.</span>
<span id="cb91-612"><a href="#cb91-612" aria-hidden="true" tabindex="-1"></a>   If we discarded failed iterations, our model would appear to perform well despite it failing to make predictions for an entire class of features.</span>
<span id="cb91-613"><a href="#cb91-613" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Penalize failing learners -- Instead of ignoring failed iterations, we could impute the worst possible score (as defined by a given <span class="in">`r ref("Measure")`</span>) and thereby heavily penalize the learner for failing.</span>
<span id="cb91-614"><a href="#cb91-614" aria-hidden="true" tabindex="-1"></a>   However, this will often be too harsh for many problems, and for some measures, there is no reasonable value to impute.</span>
<span id="cb91-615"><a href="#cb91-615" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Train and predict with a <span class="in">`r index('fallback learner', aside = TRUE)`</span> --  Instead of imputing with the worst possible score, we could train a baseline learner and make predictions from this model.</span>
<span id="cb91-616"><a href="#cb91-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-617"><a href="#cb91-617" aria-hidden="true" tabindex="-1"></a>We strongly recommend the final option, which is statistically sound and can be easily used in any practical experiment.</span>
<span id="cb91-618"><a href="#cb91-618" aria-hidden="true" tabindex="-1"></a><span class="in">`mlr3`</span> includes two baseline learners: <span class="in">`lrn("classif.featureless")`</span>, which, in its default configuration, always predicts the majority class, and <span class="in">`lrn("regr.featureless")`</span>, which predicts the average response by default.</span>
<span id="cb91-619"><a href="#cb91-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-620"><a href="#cb91-620" aria-hidden="true" tabindex="-1"></a>To make this procedure convenient during resampling and benchmarking, we support fitting a baseline (though in theory you could use any <span class="in">`Learner`</span>) as a <span class="in">`r index('fallback learner')`</span> by passing a <span class="in">`r ref("Learner")`</span> to <span class="in">`r index('$fallback', parent = "Learner", aside = TRUE, code = TRUE)`</span>.</span>
<span id="cb91-621"><a href="#cb91-621" aria-hidden="true" tabindex="-1"></a>In the next example, we add a classification baseline to our debug learner, so that when the debug learner errors, <span class="in">`mlr3`</span> falls back to the predictions of the featureless learner internally.</span>
<span id="cb91-622"><a href="#cb91-622" aria-hidden="true" tabindex="-1"></a>Note that while encapsulation is not enabled explicitly, it is automatically enabled and set to <span class="in">`"evaluate"`</span> if a fallback learner is added.</span>
<span id="cb91-623"><a href="#cb91-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-624"><a href="#cb91-624" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-022}</span></span>
<span id="cb91-625"><a href="#cb91-625" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug = lrn("classif.debug", error_train = 1)</span></span>
<span id="cb91-626"><a href="#cb91-626" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$fallback = lrn("classif.featureless")</span></span>
<span id="cb91-627"><a href="#cb91-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-628"><a href="#cb91-628" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$train(tsk_penguins)</span></span>
<span id="cb91-629"><a href="#cb91-629" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug</span></span>
<span id="cb91-630"><a href="#cb91-630" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-631"><a href="#cb91-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-632"><a href="#cb91-632" aria-hidden="true" tabindex="-1"></a>The learner's log contains the captured error, and although no model is stored as the error was in training, we can still obtain predictions from our fallback:</span>
<span id="cb91-633"><a href="#cb91-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-634"><a href="#cb91-634" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-023}</span></span>
<span id="cb91-635"><a href="#cb91-635" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$log</span></span>
<span id="cb91-636"><a href="#cb91-636" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$model</span></span>
<span id="cb91-637"><a href="#cb91-637" aria-hidden="true" tabindex="-1"></a><span class="in">prediction = lrn_debug$predict(tsk_penguins)</span></span>
<span id="cb91-638"><a href="#cb91-638" aria-hidden="true" tabindex="-1"></a><span class="in">prediction$score()</span></span>
<span id="cb91-639"><a href="#cb91-639" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-640"><a href="#cb91-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-641"><a href="#cb91-641" aria-hidden="true" tabindex="-1"></a>In the following snippet, we compare the debug learner with a simple classification tree.</span>
<span id="cb91-642"><a href="#cb91-642" aria-hidden="true" tabindex="-1"></a>We re-parametrize the debug learner to fail in roughly 50% of the resampling iterations during the training step:</span>
<span id="cb91-643"><a href="#cb91-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-644"><a href="#cb91-644" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-024}</span></span>
<span id="cb91-645"><a href="#cb91-645" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug = lrn("classif.debug", error_train = 0.5)</span></span>
<span id="cb91-646"><a href="#cb91-646" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug$fallback = lrn("classif.featureless")</span></span>
<span id="cb91-647"><a href="#cb91-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-648"><a href="#cb91-648" aria-hidden="true" tabindex="-1"></a><span class="in">aggr = benchmark(benchmark_grid(</span></span>
<span id="cb91-649"><a href="#cb91-649" aria-hidden="true" tabindex="-1"></a><span class="in">  tsk_penguins,</span></span>
<span id="cb91-650"><a href="#cb91-650" aria-hidden="true" tabindex="-1"></a><span class="in">  list(lrn_debug, lrn("classif.rpart")),</span></span>
<span id="cb91-651"><a href="#cb91-651" aria-hidden="true" tabindex="-1"></a><span class="in">  rsmp("cv", folds = 20)))$aggregate(conditions = TRUE)</span></span>
<span id="cb91-652"><a href="#cb91-652" aria-hidden="true" tabindex="-1"></a><span class="in">aggr[, .(learner_id, warnings, errors, classif.ce)]</span></span>
<span id="cb91-653"><a href="#cb91-653" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-654"><a href="#cb91-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-655"><a href="#cb91-655" aria-hidden="true" tabindex="-1"></a>Even though the debug learner occasionally failed to provide predictions, we still obtained a statistically sound aggregated performance value which we can compare to the aggregated performance of the classification tree.</span>
<span id="cb91-656"><a href="#cb91-656" aria-hidden="true" tabindex="-1"></a>It is also possible to split the benchmark up into separate <span class="in">`r ref("ResampleResult")`</span> objects which sometimes helps to get more context.</span>
<span id="cb91-657"><a href="#cb91-657" aria-hidden="true" tabindex="-1"></a>E.g., if we only want to have a closer look into the debug learner, we can extract the errors from the corresponding resample results:</span>
<span id="cb91-658"><a href="#cb91-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-659"><a href="#cb91-659" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-025}</span></span>
<span id="cb91-660"><a href="#cb91-660" aria-hidden="true" tabindex="-1"></a><span class="in">rr = aggr[learner_id == "classif.debug"]$resample_result[[1L]]</span></span>
<span id="cb91-661"><a href="#cb91-661" aria-hidden="true" tabindex="-1"></a><span class="in">rr$errors[1:2]</span></span>
<span id="cb91-662"><a href="#cb91-662" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-663"><a href="#cb91-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-664"><a href="#cb91-664" aria-hidden="true" tabindex="-1"></a>In summary, combining encapsulation and fallback learners makes it possible to benchmark and tune unreliable or unstable learning algorithms in a convenient and statistically sound fashion.</span>
<span id="cb91-665"><a href="#cb91-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-666"><a href="#cb91-666" aria-hidden="true" tabindex="-1"></a><span class="fu">## `r index("Logging")` {#sec-logging}</span></span>
<span id="cb91-667"><a href="#cb91-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-668"><a href="#cb91-668" aria-hidden="true" tabindex="-1"></a><span class="in">`mlr3`</span> uses the <span class="in">`r ref_pkg("lgr")`</span> package to control the verbosity of the output, i.e., to decide how much output is shown when <span class="in">`mlr3`</span> operations are run, from suppression of all non-critical messages to detailed messaging for debugging.</span>
<span id="cb91-669"><a href="#cb91-669" aria-hidden="true" tabindex="-1"></a>In this section, we will cover how to change logging levels, redirect output, and finally change the timing of logging feedback.</span>
<span id="cb91-670"><a href="#cb91-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-671"><a href="#cb91-671" aria-hidden="true" tabindex="-1"></a><span class="in">`mlr3`</span> uses the following verbosity levels from <span class="in">`lgr`</span>:</span>
<span id="cb91-672"><a href="#cb91-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-673"><a href="#cb91-673" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`"warn"`</span> -- Only non-breaking warnings are logged</span>
<span id="cb91-674"><a href="#cb91-674" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`"info"`</span> -- Information such as model runtimes are logged, as well as warnings</span>
<span id="cb91-675"><a href="#cb91-675" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`"debug"`</span> -- Detailed messaging for debugging, as well as information and warnings</span>
<span id="cb91-676"><a href="#cb91-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-677"><a href="#cb91-677" aria-hidden="true" tabindex="-1"></a>The default log level in <span class="in">`mlr3`</span> is <span class="in">`"info"`</span>, this means that messages are only displayed for messages that are informative or worse, i.e., <span class="in">`"info"`</span> and <span class="in">`"warn"`</span>.</span>
<span id="cb91-678"><a href="#cb91-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-679"><a href="#cb91-679" aria-hidden="true" tabindex="-1"></a>To change the logging threshold you need to retrieve the <span class="in">`R6`</span> logger object from <span class="in">`lgr`</span>, and then call <span class="in">`$set_threshold()`</span>, for example, to lower the logging threshold to enable debugging messaging we would change the threshold to <span class="in">`"debug"`</span>:</span>
<span id="cb91-680"><a href="#cb91-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-681"><a href="#cb91-681" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-027, eval = FALSE}</span></span>
<span id="cb91-682"><a href="#cb91-682" aria-hidden="true" tabindex="-1"></a><span class="in">lgr::get_logger("mlr3")$set_threshold("debug")</span></span>
<span id="cb91-683"><a href="#cb91-683" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-684"><a href="#cb91-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-685"><a href="#cb91-685" aria-hidden="true" tabindex="-1"></a>Or to suppress all messaging except warnings:</span>
<span id="cb91-686"><a href="#cb91-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-687"><a href="#cb91-687" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, eval = FALSE}</span></span>
<span id="cb91-688"><a href="#cb91-688" aria-hidden="true" tabindex="-1"></a><span class="in">lgr::get_logger("mlr3")$set_threshold("warn")</span></span>
<span id="cb91-689"><a href="#cb91-689" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-690"><a href="#cb91-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-691"><a href="#cb91-691" aria-hidden="true" tabindex="-1"></a><span class="in">`lgr`</span> comes with a global option called <span class="in">`"lgr.default_threshold"`</span> which can be set via <span class="in">`options()`</span> to make your choice permanent across sessions (note this will affect all packages using <span class="in">`lgr`</span>), e.g., <span class="in">`options(lgr.default_threshold = "info")`</span>.</span>
<span id="cb91-692"><a href="#cb91-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-693"><a href="#cb91-693" aria-hidden="true" tabindex="-1"></a>The packages in <span class="in">`mlr3`</span> that make use of optimization, i.e., <span class="in">`r mlr3tuning`</span> or <span class="in">`r mlr3fselect`</span>, use the logger of their base package <span class="in">`r ref_pkg("bbotk")`</span>.</span>
<span id="cb91-694"><a href="#cb91-694" aria-hidden="true" tabindex="-1"></a>This means you could disable "info"-logging from the <span class="in">`mlr3`</span> logger, but keep the output from <span class="in">`mlr3tuning`</span>:</span>
<span id="cb91-695"><a href="#cb91-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-696"><a href="#cb91-696" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-031, eval=FALSE}</span></span>
<span id="cb91-697"><a href="#cb91-697" aria-hidden="true" tabindex="-1"></a><span class="in">lgr::get_logger("mlr3")$set_threshold("warn")</span></span>
<span id="cb91-698"><a href="#cb91-698" aria-hidden="true" tabindex="-1"></a><span class="in">lgr::get_logger("bbotk")$set_threshold("info")</span></span>
<span id="cb91-699"><a href="#cb91-699" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-700"><a href="#cb91-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-701"><a href="#cb91-701" aria-hidden="true" tabindex="-1"></a>By default, output from <span class="in">`lgr`</span> is printed in the console, however, you could choose to redirect this to a file in various formats, for example to a JSON file:</span>
<span id="cb91-702"><a href="#cb91-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-703"><a href="#cb91-703" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-032}</span></span>
<span id="cb91-704"><a href="#cb91-704" aria-hidden="true" tabindex="-1"></a><span class="in">tf = tempfile("mlr3log_", fileext = ".json")</span></span>
<span id="cb91-705"><a href="#cb91-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-706"><a href="#cb91-706" aria-hidden="true" tabindex="-1"></a><span class="in"># get the logger as R6 object</span></span>
<span id="cb91-707"><a href="#cb91-707" aria-hidden="true" tabindex="-1"></a><span class="in">logger = lgr::get_logger("mlr3")</span></span>
<span id="cb91-708"><a href="#cb91-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-709"><a href="#cb91-709" aria-hidden="true" tabindex="-1"></a><span class="in"># add Json appender</span></span>
<span id="cb91-710"><a href="#cb91-710" aria-hidden="true" tabindex="-1"></a><span class="in">logger$add_appender(lgr::AppenderJson$new(tf), name = "json")</span></span>
<span id="cb91-711"><a href="#cb91-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-712"><a href="#cb91-712" aria-hidden="true" tabindex="-1"></a><span class="in"># signal a warning</span></span>
<span id="cb91-713"><a href="#cb91-713" aria-hidden="true" tabindex="-1"></a><span class="in">logger$warn("this is a warning from mlr3")</span></span>
<span id="cb91-714"><a href="#cb91-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-715"><a href="#cb91-715" aria-hidden="true" tabindex="-1"></a><span class="in"># print the contents of the file (splitting over two lines)</span></span>
<span id="cb91-716"><a href="#cb91-716" aria-hidden="true" tabindex="-1"></a><span class="in">x = readLines(tf)</span></span>
<span id="cb91-717"><a href="#cb91-717" aria-hidden="true" tabindex="-1"></a><span class="in">cat(paste0(substr(x, 1, 71), "\n", substr(x, 72, nchar(x))))</span></span>
<span id="cb91-718"><a href="#cb91-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-719"><a href="#cb91-719" aria-hidden="true" tabindex="-1"></a><span class="in"># remove the appender again</span></span>
<span id="cb91-720"><a href="#cb91-720" aria-hidden="true" tabindex="-1"></a><span class="in">logger$remove_appender("json")</span></span>
<span id="cb91-721"><a href="#cb91-721" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-722"><a href="#cb91-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-723"><a href="#cb91-723" aria-hidden="true" tabindex="-1"></a>See the vignettes in the <span class="in">`lgr`</span> for more comprehensive examples.</span>
<span id="cb91-724"><a href="#cb91-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-725"><a href="#cb91-725" aria-hidden="true" tabindex="-1"></a>When using parallelization and/or encapsulation, logs may be delayed, out of order, or, in case of some errors, not present at all.</span>
<span id="cb91-726"><a href="#cb91-726" aria-hidden="true" tabindex="-1"></a>When it is necessary to have immediate access to log messages, e.g., when debugging, one may choose to disable <span class="in">`future`</span> and encapsulation.</span>
<span id="cb91-727"><a href="#cb91-727" aria-hidden="true" tabindex="-1"></a>To enable 'debug mode', set <span class="in">`options(mlr3.debug = TRUE)`</span> and ensure the <span class="in">`$encapsulate`</span> slot of learners is set to <span class="in">`"none"`</span> (default) or <span class="in">`"evaluate"`</span>.</span>
<span id="cb91-728"><a href="#cb91-728" aria-hidden="true" tabindex="-1"></a>Debug mode should only be enabled during debugging and not in production use as it disables parallelization and leads to unexpected RNG behavior that prevents reproducibility.</span>
<span id="cb91-729"><a href="#cb91-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-730"><a href="#cb91-730" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data Backends {#sec-backends}</span></span>
<span id="cb91-731"><a href="#cb91-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-732"><a href="#cb91-732" aria-hidden="true" tabindex="-1"></a><span class="in">`Task`</span> objects store their data in an abstract data object, the <span class="in">`r ref("DataBackend")`</span>.</span>
<span id="cb91-733"><a href="#cb91-733" aria-hidden="true" tabindex="-1"></a>A <span class="in">`r index("data backend")`</span> provides a unified API to retrieve subsets of the data or query information about it, regardless of how the data is stored on the system.</span>
<span id="cb91-734"><a href="#cb91-734" aria-hidden="true" tabindex="-1"></a>The default backend uses <span class="in">`r ref_pkg("data.table")`</span> via the <span class="in">`r ref("DataBackendDataTable")`</span> class as a very fast and efficient in-memory database.</span>
<span id="cb91-735"><a href="#cb91-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-736"><a href="#cb91-736" aria-hidden="true" tabindex="-1"></a>While storing the task's data in memory is most efficient for accessing it for model fitting, there are two major disadvantages:</span>
<span id="cb91-737"><a href="#cb91-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-738"><a href="#cb91-738" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Even if only a small proportion of the data is required, for example when doing subsampling, the complete dataset sits in, and consumes, memory.</span>
<span id="cb91-739"><a href="#cb91-739" aria-hidden="true" tabindex="-1"></a>  This is especially a problem if you work with large tasks or many tasks simultaneously, e.g., for benchmarking\index{benchmark experiments}.</span>
<span id="cb91-740"><a href="#cb91-740" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>During parallelization (@sec-parallelization), the complete data needs to be transferred to the workers which can increase the overhead.</span>
<span id="cb91-741"><a href="#cb91-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-742"><a href="#cb91-742" aria-hidden="true" tabindex="-1"></a>To avoid these drawbacks, especially for larger data, it can be necessary to interface out-of-memory data to reduce the memory requirements.</span>
<span id="cb91-743"><a href="#cb91-743" aria-hidden="true" tabindex="-1"></a>This way, only the part of the data which is currently required by the learners will be placed in the main memory to operate on.</span>
<span id="cb91-744"><a href="#cb91-744" aria-hidden="true" tabindex="-1"></a>There are multiple options to handle this:</span>
<span id="cb91-745"><a href="#cb91-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-746"><a href="#cb91-746" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span><span class="in">`r ref("DataBackendDplyr")`</span>, which interfaces the R package <span class="in">`r ref_pkg("dbplyr")`</span>, extending <span class="in">`r ref_pkg("dplyr")`</span> to work on many popular <span class="in">`r index("SQL", lower = FALSE)`</span> databases like *MariaDB*, *PostgresSQL*, or *SQLite*.</span>
<span id="cb91-747"><a href="#cb91-747" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span><span class="in">`r ref("DataBackendDuckDB")`</span> for the *`r index('DuckDB', lower = FALSE)`* database connected via <span class="in">`r ref_pkg("duckdb")`</span>, which is a fast, zero-configuration alternative to <span class="in">`r index('SQLite', lower = FALSE)`</span>.</span>
<span id="cb91-748"><a href="#cb91-748" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span><span class="in">`r ref("DataBackendDuckDB")`</span> for <span class="in">`r index("Parquet", lower = FALSE)`</span> files. This means the data does not need to be converted to DuckDB's native storage format and instead you can work directly on directories containing one or multiple files stored in the popular Parquet format.</span>
<span id="cb91-749"><a href="#cb91-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-750"><a href="#cb91-750" aria-hidden="true" tabindex="-1"></a>In the following, we will show how to work with each of these choices using <span class="in">`r ref_pkg("mlr3db")`</span>\index{\texttt{mlr3db}}.</span>
<span id="cb91-751"><a href="#cb91-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-752"><a href="#cb91-752" aria-hidden="true" tabindex="-1"></a><span class="fu">### Databases with DataBackendDplyr</span></span>
<span id="cb91-753"><a href="#cb91-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-754"><a href="#cb91-754" aria-hidden="true" tabindex="-1"></a>To demonstrate <span class="in">`r ref("mlr3db::DataBackendDplyr")`</span> we use the (pretty big) NYC flights dataset from the <span class="in">`r ref_pkg("nycflights13")`</span> package and move it into a <span class="in">`r index("SQLite", lower = FALSE)`</span> database.</span>
<span id="cb91-755"><a href="#cb91-755" aria-hidden="true" tabindex="-1"></a>Although <span class="in">`r ref("mlr3db::as_sqlite_backend()")`</span> provides a convenient function to perform this step, we construct the database manually here.</span>
<span id="cb91-756"><a href="#cb91-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-757"><a href="#cb91-757" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-034, message = FALSE}</span></span>
<span id="cb91-758"><a href="#cb91-758" aria-hidden="true" tabindex="-1"></a><span class="in"># load data</span></span>
<span id="cb91-759"><a href="#cb91-759" aria-hidden="true" tabindex="-1"></a><span class="in">requireNamespace("DBI")</span></span>
<span id="cb91-760"><a href="#cb91-760" aria-hidden="true" tabindex="-1"></a><span class="in">requireNamespace("RSQLite")</span></span>
<span id="cb91-761"><a href="#cb91-761" aria-hidden="true" tabindex="-1"></a><span class="in">requireNamespace("nycflights13")</span></span>
<span id="cb91-762"><a href="#cb91-762" aria-hidden="true" tabindex="-1"></a><span class="in">data("flights", package = "nycflights13")</span></span>
<span id="cb91-763"><a href="#cb91-763" aria-hidden="true" tabindex="-1"></a><span class="in">dim(flights)</span></span>
<span id="cb91-764"><a href="#cb91-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-765"><a href="#cb91-765" aria-hidden="true" tabindex="-1"></a><span class="in"># add column of unique row ids</span></span>
<span id="cb91-766"><a href="#cb91-766" aria-hidden="true" tabindex="-1"></a><span class="in">flights$row_id = seq(nrow(flights))</span></span>
<span id="cb91-767"><a href="#cb91-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-768"><a href="#cb91-768" aria-hidden="true" tabindex="-1"></a><span class="in"># create sqlite database in temporary file</span></span>
<span id="cb91-769"><a href="#cb91-769" aria-hidden="true" tabindex="-1"></a><span class="in">path = tempfile("flights", fileext = ".sqlite")</span></span>
<span id="cb91-770"><a href="#cb91-770" aria-hidden="true" tabindex="-1"></a><span class="in">con = DBI::dbConnect(RSQLite::SQLite(), path)</span></span>
<span id="cb91-771"><a href="#cb91-771" aria-hidden="true" tabindex="-1"></a><span class="in">tbl = DBI::dbWriteTable(con, "flights", as.data.frame(flights))</span></span>
<span id="cb91-772"><a href="#cb91-772" aria-hidden="true" tabindex="-1"></a><span class="in">DBI::dbDisconnect(con)</span></span>
<span id="cb91-773"><a href="#cb91-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-774"><a href="#cb91-774" aria-hidden="true" tabindex="-1"></a><span class="in"># remove in-memory data</span></span>
<span id="cb91-775"><a href="#cb91-775" aria-hidden="true" tabindex="-1"></a><span class="in">rm(flights)</span></span>
<span id="cb91-776"><a href="#cb91-776" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-777"><a href="#cb91-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-778"><a href="#cb91-778" aria-hidden="true" tabindex="-1"></a>With the SQLite database stored in file <span class="in">`path`</span>, we now re-establish a connection and switch to <span class="in">`r ref_pkg("dplyr")`</span>/<span class="in">`r ref_pkg("dbplyr")`</span> for some essential preprocessing.</span>
<span id="cb91-779"><a href="#cb91-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-780"><a href="#cb91-780" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-035, message = FALSE}</span></span>
<span id="cb91-781"><a href="#cb91-781" aria-hidden="true" tabindex="-1"></a><span class="in"># establish connection</span></span>
<span id="cb91-782"><a href="#cb91-782" aria-hidden="true" tabindex="-1"></a><span class="in">con = DBI::dbConnect(RSQLite::SQLite(), path)</span></span>
<span id="cb91-783"><a href="#cb91-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-784"><a href="#cb91-784" aria-hidden="true" tabindex="-1"></a><span class="in"># select the "flights" table</span></span>
<span id="cb91-785"><a href="#cb91-785" aria-hidden="true" tabindex="-1"></a><span class="in">library(dplyr)</span></span>
<span id="cb91-786"><a href="#cb91-786" aria-hidden="true" tabindex="-1"></a><span class="in">library(dbplyr)</span></span>
<span id="cb91-787"><a href="#cb91-787" aria-hidden="true" tabindex="-1"></a><span class="in">tbl = tbl(con, "flights")</span></span>
<span id="cb91-788"><a href="#cb91-788" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-789"><a href="#cb91-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-790"><a href="#cb91-790" aria-hidden="true" tabindex="-1"></a>As databases are intended to store large volumes of data, a natural first step is to subset and filter the data to suitable dimensions.</span>
<span id="cb91-791"><a href="#cb91-791" aria-hidden="true" tabindex="-1"></a>Therefore, we build up an SQL query in a step-wise fashion using <span class="in">`dplyr`</span> verbs and:</span>
<span id="cb91-792"><a href="#cb91-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-793"><a href="#cb91-793" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Select a subset of columns to work on;</span>
<span id="cb91-794"><a href="#cb91-794" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Remove observations where the arrival delay (<span class="in">`arr_delay`</span>) has a missing value;</span>
<span id="cb91-795"><a href="#cb91-795" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Filter the data to only use every second row (to reduce example runtime); and</span>
<span id="cb91-796"><a href="#cb91-796" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Merge factor levels of the feature <span class="in">`carrier`</span> so infrequent carriers are replaced by level "other".</span>
<span id="cb91-797"><a href="#cb91-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-798"><a href="#cb91-798" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-036}</span></span>
<span id="cb91-799"><a href="#cb91-799" aria-hidden="true" tabindex="-1"></a><span class="in"># 1. subset columns</span></span>
<span id="cb91-800"><a href="#cb91-800" aria-hidden="true" tabindex="-1"></a><span class="in">keep = c("row_id", "year", "month", "day", "hour", "minute", "dep_time",</span></span>
<span id="cb91-801"><a href="#cb91-801" aria-hidden="true" tabindex="-1"></a><span class="in">  "arr_time", "carrier", "flight", "air_time", "distance", "arr_delay")</span></span>
<span id="cb91-802"><a href="#cb91-802" aria-hidden="true" tabindex="-1"></a><span class="in">tbl = select(tbl, all_of(keep))</span></span>
<span id="cb91-803"><a href="#cb91-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-804"><a href="#cb91-804" aria-hidden="true" tabindex="-1"></a><span class="in"># 2. filter by missing</span></span>
<span id="cb91-805"><a href="#cb91-805" aria-hidden="true" tabindex="-1"></a><span class="in">tbl = filter(tbl, !is.na(arr_delay))</span></span>
<span id="cb91-806"><a href="#cb91-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-807"><a href="#cb91-807" aria-hidden="true" tabindex="-1"></a><span class="in"># 3. select every other row</span></span>
<span id="cb91-808"><a href="#cb91-808" aria-hidden="true" tabindex="-1"></a><span class="in">tbl = filter(tbl, row_id %% 2 == 0)</span></span>
<span id="cb91-809"><a href="#cb91-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-810"><a href="#cb91-810" aria-hidden="true" tabindex="-1"></a><span class="in"># 4. merge infrequent carriers</span></span>
<span id="cb91-811"><a href="#cb91-811" aria-hidden="true" tabindex="-1"></a><span class="in">infrequent = c("OO", "HA", "YV", "F9", "AS", "FL", "VX", "WN")</span></span>
<span id="cb91-812"><a href="#cb91-812" aria-hidden="true" tabindex="-1"></a><span class="in">tbl = mutate(tbl, carrier = case_when(</span></span>
<span id="cb91-813"><a href="#cb91-813" aria-hidden="true" tabindex="-1"></a><span class="in">  carrier %in% infrequent ~ "other",</span></span>
<span id="cb91-814"><a href="#cb91-814" aria-hidden="true" tabindex="-1"></a><span class="in">  TRUE ~ carrier))</span></span>
<span id="cb91-815"><a href="#cb91-815" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-816"><a href="#cb91-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-817"><a href="#cb91-817" aria-hidden="true" tabindex="-1"></a>Having prepared our data, we can now create a <span class="in">`r ref("mlr3db::DataBackendDplyr")`</span> and can then query basic information from our new <span class="in">`r ref("DataBackend")`</span>:</span>
<span id="cb91-818"><a href="#cb91-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-819"><a href="#cb91-819" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-040}</span></span>
<span id="cb91-820"><a href="#cb91-820" aria-hidden="true" tabindex="-1"></a><span class="in">library(mlr3db)</span></span>
<span id="cb91-821"><a href="#cb91-821" aria-hidden="true" tabindex="-1"></a><span class="in">backend_flights = as_data_backend(tbl, primary_key = "row_id")</span></span>
<span id="cb91-822"><a href="#cb91-822" aria-hidden="true" tabindex="-1"></a><span class="in">c(nrow = backend_flights$nrow, ncol = backend_flights$ncol)</span></span>
<span id="cb91-823"><a href="#cb91-823" aria-hidden="true" tabindex="-1"></a><span class="in">backend_flights$head()</span></span>
<span id="cb91-824"><a href="#cb91-824" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-825"><a href="#cb91-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-826"><a href="#cb91-826" aria-hidden="true" tabindex="-1"></a>Note that the <span class="in">`DataBackendDplyr`</span> can only operate on the data we provided, so does not 'know' about the rows and columns we already filtered out (this is in contrast to using <span class="in">`$filter`</span> and <span class="in">`$subset`</span> as in @sec-tasks-mutators, which only remove row or column roles and not the rows/columns themselves).</span>
<span id="cb91-827"><a href="#cb91-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-828"><a href="#cb91-828" aria-hidden="true" tabindex="-1"></a>With a backend constructed, we can now use the standard <span class="in">`mlr3`</span> API:</span>
<span id="cb91-829"><a href="#cb91-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-830"><a href="#cb91-830" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-042}</span></span>
<span id="cb91-831"><a href="#cb91-831" aria-hidden="true" tabindex="-1"></a><span class="in">tsk_flights = as_task_regr(backend_flights, id = "flights_sqlite",</span></span>
<span id="cb91-832"><a href="#cb91-832" aria-hidden="true" tabindex="-1"></a><span class="in">  target = "arr_delay")</span></span>
<span id="cb91-833"><a href="#cb91-833" aria-hidden="true" tabindex="-1"></a><span class="in">rsmp_sub002 = rsmp("subsampling", ratio = 0.02, repeats = 3)</span></span>
<span id="cb91-834"><a href="#cb91-834" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-835"><a href="#cb91-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-836"><a href="#cb91-836" aria-hidden="true" tabindex="-1"></a>Above we created a regression task by passing a backend as the first argument and then created a resampling strategy where we will subsample 2% of the observations three times.</span>
<span id="cb91-837"><a href="#cb91-837" aria-hidden="true" tabindex="-1"></a>In each resampling iteration, only the required subset of the data is queried from the SQLite database and passed to our learner:</span>
<span id="cb91-838"><a href="#cb91-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-839"><a href="#cb91-839" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-043, message=FALSE}</span></span>
<span id="cb91-840"><a href="#cb91-840" aria-hidden="true" tabindex="-1"></a><span class="in">rr = resample(tsk_flights, lrn("regr.rpart"), rsmp_sub002)</span></span>
<span id="cb91-841"><a href="#cb91-841" aria-hidden="true" tabindex="-1"></a><span class="in">measures = msrs(c("regr.rmse", "time_train", "time_predict"))</span></span>
<span id="cb91-842"><a href="#cb91-842" aria-hidden="true" tabindex="-1"></a><span class="in">rr$aggregate(measures)</span></span>
<span id="cb91-843"><a href="#cb91-843" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-844"><a href="#cb91-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-845"><a href="#cb91-845" aria-hidden="true" tabindex="-1"></a>As we have finished our experiment we can now close our connection, which we can do by removing the <span class="in">`tbl`</span> object referencing the connection and then closing it.</span>
<span id="cb91-846"><a href="#cb91-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-847"><a href="#cb91-847" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-044}</span></span>
<span id="cb91-848"><a href="#cb91-848" aria-hidden="true" tabindex="-1"></a><span class="in">rm(tbl)</span></span>
<span id="cb91-849"><a href="#cb91-849" aria-hidden="true" tabindex="-1"></a><span class="in">DBI::dbDisconnect(con)</span></span>
<span id="cb91-850"><a href="#cb91-850" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-851"><a href="#cb91-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-852"><a href="#cb91-852" aria-hidden="true" tabindex="-1"></a><span class="fu">### Parquet Files with DataBackendDuckDB</span></span>
<span id="cb91-853"><a href="#cb91-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-854"><a href="#cb91-854" aria-hidden="true" tabindex="-1"></a><span class="in">`r index("DuckDB", lower = FALSE)`</span> databases provide a modern alternative to SQLite, tailored to the needs of ML.</span>
<span id="cb91-855"><a href="#cb91-855" aria-hidden="true" tabindex="-1"></a><span class="in">`r index('Parquet', lower = FALSE)`</span> is a popular column-oriented data storage format supporting efficient compression, making it far superior to other popular data exchange formats such as CSV.</span>
<span id="cb91-856"><a href="#cb91-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-857"><a href="#cb91-857" aria-hidden="true" tabindex="-1"></a>Converting a <span class="in">`data.frame`</span> to DuckDB is possible by passing the <span class="in">`data.frame`</span> to convert and the <span class="in">`path`</span> to store the data to <span class="in">`r ref("mlr3db::as_duckdb_backend()")`</span>.</span>
<span id="cb91-858"><a href="#cb91-858" aria-hidden="true" tabindex="-1"></a>By example, below we first query the location of an example dataset in a Parquet file shipped with <span class="in">`mlr3db`</span> and then convert the resulting <span class="in">`r ref("DataBackendDuckDB")`</span> object into a classification task, all without loading the dataset into memory:</span>
<span id="cb91-859"><a href="#cb91-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-860"><a href="#cb91-860" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-045}</span></span>
<span id="cb91-861"><a href="#cb91-861" aria-hidden="true" tabindex="-1"></a><span class="in">path = system.file(file.path("extdata", "spam.parquet"),</span></span>
<span id="cb91-862"><a href="#cb91-862" aria-hidden="true" tabindex="-1"></a><span class="in">  package = "mlr3db")</span></span>
<span id="cb91-863"><a href="#cb91-863" aria-hidden="true" tabindex="-1"></a><span class="in">backend = as_duckdb_backend(path)</span></span>
<span id="cb91-864"><a href="#cb91-864" aria-hidden="true" tabindex="-1"></a><span class="in">as_task_classif(backend, target = "type")</span></span>
<span id="cb91-865"><a href="#cb91-865" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-866"><a href="#cb91-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-867"><a href="#cb91-867" aria-hidden="true" tabindex="-1"></a>Accessing the data internally triggers a query and the required subsets of data are fetched to be stored in an in-memory <span class="in">`data.frame`</span>.</span>
<span id="cb91-868"><a href="#cb91-868" aria-hidden="true" tabindex="-1"></a>After the retrieved data is processed, the garbage collector can release the occupied memory.</span>
<span id="cb91-869"><a href="#cb91-869" aria-hidden="true" tabindex="-1"></a>The backend can also operate on a folder with multiple parquet files.</span>
<span id="cb91-870"><a href="#cb91-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-871"><a href="#cb91-871" aria-hidden="true" tabindex="-1"></a><span class="fu">## Extending mlr3 and Defining a New `Measure` {#sec-extending}</span></span>
<span id="cb91-872"><a href="#cb91-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-873"><a href="#cb91-873" aria-hidden="true" tabindex="-1"></a>After getting this far in the book you are well on your way to being an <span class="in">`mlr3`</span> expert and may even want to add more classes to our universe.</span>
<span id="cb91-874"><a href="#cb91-874" aria-hidden="true" tabindex="-1"></a>While many classes could be extended, all have a similar design interface and so, we will only demonstrate how to create a custom <span class="in">`r ref("Measure")`</span>.</span>
<span id="cb91-875"><a href="#cb91-875" aria-hidden="true" tabindex="-1"></a>If you are interested in implementing new learners, <span class="in">`PipeOp`</span>s, or tuners, then check out the vignettes in the respective packages: <span class="in">`r mlr3extralearners`</span>, <span class="in">`r mlr3pipelines`</span>, or <span class="in">`r mlr3tuning`</span>.</span>
<span id="cb91-876"><a href="#cb91-876" aria-hidden="true" tabindex="-1"></a>If you are considering creating a package that adds an entirely new task type then feel free to contact us for some support via GitHub, email, or Mattermost.</span>
<span id="cb91-877"><a href="#cb91-877" aria-hidden="true" tabindex="-1"></a>This section assumes good knowledge of <span class="in">`R6`</span>, see @sec-r6 for a brief introduction and references to further resources.</span>
<span id="cb91-878"><a href="#cb91-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-879"><a href="#cb91-879" aria-hidden="true" tabindex="-1"></a>As an example, let us consider a regression measure that scores a prediction as <span class="in">`1`</span> if the difference between the true and predicted values is less than one standard deviation of the truth, or scores the prediction as <span class="in">`0`</span> otherwise.</span>
<span id="cb91-880"><a href="#cb91-880" aria-hidden="true" tabindex="-1"></a>In maths this would be defined as $f(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^n \mathbb{I}(|y_i - \hat{y}_i| &lt; \sigma_y)$, where $\sigma_y$ is the standard deviation of the truth and $\mathbb{I}$ is the indicator function.</span>
<span id="cb91-881"><a href="#cb91-881" aria-hidden="true" tabindex="-1"></a>In code, this measure may be written as:</span>
<span id="cb91-882"><a href="#cb91-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-883"><a href="#cb91-883" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-047}</span></span>
<span id="cb91-884"><a href="#cb91-884" aria-hidden="true" tabindex="-1"></a><span class="in">threshold_acc = function(truth, response) {</span></span>
<span id="cb91-885"><a href="#cb91-885" aria-hidden="true" tabindex="-1"></a><span class="in">  mean(ifelse(abs(truth - response) &lt; sd(truth), 1, 0))</span></span>
<span id="cb91-886"><a href="#cb91-886" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb91-887"><a href="#cb91-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-888"><a href="#cb91-888" aria-hidden="true" tabindex="-1"></a><span class="in">threshold_acc(c(100, 0, 1), c(1, 11, 6))</span></span>
<span id="cb91-889"><a href="#cb91-889" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-890"><a href="#cb91-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-891"><a href="#cb91-891" aria-hidden="true" tabindex="-1"></a>By definition of this measure, its values are bounded in $<span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$ where a perfect score of $1$ would mean all predictions are within a standard deviation of the truth, hence for this measure larger scores are better.</span>
<span id="cb91-892"><a href="#cb91-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-893"><a href="#cb91-893" aria-hidden="true" tabindex="-1"></a>To use this measure in <span class="in">`mlr3`</span>, we need to create a new <span class="in">`r ref("R6::R6Class")`</span>, which will inherit from <span class="in">`Measure`</span> and in this case specifically from <span class="in">`r ref("MeasureRegr")`</span>.</span>
<span id="cb91-894"><a href="#cb91-894" aria-hidden="true" tabindex="-1"></a>The code for this new measure is in the snippet below, with an explanation following it.</span>
<span id="cb91-895"><a href="#cb91-895" aria-hidden="true" tabindex="-1"></a>This code chunk can be used as a template for the majority of performance measures.</span>
<span id="cb91-896"><a href="#cb91-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-897"><a href="#cb91-897" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-048}</span></span>
<span id="cb91-898"><a href="#cb91-898" aria-hidden="true" tabindex="-1"></a><span class="in">MeasureRegrThresholdAcc = R6::R6Class("MeasureRegrThresholdAcc",</span></span>
<span id="cb91-899"><a href="#cb91-899" aria-hidden="true" tabindex="-1"></a><span class="in">  inherit = mlr3::MeasureRegr, # regression measure</span></span>
<span id="cb91-900"><a href="#cb91-900" aria-hidden="true" tabindex="-1"></a><span class="in">  public = list(</span></span>
<span id="cb91-901"><a href="#cb91-901" aria-hidden="true" tabindex="-1"></a><span class="in">    initialize = function() { # initialize class</span></span>
<span id="cb91-902"><a href="#cb91-902" aria-hidden="true" tabindex="-1"></a><span class="in">      super$initialize(</span></span>
<span id="cb91-903"><a href="#cb91-903" aria-hidden="true" tabindex="-1"></a><span class="in">        id = "thresh_acc", # unique ID</span></span>
<span id="cb91-904"><a href="#cb91-904" aria-hidden="true" tabindex="-1"></a><span class="in">        packages = character(), # no package dependencies</span></span>
<span id="cb91-905"><a href="#cb91-905" aria-hidden="true" tabindex="-1"></a><span class="in">        properties = character(), #&nbsp;no special properties</span></span>
<span id="cb91-906"><a href="#cb91-906" aria-hidden="true" tabindex="-1"></a><span class="in">        predict_type = "response", # measures response prediction</span></span>
<span id="cb91-907"><a href="#cb91-907" aria-hidden="true" tabindex="-1"></a><span class="in">        range = c(0, 1), # results in values between (0, 1)</span></span>
<span id="cb91-908"><a href="#cb91-908" aria-hidden="true" tabindex="-1"></a><span class="in">        minimize = FALSE # larger values are better</span></span>
<span id="cb91-909"><a href="#cb91-909" aria-hidden="true" tabindex="-1"></a><span class="in">      )</span></span>
<span id="cb91-910"><a href="#cb91-910" aria-hidden="true" tabindex="-1"></a><span class="in">    }</span></span>
<span id="cb91-911"><a href="#cb91-911" aria-hidden="true" tabindex="-1"></a><span class="in">  ),</span></span>
<span id="cb91-912"><a href="#cb91-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-913"><a href="#cb91-913" aria-hidden="true" tabindex="-1"></a><span class="in">  private = list(</span></span>
<span id="cb91-914"><a href="#cb91-914" aria-hidden="true" tabindex="-1"></a><span class="in">    # define score as private method</span></span>
<span id="cb91-915"><a href="#cb91-915" aria-hidden="true" tabindex="-1"></a><span class="in">    .score = function(prediction, ...) {</span></span>
<span id="cb91-916"><a href="#cb91-916" aria-hidden="true" tabindex="-1"></a><span class="in">      # define loss</span></span>
<span id="cb91-917"><a href="#cb91-917" aria-hidden="true" tabindex="-1"></a><span class="in">      threshold_acc = function(truth, response) {</span></span>
<span id="cb91-918"><a href="#cb91-918" aria-hidden="true" tabindex="-1"></a><span class="in">        mean(ifelse(abs(truth - response) &lt; sd(truth), 1, 0))</span></span>
<span id="cb91-919"><a href="#cb91-919" aria-hidden="true" tabindex="-1"></a><span class="in">      }</span></span>
<span id="cb91-920"><a href="#cb91-920" aria-hidden="true" tabindex="-1"></a><span class="in">      # call loss function</span></span>
<span id="cb91-921"><a href="#cb91-921" aria-hidden="true" tabindex="-1"></a><span class="in">      threshold_acc(prediction$truth, prediction$response)</span></span>
<span id="cb91-922"><a href="#cb91-922" aria-hidden="true" tabindex="-1"></a><span class="in">    }</span></span>
<span id="cb91-923"><a href="#cb91-923" aria-hidden="true" tabindex="-1"></a><span class="in">  )</span></span>
<span id="cb91-924"><a href="#cb91-924" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb91-925"><a href="#cb91-925" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-926"><a href="#cb91-926" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-927"><a href="#cb91-927" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>In the first two lines we name the class, here <span class="in">`MeasureRegrThresholdAcc`</span>, and then state this is a regression measure that inherits from <span class="in">`MeasureRegr`</span>.</span>
<span id="cb91-928"><a href="#cb91-928" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>We initialize the class by stating its unique ID is <span class="in">`"thresh_acc"`</span>, that it does not require any external packages (<span class="in">`packages = character()`</span>) and that it has no special properties (<span class="in">`properties = character()`</span>).</span>
<span id="cb91-929"><a href="#cb91-929" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>We then pass specific details of the loss function which are: it measures the quality of a <span class="in">`"response"`</span> type prediction, its values range between <span class="in">`(0, 1)`</span>, and that the loss is optimized as its maximum (<span class="in">`minimize = FALSE`</span>).</span>
<span id="cb91-930"><a href="#cb91-930" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Finally, we define the score itself as a private method called <span class="in">`.score`</span> where we pass the predictions to the function we defined just above.</span>
<span id="cb91-931"><a href="#cb91-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-932"><a href="#cb91-932" aria-hidden="true" tabindex="-1"></a>Sometimes measures require data from the training set, the task, or the learner.</span>
<span id="cb91-933"><a href="#cb91-933" aria-hidden="true" tabindex="-1"></a>These are usually complex edge-cases examples, so we will not go into detail here, for working examples we suggest looking at the code for <span class="in">`r ref("mlr3proba::MeasureSurvSongAUC")`</span> and <span class="in">`r ref("mlr3proba::MeasureSurvAUC")`</span>.</span>
<span id="cb91-934"><a href="#cb91-934" aria-hidden="true" tabindex="-1"></a>You can also consult the manual page of the <span class="in">`Measure`</span> for an overview of other properties and meta-data that can be specified.</span>
<span id="cb91-935"><a href="#cb91-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-936"><a href="#cb91-936" aria-hidden="true" tabindex="-1"></a>Once you have defined your measure you can load it with the <span class="in">`R6`</span> constructor (<span class="in">`$new()`</span>), or make it available to be constructed with the <span class="in">`msr()`</span> sugar function by adding it to the <span class="in">`r ref("mlr_measures")`</span> dictionary:</span>
<span id="cb91-937"><a href="#cb91-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-938"><a href="#cb91-938" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-049}</span></span>
<span id="cb91-939"><a href="#cb91-939" aria-hidden="true" tabindex="-1"></a><span class="in">tsk_mtcars = tsk("mtcars")</span></span>
<span id="cb91-940"><a href="#cb91-940" aria-hidden="true" tabindex="-1"></a><span class="in">split = partition(tsk_mtcars)</span></span>
<span id="cb91-941"><a href="#cb91-941" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_featureless = lrn("regr.featureless")$train(tsk_mtcars, split$train)</span></span>
<span id="cb91-942"><a href="#cb91-942" aria-hidden="true" tabindex="-1"></a><span class="in">prediction = lrn_featureless$predict(tsk_mtcars, split$test)</span></span>
<span id="cb91-943"><a href="#cb91-943" aria-hidden="true" tabindex="-1"></a><span class="in">prediction$score(MeasureRegrThresholdAcc$new())</span></span>
<span id="cb91-944"><a href="#cb91-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-945"><a href="#cb91-945" aria-hidden="true" tabindex="-1"></a><span class="in"># or add to dictionary by passing a unique key to the first argument</span></span>
<span id="cb91-946"><a href="#cb91-946" aria-hidden="true" tabindex="-1"></a><span class="in">#  and the class to the second</span></span>
<span id="cb91-947"><a href="#cb91-947" aria-hidden="true" tabindex="-1"></a><span class="in">mlr3::mlr_measures$add("regr.thresh_acc", MeasureRegrThresholdAcc)</span></span>
<span id="cb91-948"><a href="#cb91-948" aria-hidden="true" tabindex="-1"></a><span class="in">prediction$score(msr("regr.thresh_acc"))</span></span>
<span id="cb91-949"><a href="#cb91-949" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-950"><a href="#cb91-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-951"><a href="#cb91-951" aria-hidden="true" tabindex="-1"></a>While we only covered how to create a simple regression measure, the process of adding other classes to our universe is in essence the same:</span>
<span id="cb91-952"><a href="#cb91-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-953"><a href="#cb91-953" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Find the right class to inherit from</span>
<span id="cb91-954"><a href="#cb91-954" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Add methods that:</span>
<span id="cb91-955"><a href="#cb91-955" aria-hidden="true" tabindex="-1"></a>    a) Initialize the object with the correct properties (<span class="in">`$initialize()`</span>).</span>
<span id="cb91-956"><a href="#cb91-956" aria-hidden="true" tabindex="-1"></a>    b) Implement the public and private methods that do the actual computation. In the above example, this was the private <span class="in">`$.score()`</span> method.</span>
<span id="cb91-957"><a href="#cb91-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-958"><a href="#cb91-958" aria-hidden="true" tabindex="-1"></a>We are always happy to chat and welcome new contributors, please get in touch if you need assistance in extending <span class="in">`mlr3`</span>.</span>
<span id="cb91-959"><a href="#cb91-959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-960"><a href="#cb91-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-961"><a href="#cb91-961" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb91-962"><a href="#cb91-962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-963"><a href="#cb91-963" aria-hidden="true" tabindex="-1"></a>This chapter covered several advanced topics including parallelization, error handling, logging, working with databases, and extending the <span class="in">`mlr3`</span> universe.</span>
<span id="cb91-964"><a href="#cb91-964" aria-hidden="true" tabindex="-1"></a>For simple use cases, you will probably not need to know each of these topics in detail, however, we do recommend being familiar at least with error handling and fallback learners, as these are essential to preventing even simple experiments being interrupted.</span>
<span id="cb91-965"><a href="#cb91-965" aria-hidden="true" tabindex="-1"></a>If you are working with large experiments or datasets, then understanding parallelization, logging, and databases will also be essential.</span>
<span id="cb91-966"><a href="#cb91-966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-967"><a href="#cb91-967" aria-hidden="true" tabindex="-1"></a>We have not covered any of these topics extensively and therefore recommended the following resources should you want to read more about these areas.</span>
<span id="cb91-968"><a href="#cb91-968" aria-hidden="true" tabindex="-1"></a>If you are interested to learn more about parallelization in R, we recommend @Schmidberger2009 and @Eddelbuettel2020.</span>
<span id="cb91-969"><a href="#cb91-969" aria-hidden="true" tabindex="-1"></a>To find out more about logging, have a read of the vignettes in <span class="in">`lgr`</span>, which cover everything from logging to JSON files to retrieving logged objects for debugging.</span>
<span id="cb91-970"><a href="#cb91-970" aria-hidden="true" tabindex="-1"></a>For an overview of available DBMS in R, see the CRAN task view on databases at <span class="in">`r link("https://cran.r-project.org/view=Databases")`</span>, and in particular the vignettes of the <span class="in">`dbplyr`</span> package for DBMS readily available in <span class="in">`mlr3`</span>.</span>
<span id="cb91-971"><a href="#cb91-971" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-972"><a href="#cb91-972" aria-hidden="true" tabindex="-1"></a>| Class | Constructor/Function | Fields/Methods |</span>
<span id="cb91-973"><a href="#cb91-973" aria-hidden="true" tabindex="-1"></a>| --- | --- | --- |</span>
<span id="cb91-974"><a href="#cb91-974" aria-hidden="true" tabindex="-1"></a>| - | <span class="in">`r ref("future::plan()")`</span> | - |</span>
<span id="cb91-975"><a href="#cb91-975" aria-hidden="true" tabindex="-1"></a>| - | <span class="in">`r ref("set_threads()")`</span> | - |</span>
<span id="cb91-976"><a href="#cb91-976" aria-hidden="true" tabindex="-1"></a>| - | <span class="in">`r ref("future::tweak()")`</span> | - |</span>
<span id="cb91-977"><a href="#cb91-977" aria-hidden="true" tabindex="-1"></a>| <span class="in">`Learner`</span> | <span class="in">`lrn()`</span> | <span class="in">`$encapsulate`</span>; <span class="in">`$fallback`</span>; <span class="in">`$timeout`</span>; <span class="in">`$parallel_predict`</span>; <span class="in">`$log`</span> |</span>
<span id="cb91-978"><a href="#cb91-978" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("lgr::Logger")`</span> | <span class="in">`r ref("lgr::get_logger")`</span> | <span class="in">`$set_threshold()`</span> |</span>
<span id="cb91-979"><a href="#cb91-979" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("mlr3db::DataBackendDplyr")`</span> | <span class="in">`r ref("mlr3::as_data_backend")`</span> | - |</span>
<span id="cb91-980"><a href="#cb91-980" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("mlr3db::DataBackendDuckDB")`</span> | <span class="in">`r ref("as_duckdb_backend")`</span> | - |</span>
<span id="cb91-981"><a href="#cb91-981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-982"><a href="#cb91-982" aria-hidden="true" tabindex="-1"></a>: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable). {#tbl-technical-api}</span>
<span id="cb91-983"><a href="#cb91-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-984"><a href="#cb91-984" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb91-985"><a href="#cb91-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-986"><a href="#cb91-986" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Consider the following example where you resample a learner (debug learner, sleeps for three seconds during train) on four workers using the multisession backend:</span>
<span id="cb91-987"><a href="#cb91-987" aria-hidden="true" tabindex="-1"></a><span class="in">```{r technical-050, eval = FALSE}</span></span>
<span id="cb91-988"><a href="#cb91-988" aria-hidden="true" tabindex="-1"></a><span class="in">tsk_penguins = tsk("penguins")</span></span>
<span id="cb91-989"><a href="#cb91-989" aria-hidden="true" tabindex="-1"></a><span class="in">lrn_debug = lrn("classif.debug", sleep_train = function() 3)</span></span>
<span id="cb91-990"><a href="#cb91-990" aria-hidden="true" tabindex="-1"></a><span class="in">rsmp_cv6 = rsmp("cv", folds = 6)</span></span>
<span id="cb91-991"><a href="#cb91-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-992"><a href="#cb91-992" aria-hidden="true" tabindex="-1"></a><span class="in">future::plan("multisession", workers = 4)</span></span>
<span id="cb91-993"><a href="#cb91-993" aria-hidden="true" tabindex="-1"></a><span class="in">resample(tsk_penguins, lrn_debug, rsmp_cv6)</span></span>
<span id="cb91-994"><a href="#cb91-994" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb91-995"><a href="#cb91-995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-996"><a href="#cb91-996" aria-hidden="true" tabindex="-1"></a>(a) Assuming you were running this experiment on a computer with four CPUs, and that the learner would actually calculate something and not just sleep: Would all CPUs be busy for the entire time of this calculation?</span>
<span id="cb91-997"><a href="#cb91-997" aria-hidden="true" tabindex="-1"></a>(b) Prove your point by measuring the elapsed time, e.g., using <span class="in">`r ref("system.time()")`</span>.</span>
<span id="cb91-998"><a href="#cb91-998" aria-hidden="true" tabindex="-1"></a>(c) What would you change in the setup and why?</span>
<span id="cb91-999"><a href="#cb91-999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-1000"><a href="#cb91-1000" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Create a new custom binary classification measure which scores ("prob"-type) predictions.</span>
<span id="cb91-1001"><a href="#cb91-1001" aria-hidden="true" tabindex="-1"></a>  This measure should compute the absolute difference between the predicted probability for the positive class and a 0-1 encoding of the ground truth and then average these values across the test set.</span>
<span id="cb91-1002"><a href="#cb91-1002" aria-hidden="true" tabindex="-1"></a>  Test this with <span class="in">`classif.log_reg`</span> on <span class="in">`tsk(“sonar”)`</span>.</span>
<span id="cb91-1003"><a href="#cb91-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-1004"><a href="#cb91-1004" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>"Tune" the <span class="in">`error_train`</span> hyperparameter of the <span class="in">`classif.debug`</span> learner on a continuous interval from 0 to 1, using a simple classification tree as the fallback learner and the penguins task.</span>
<span id="cb91-1005"><a href="#cb91-1005" aria-hidden="true" tabindex="-1"></a>  Tune for 50 iterations using random search and 10-fold cross-validation.</span>
<span id="cb91-1006"><a href="#cb91-1006" aria-hidden="true" tabindex="-1"></a>  Inspect the resulting archive and find out which evaluations resulted in an error, and which did not.</span>
<span id="cb91-1007"><a href="#cb91-1007" aria-hidden="true" tabindex="-1"></a>  Now do the same in the interval 0.3 to 0.7.</span>
<span id="cb91-1008"><a href="#cb91-1008" aria-hidden="true" tabindex="-1"></a>  Are your results surprising?</span>
<span id="cb91-1009"><a href="#cb91-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-1010"><a href="#cb91-1010" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb91-1011"><a href="#cb91-1011" aria-hidden="true" tabindex="-1"></a><span class="in">`r citeas(chapter)`</span></span>
<span id="cb91-1012"><a href="#cb91-1012" aria-hidden="true" tabindex="-1"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>All content licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> <br> © Bernd Bischl, Raphael Sonabend, Lars Kotthoff, Michel Lang.</p>
</div>   
    <div class="nav-footer-center">
<p><a href="https://mlr-org.com">Website</a> | <a href="https://github.com/mlr-org/mlr3book">GitHub</a> | <a href="https://mlr-org.com/gallery">Gallery</a> | <a href="https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/">Mattermost</a></p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mlr-org/mlr3book/edit/main/book/chapters/chapter10/advanced_technical_aspects_of_mlr3.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mlr-org/mlr3book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/mlr-org/mlr3book/blob/main/book/chapters/chapter10/advanced_technical_aspects_of_mlr3.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>